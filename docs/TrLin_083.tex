%\documentclass{article}
\documentclass[openany, oneside, 11pt]{book}% landscape
\pagestyle{plain}
\newcommand{\newslide}{\newpage} 
\newenvironment{slide}{}{}
%\hoffset-1.5in \voffset-1.5in \setlength{\parskip}{5mm}
\hoffset-1.5in \voffset-1.5in \setlength{\parskip}{5mm} \textwidth7.7in \textheight9in
%\documentclass{slides} \newcommand{\newslide}{\end{slide}\begin{slide}}
%\documentclass{IEEEtran}

\usepackage[active]{srcltx}

\usepackage{amsthm,amssymb,latexsym,stmaryrd}

\usepackage[spanish]{babel}

\usepackage[latin1]{inputenc}
 
\usepackage{pst-all}

\usepackage{verbatim}
 
\usepackage{enumerate}

\usepackage[leqno]{amsmath}

\allowdisplaybreaks

%\usepackage[notcite,notref]{showkeys}   

\usepackage[dvips]{graphicx}

%\usepackage{makeidx}
%\makeindex

\theoremstyle{plain}
 \newtheorem{prop}{Proposición}%[{section}]
 \newtheorem{theo}[prop]{{Teorema}}
 \newtheorem{corol}[prop]{{Corolario}}
   \theoremstyle{definition}
 \newtheorem{defi}[prop]{{Definición}}
 \newtheorem{noci}[prop]{{Noción}}
 \newtheorem*{defin}{}
 \newtheorem{ejem}[prop]{{Ejemplo}}
 \newtheorem{note}[prop]{{Notación}}
   \newtheorem{nb}{ELIMINAR}
       \newtheorem{explicacion}[prop]{}
\newpsobject{grilla}{psgrid}{subgriddiv=1,griddots=10,gridlabels=6pt}
\newcommand{\teorema}[1]{{\setlength{\fboxrule}{2pt}\fbox{\parbox{17cm}{#1}}}}
\newcommand{\perspectiva}[1]{\centerline{\fbox{\parbox{10.5cm}{#1}}}. \\[0.1cm]}


\newcommand{\de}[1]{\textbf{#1}\index{#1|textbf}{}}
\newcommand{\dn}[1]{\textit{#1}\index{#1|textbf}{}}
\newcommand{\ds}[1]{{#1}}
\newcommand{\dt}[1]{\tag{#1}\label{eq_#1}\index{$#1$|textbf}{} } 
\newcommand{\dc}[1]{\text{\textbf{(#1)}}\index{#1|textbf}{}}  
\newcommand{\dct}[2]{}%{\dc{#2}\dt{#1}} 
\newcommand{\dctc}[4]{\de{(#1) #2}, #3. #4.\par} 
\newcommand{\dctb}[3]{\de{(#1) #2}, #3.\par} 
\newcommand{\dctd}[3]{\item[#2 (#1)] #3.}
\newcommand{\dcta}[3]{ &#3 && \dc{#2}\dt{#1}.}
\newcommand{\dcB}[2]{\left(\begin{aligned}&\text{\textbf{#1}}\\ &\text{\textbf{#2}}\end{aligned}\right)\index{#1 #2|textbf}{}}  
\newcommand{\dcC}[3]{\left(\begin{aligned}&\text{\textbf{#1}}\\ &\text{\textbf{#2}}\\&\text{\textbf{#3}}\end{aligned}\right)\index{#1 #2|textbf}{}}  
\newcommand{\<}{\langle}
\renewcommand{\>}{\rangle}
\newcommand{\la}{\leftarrow}
\newcommand{\ra}{\rightarrow}
\newcommand{\La}{\Leftarrow}
\newcommand{\Ra}{\Rightarrow}
\newcommand{\lra}{\leftrightarrow}
\newcommand{\Lra}{\Leftrightarrow}
\newcommand{\N}{\ensuremath{\mathbb{N}}}
\newcommand{\Z}{\ensuremath{\mathbb{Z}}}
\newcommand{\Q}{\ensuremath{\mathbb{Q}}}
\newcommand{\R}{\ensuremath{\mathbb{R}}}
\newcommand{\C}{\ensuremath{\mathbb{C}}}
\newcommand{\x}{\times}
%\newcommand{\A}{a} % no se puede cambiar este valor "a" ya que en algunos lugares se tene (A)_ij + (B)ij.
\newcommand{\matriz}[1]{\begin{matrix}#1\end{matrix}}
\newcommand{\bmatriz}[1]{\begin{bmatrix}#1\end{bmatrix}}
\newcommand{\vmatriz}[1]{\begin{vmatrix}#1\end{vmatrix}}

%\newcommand{\codigo}[1]{\vspace{0.3cm}\fbox{\parbox{8cm}{\codig #1}}\vspace{0.3cm}}
%\newcommand{\codig}[1]{{\ttfamily  #1}}
%\newcommand{\codig}[1]{\ttfamily \begin{verbatim} #1
%\end{verbatim} }
%\newenvironment{codi}{\ttfamily }{}
\newcommand{\bibite}[1]{\bibitem[#1]{#1}}
\newcommand{\defOpMat}[6]{\textbf{$#1$ #2}
\begin{itemize} 
\item $#1$ existe  #3 
\item su tamaño es $size(#1):=#4$ 
\item sus elementos son $(#1)_{ij}:=#5$ 
%\item  
\end{itemize}
en Scilab se escribe #6}
\newcommand{\defOpMatA}[6]{\textbf{$#1$ #2}
\begin{itemize} 
\item $#1$ existe  #3 
\item el resultado es #4 
\item y se calcula #5 
%\item  
\end{itemize}
en Scilab se escribe #6}
\newcommand{\bsmatrizA}[1]{\bigl[\begin{smallmatrix} #1  \end{smallmatrix}\bigr]}
\newcommand{\bsmatrizB}[1]{\Bigl[\begin{smallmatrix} #1  \end{smallmatrix}\Bigr]}
\newcommand{\bsmatrizC}[1]{\biggl[\begin{smallmatrix} #1  \end{smallmatrix}\biggr]}
\newcommand{\bsmatrizD}[1]{\Biggl[\begin{smallmatrix} #1  \end{smallmatrix}\Biggr]}
\newcommand{\bsmatriz}[1]{\left[\begin{smallmatrix} #1  \end{smallmatrix}\right]}
\newcommand{\vsmatriz}[1]{\left|\begin{smallmatrix} #1  \end{smallmatrix}\right|}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\parr}[1]{\left( #1 \right)}
\newcommand{\corch}[1]{\left[ #1 \right]}

%\newcommand{\bmatriz}[1]{\begin{bmatrix} #1 \end{bmatrix} }
%\newenvironmet{bsmatrix}{\bigl[\begin{smallmatrix}}{\bigl]\end{smallmatrix}}
%\newenvironmet{bsmatrix}{\bigl[\begin{smallmatrix}}{\bigl]\end{smallmatrix}}
\newcommand{\ve}[1]{\overline{#1}}
\newcommand{\veu}[1]{\widehat{#1}}
%\newcommand{\ve}[1]{\vect{#1}}

%\setcounter{MaxMatrixCols}{30}
\definecolor{Black}{rgb}{0,0,0}
% \usepackage[usenames,dvipsnames]{pstricks}
% \usepackage{epsfig}
% \usepackage{pst-grad} % For gradients
% \usepackage{pst-plot} % For axes


\begin{document}


\title{Algunas definiciones, ejemplos y teoremas de Álgebra Lineal orientados a Ingenieros}
%\title{Combinación Lineal en $\R^n$ y Transformación Matricial con ejemplos en $\R^2$ y $\R^3$}
\author{Bajo Licencia GPL (GFDL)\\
desarrollado por:\\
Gerardo Muñoz gmunoz@udistrital.edu.co\\
Material en desarrollo, agradezco todas sus sugerencias para mejorarlo.}
\maketitle 
 \tableofcontents

%Contenidos evaluados por cada quiz.
%\begin{itemize}
%\item Quiz 1 
%\begin{itemize} 
%  \item Propiedades básicas de los números reales	
%\end{itemize}
%\item Quiz 2 
%\begin{itemize}
%  \item			Matrices	
%  \item	Operaciones entre matrices	
%  \begin{itemize}
%    \item		Transposición
%    \item		Suma
%    \item			Opuesto
%    \item			Resta
%    \item			Escalar por matriz
%  \end{itemize}
%  \item		Combinación lineal	
%  \begin{itemize}
%    \item			Matriz por vector
%    \item			Multiplicación matricial
%  \end{itemize}
%\end{itemize}
%\item Quiz 3 
%\begin{itemize} 	
%  \item		Solución de sistema de ecuaciones	
%  \item			Matrices extendidas 
%  \item			Matrices escalón y reducidas 
%  \item			Eliminación de Gauss
%\end{itemize}
%\item Quiz 4
%\begin{itemize} 
%\item	Pseudo-inversa e inversa		
%\end{itemize}
%\item Quiz 5 
%\begin{itemize} 
%\item		Determinante	
%\begin{itemize}
%\item			Adjunta
%\item		Regla de Cramer
%\end{itemize}
%\end{itemize}
%\item Quiz 6 
%\begin{itemize} 
%\item		Subespacios de $\R^n$ y transformaciones matriciales	
%\begin{itemize}
%\item			Composición
%\item			Imagen
%\item			Transf. Inversa
%\item		Núcleo
%\end{itemize}
%\end{itemize}
%\item Quiz 7
% \begin{itemize} 
%%\item		Espacio Euclidiano	
%\item		Producto punto
%\item		Repaso de trigonometría
%\end{itemize}\item Quiz 8 
%\begin{itemize} 
%\item		Proyección
%\end{itemize}
%\item Quiz 9 
%\begin{itemize} 
%\item			Magnitud
%\item		Distancia
%\end{itemize}
%\item Quiz 10 
%\begin{itemize} 
%\item		Rectas en $\R^2$
%\end{itemize}
%\item Quiz 11
%\begin{itemize} 
%\item		Rectas en $\R^3$
%\end{itemize}
%\item Quiz 12 \begin{itemize} 
% \item		Planos en $\R^3$
%\end{itemize}
%\item Quiz 13 
%\begin{itemize}
%%\item		Bases	
%\begin
%%\item			Bases e isomorfismos
%%\item			Cambio de base
%%\item		Bases ortogonales
%%\end{itemize}\item Quiz \begin{itemize} 
%%------------------------------
%%\item			Vectores propios
%%\item		Diagonalización
%%\end{itemize}\item Quiz \begin{itemize} 
%%--------------------------
%%\end{itemize}
%\end{itemize}
%-------------- falta referencias a ejercicios y ejemplos

%\begin{abstract}
%En este artículo se resumen algunos resultados básicos del álgebra lineal enfocados 
%a poder comparar las soluciones de sistemas lineales.
%\end{abstract}

\chapter*{Introducción}



%Con este material se pretende hacer una lista de los contenidos del libro \cite{NJ99} para hacer una introducción `empírica' a los espacios vectoriales de $\R^n$ y las transformaciones matriciales, con el fin de que se familiaricen con las nociones, para que cuando vean las definiciones formales de los espacios vectoriales en general les quede más fácil la comprensión de los conceptos. 



%Las palabras en cursiva hace referencia a conceptos que aun no se han introducido pero que por el momento basta una noción intuitiva. Las palabras en negrilla son los conceptos que se están definiendo.

%Aunque se presentan algunas instrucciones de Scilab, no se recomienda utilizar esta herramienta en el curso, ya que para comprender mejor los conceptos es mejor realizar los ejercicios en papel, además en las evaluaciones no se utilizan estos programas.

%La ó se utiliza para enfatizar la diferencia de la o con el 0 en las ecuaciones.

En este material se presentan las notas de clase de un curso de Álgebra Lineal para ingenieros con algunos comandos de Scilab para ser usados como referencia en cursos posteriores. Se advierte que, usualmente para los estudiantes que hasta ahora están aprendiendo Álgebra Lineal, Scilab se convierte más en una distracción que en una ayuda y por lo tanto se sugiere ignorar los comandos de Scilab mientras se apropia de los conocimientos de la materia.

Este documento no remplaza el texto guia \cite{NJ99}.

------------------------ falta completar
\chapter{Números, Vectores y Matrices}
Borrador de las notas de clase, enviar las sugerencias a  gmunoz@udistrital.edu.co. (Bajo licencia GPL) 
\section{Matrices y la teoría del juego} 
Con el fin de introducir las matrices se presenta un sencillo ejemplo de la teoría de juegos, el conocido juego de piedra, papel o tijera. Este ejemplo fue tomado el primero de agosto de 2012 de 

{ {\verb!http://es.wikipedia.org/wiki/Equilibrio_de_Nash!}}

%Al final de esta sección se definen los primeros conceptos de matrices.

%\subsection{Piedra, papel y tijera}
Consideremos el juego de piedra, papel o tijera con la matriz de pagos  para el jugador 1 dada por:	
%\rotatebox{90}{\fbox{Jugador 1}}
\begin{center}
\begin{tabular}{cc|c|c|c|}%\hline
&\multicolumn{4}{c}{Jugador 2}\\%\hline
&&Piedra&	Papel&	Tijera\\\cline{2-5}%\hline
&Piedra&	0&	-1&	+1\\\cline{2-5}
Jugador 1&Papel&	+1&	0&	-1\\\cline{2-5}
&Tijera&	-1&	+1&	0\\\cline{2-5}%\hline
\end{tabular}
\end{center}
Y para el jugador 2 dada por:	
%\rotatebox{90}{\fbox{Jugador 1}}
\begin{center}
\begin{tabular}{cc|c|c|c|}%\hline
&\multicolumn{4}{c}{Jugador 2}\\%\hline
&&Piedra&	Papel&	Tijera\\\cline{2-5}%\hline
&Piedra&	0&	+1&	-1\\\cline{2-5}
Jugador 1&Papel&	-1&	0&	+1\\\cline{2-5}
&Tijera&	+1&	-1&	0\\\cline{2-5}%\hline
\end{tabular}
\end{center}

Si el jugador 1 gana recibe un pago de +1, pero si pierde su pago es de -1, cuando empata su pago es 0. En este Juego siempre que gana un jugador el otro pierde lo mismo, este tipo de juegos se llaman de suma cero, porque si se suma lo que ganan los jugadores siempre da cero. Sin embargo, en la vida real hay muchos juegos que no son de suma cero, por ejemplo usualmente en los negocio es un tipo de juego en el que ambos jugadores ganan. Si se mira el marcador en un partido de fútbol, este es un juego de suma cero, pero si se mira como han mejorado las tácticas del juego a lo largo de la historia, este no es un juego de suma cero ya que después de cada partido gane o pierda siempre todos los equipos pueden mejorar sus estrategias. 

\perspectiva{Si los recursos de la humanidad fueran fijos, estaríamos jugando un juego de suma cero, por lo tanto el que tiene más recursos sería el más amenazado, porque los demás tratarían que quitárselos.  
¿Podría toda la humanidad encontrar una forma de convivencia en la que todos siempre ganen? 
Yo creo que sí, si el objetivo del juego consistiera en aumentar los recursos de la humanidad, una forma sería aumentando la vida en los desiertos, el fondo marino, incluso en nuestras ciudades; otra forma es consiguiendo nuevos recursos al ir a otros planetas.   
En este caso cada vez que alguno logre obtener algún recurso sería una alegría para toda la humanidad.}
%
%De aquí yo concluyo que si los recursos de la humanidad fueran fijos,
%
% sería una mentira el sentir que entre más recursos acaparemos estamos más seguros, ya que si un jugador gana recursos implica que otro pierde recursos y este debe hacer lo posible por quitarle los recursos al que tiene para sentirse seguro. Entonces lleva a todo lo contrario, el que tiene más recursos es el más amenazado. ¿Podría toda la humanidad encontrar una forma de convivencia en la que todos siempre ganen? Yo creo que sí, si el objetivo del juego consistiera en aumentar los recursos de la humanidad, una forma sería aumentando la vida en los desiertos, el fondo marino, incluso en nuestras ciudades; otra forma es 
%%por ejemplo 
%consiguiendo nuevos recursos al ir a otros planetas o sistemas solares.  
%%conseguir los recursos fuera en estudiar y llevar la vida `simbiótica' a todo el universo. 
%En este caso cada vez que alguno logre obtener algún recurso sería una alegría para toda la humanidad.}  
%Bueno, podemos comenzar por llevar una vida agradable a nuestros ancianos, nuestras ciudades, nuestros desiertos, nuestros océanos, etc.} 

%Supongamos que el jugador 1 juega siempre en estrategias puras, por ejemplo piedra. Entonces el jugador 2 podría sacar ventaja de ello jugando siempre papel. Una mejor respuesta del jugador 1 sería entonces jugar con estrategias mixtas, es decir, asignarle cierta probabilidad a cada estrategia y en cada jugada elegir aleatoriamente de acuerdo a la distribución elegida.

%Puede demostrarse que siempre que haya sesgo en estas probabilidades (es decir, cuando se le asigne más probabilidad a una estrategia que a otra), el otro jugador puede sacar ventaja de ello y mejorar su pago esperado. De éste modo, el juego sólo tiene un equilibrio de Nash y es (1/3,1/3,1/3), es decir, jugar con igual probabilidad cada estrategia (siempre y cuando se mantengan los pagos dados por la matriz).


\newslide

\section{Algunas nociones y definiciones preliminares}

\perspectiva{Hago una aclaración entre las nociones (letras cursivas) y las definiciones (letras en negrilla), las nociones están basadas en nuestra experiencia, especialmente la que se tiene de los cursos de matemáticas del colegio. Por otro lado las definiciones están basadas en las nociones y en otras definiciones. En matemáticas se busca reducir al máximo el número de nociones con el fin de que dependa al mínimo de las experiencias de cada sujeto, ya que esto puede traer serios problemas como el ocurrido con la paradoja de Roussel con la noción de conjunto. Sin embargo no es posible eliminar todas las nociones debido a que es necesario comenzar de algún punto.} %Siendo rigurosos, todas las palabras escritas son nociones pero hay algunas especialmente relacionadas con objetos matemáticos cuyas definiciones están fuera del alcance de este texto, las cuales son las que se resaltan con letras cursivas.}

La primera noción es la de \dn{elemento} denotado con la letras minúsculas, los elementos le dan sentido a la noción de \dn{conjunto} denotados con las letras mayúsculas. Estas dos primeras nociones se relacionan mediante la \dn{pertenencia} ($\in$). Podemos distinguir dos tipos de conjuntos, los conjuntos \dn{finitos} en los cuales podemos enumerar los elementos, por ejemplo el conjunto de los dígitos $\{1,2,3,4,5,6,7,8,9,0\}$. El segundo tipo es el de los conjuntos \dn{infinitos}, de los cuales estamos familiarizados con los  conjuntos de números con sus respectivas operaciones como \dn{suma}, \dn{resta}, \dn{producto} y \dn{división}.
\begin{itemize}
\item $\ds{\N_1}:=\{1,2,3,\ldots\}$ es el conjunto de los \dn{números naturales} a partir del número 1, el cual tiene las operaciones de suma y producto.
\item $\ds{\N_0}:=\{0,1,2,3,\ldots\}$ es el conjunto de los \dn{números naturales} a partir del número 0.
\item $\ds{\Z}:=\{\ldots,-3,-2,-1,0,1,2,3,\ldots\}$ es el conjunto de los \dn{números enteros}, el cual  además tiene la operación de resta.

\item El conjunto de los \de{números racionales} se define $\ds{\Q}:=\{ \frac{a}{b} \mid a \in \Z \text{ y } b \in \N_1 \}$, teniendo en cuenta que $\frac{a_1}{b_1}=\frac{a_2}{b_2}$ si y sólo si $a_1b_2=a_2b_1$, el cual  además tiene la operación de división entre números diferentes del cero.% aunque para fines prácticos se utilizan tres \dn{sucesiones} finitas de dígitos, la primera sucesión indica la parte entera, luego de un punto sigue la segunda suceción que indica la parte .
\item $\ds{\R}$ denota el conjunto de los \dn{números reales} los cuales representamos como una \dn{sucesión} `infinita' de dígitos entre los cuales hay un punto y comienza con un \dn{signo}.
\item $\ds{\C}:=\{a+bi \mid a,b \in \R\}$ es el conjunto de los \de{números complejos} en donde $i^2=-1$.
\end{itemize}
Es de resaltar la notación usada para definir los conjuntos $\Q$ y $\C$, en las cuales ente corchetes primero se colocan los elementos genéricos del conjunto con \dn{variables}, luego se separa con `$\mid$' y finalmente las condiciones que deben cumplir las variables. 
\[\{ \text{ elementos genéricos } \mid \text{ condiciones } \}\]
Algunas veces indicaremos a que conjunto pertenecen los elementos genéricos de la siguiente forma.
\[\{ \text{ elementos genéricos } \in \R \mid \text{ condiciones } \}\]

%El concepto formal de conjunto no tiene asociado un orden para los elementos. %Sin embargo, debido a la forma de denotar los elementos de conjuntos finitos siempre hay un orden al escribirlos. 
%además los conjuntos infinitos de números también están \dn{totalmente ordenados}. 
Para el tema que nos ocupa, es conveniente considerar el $\de{vector-$m$}$ de números reales, que es una \dn{función} del intervalo de números naturales $\{1,2,\dots,m\}$ al conjunto de los números reales $\R$, la cual se denota como una columna \[ \bmatriz{a_1\\ a_2\\ \vdots\\ a_m} \] En algunos casos separaremos los renglones con un punto y coma (;) como se hace con el programa Scilab \[ [a_1; a_2; \ldots; a_m] \] 
%que son conjuntos finitos de números reales indexados $\{a_i \in \R \mid i \in 1,2,\dots,m\}$.

Usualmente los vectores los denotaremos con letras minúsculas con una flecha o línea encima. 

%Un \de{vector-$m$} en el \dn{espacio} $\R^m$ (o simplemente en $\R^m$) es de la forma 
\[\ve{u}=\bmatriz{a_1\\a_2\\ \vdots \\ a_m } %\text{ o } \ve{u}=[a_1 \ a_2 \ \ldots \ a_m]
\]
%este vector se puede escribir como un conjunto indexado de objetos
%\[
%\ve{u}=\{a_i \mid i\in\{1,2,\ldots,m\}\}
%\]
%en este caso los índices son los números de $1$ a $m$.


Cada $a_i$ se denomina \de{componente} o \de{elemento} del vector y son  \dn{números reales} ($a_1,a_2,\ldots,a_m \in \R$) llamados también \de{escalares}. $m \in \N_1$ indica el \de{tamaño del vector} o la \de{dimensión del espacio}, porque el conjunto de todos los vectores-$m$ se llama el \de{espacio} $\R^m$
\[\R^m:=\set{\bmatriz{a_1\\a_2\\ \vdots \\ a_m } \mid a_1,a_2,\ldots,a_m \in \R}\]

El \de{vector-$m$ cero} ($\ve{0}:=[0;0; \ldots ; 0 ]$) %tiene $m$ componentes y cada una corresponden al numero real cero. El vector cero 
se puede asociar con el \dn{origen} del \dn{sistema de coordenadas}. 

Cuando $m$ es 1, 2 o 3, un vector se puede graficar en una, dos o tres \dn{dimensiones} respectivamente como una flecha, en donde cada componente del vector corresponde a distancia que mide la flecha en cada una de las dimensiones. Algunas veces el vector tan sólo se grafica como un punto en las \dn{coordenadas} del vector. 
{
\center{
\begin{pspicture}(0,-2.3629167)(4.1709375,2.3829167)%\grilla
%\definecolor{color42}{rgb}{0.6,0.6,0.6}
\usefont{T1}{ptm}{m}{n}
\rput(1.7823437,0.07291667){$\ve{p}=\bmatriz{2\\1}$}
\rput(0.7809375,-1.6370833){\psaxes[linewidth=0.04](0,0)(0,0)(3,4)}
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(2.8809376,-0.5370833)(3.7809374,2.3629167)
%\psline[linewidth=0.04cm,linecolor=color42,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(0.7809375,-1.6370833)(2.7809374,-0.63708335)
\psdots[dotsize=0.12](2.7809374,-0.63708335)
\psline[linewidth=0.04cm,linestyle=dashed,dash=0.16cm 0.16cm,tbarsize=0.07055555cm 5.0]{|-|}(2.8809376,-0.7370833)(3.7809374,-0.7370833)
\psline[linewidth=0.04cm,linestyle=dashed,dash=0.16cm 0.16cm,tbarsize=0.07055555cm 5.0]{|-|}(3.8809376,2.3629167)(3.8809376,-0.63708335)
\usefont{T1}{ptm}{m}{n}
\rput(4.0485935,0.87291664){3}
\usefont{T1}{ptm}{m}{n}
\rput(3.2278125,-0.9270833){1}
\usefont{T1}{ptm}{m}{n}
\rput(2.2823439,1.2729167){$\ve{v}=\bmatriz{1\\3}$}
\psline[linewidth=0.04cm,linestyle=dashed,dash=0.16cm 0.16cm](2.7809374,-0.63708335)(2.7809374,-1.7370833)
\psline[linewidth=0.04cm,linestyle=dashed,dash=0.16cm 0.16cm](0.7809375,-0.63708335)(2.6809375,-0.63708335)
\end{pspicture}  }}

El dibujo podría representar una partícula en $[2;1]$ que se mueve a una velocidad $[1;3]$.  


 En $\R^2$ se definen los vectores-2 (vectores de dos elementos) \[\ve{i}:=\bmatriz{1\\0}, \ve{j}:=\bmatriz{0\\1}\] y en $\R^3$ se definen los vectores-3 (vectores de tres elementos) 
\[\ve{i}:=\bmatriz{1\\0\\0},\ve{j}:=\bmatriz{0\\1\\0},\ve{k}:=\bmatriz{0\\0\\1}\] 



Básicamente hay dos diferencias en entre un conjunto finito y un vector, 
\begin{itemize}
\item la primera radica en que en un conjunto no hay ningún orden entre los elementos, en cambio en un vector hay un primer elemento, un segundo elemento y así sucesivamente; 
\item la segunda diferencia radica en que en un conjunto no pueden repetirse elementos mientras que en un vector sí. 
\end{itemize}

Dos \de{conjuntos iguales} son los que tienen los mismos elementos por ejemplo $\{ n \in \N_0 \mid n^2=n \}=\{0,1\}$. 
Los vectores $\ve{u}:=[a_1;a_2; \ldots ; a_m ]$ y $\ve{v}:=[b_1;b_2; \ldots ; b_n ]$
son \de{vectores iguales} si cumplen dos cosas:
\begin{itemize}
	\item el tamaño de $\ve{u}$ es igual al de $\ve{v}$, ($m=n$).
	\item las componentes de 	$\ve{u}$ son las mismas componentes de $\ve{v}$ con el mismo orden, ($a_i=b_i$ con $i$ entre 1 y $m$).
\end{itemize}




%Dos \de{vectores iguales} deben cumplir dos cosas primero tener el mismo numero de elementos y segundo tener los mismos elementos en el mismo orden. 
Observemos que mientras que $\{0,1\}=\{1,0\}=\{1,0,0\}$, por otro lado $[0,1]\neq[1,0]\neq[1,0,0]$. 
%\subsubsection{Axiomas de la igualdad} 
%Vamos a tener en cuenta las siguientes  propiedades de la igualdad que se deducen a partir de remplazar igualdades.
La igualdad, en términos generales, cumple las siguientes propiedades para los números reales $a$, $b$ y $c$


%????????????

\teorema{
%\begin{description}
%\begin{itemize}
%\item 
\dctb{TranIgu}{Transitiva de la igualdad}{Si $a=b$ y $b=c$ entonces $a=c$} 
%\item 
\dctc{OpIgu}{Operaciones iguales}{Si $a=b$ entonces al realizar la misma operación a ambos lados se mantiene la igualdad}{Por ejemplo en las siguientes operaciones:
%\begin{itemize}
 $a+c=b+c$, %, \ \ \ \ \   $c+a=c+b$.
 $a \cdot c=b \cdot c$ y %, \ \ \ \ \ \ \ \   $c \cdot a=c \cdot b$.
% $\overline{a}=\overline{b}$ y  
 $a^{-1}=b^{-1}$}
%\end{itemize}   
%\end{itemize}
}

%Sobra decir que el concepto de conjunto ordenado puede tener cualquier cantidad enumerable de elementos pero en Sailab, por limitaciones técnicas, hay un tope para el número de elementos de un conjunto ordenado. 

%En Maxima para los conjuntos ordenados usaremos  las listas

El símbolo $:=$ se usan en las definiciones, por ejemplo para definir los elementos de un conjunto se escribe $B:=\{ n \in \N \mid n^2=n \}$. 

\newslide
\section{Propiedades de los números reales}



En el álgebra lineal más importante que definir un conjunto de números es definir las propiedades que debe cumplir ese conjunto de números. De hecho, la teoría que se va a presentar basada en los números reales puede ser desarrollada para otro conjunto que tenga suma y producto y cumpla las siguientes propiedades de los número reales $a,b,c \in \R$:

\teorema{ 
%\begin{itemize} 
%\item
  \dctc{ClaSumR}{Clausurativa de la suma}{$a+b \in \R$}{Esta propiedad exige que para cualquier valor de $a$ y de $b$ la suma siempre existe}%. %Como cada vez que se usa esta operación se requiere esta propiedad, usualmente nos referiremos a esta propiedad cuando la operación no la cumpla
%\item
  \dctc{ConSumR}{Conmutativa de la suma}{$a+b=b+a$}{En este caso no importa el orden de los sumandos} 
%\item
  \dctc{AsoSumR}{Asociativa de la suma}{$(a+b)+c=a+(b+c)$}{En este caso se pueden eliminar los paréntesis, así: $a + b + c$}%. %Después de eliminados los paréntesis no es necesario nombrar esta propiedad
%\item
  \dctb{ModulR}{Modulativa de la suma o modulativa}{
Existe $0 \in \R$ tal que  para todos los elementos  $a \in \R$ se tiene que $a+0=a$} 
%\item
 \dctb{OpuestR}{Opuesto de la suma o opuesto}{
Para cada uno de los elementos  $ a \in \R $ se tiene que existe un elemento (el mismo u otro) ${(-a)} \in A$ tal que $a + {(-a)} = 0 =  {(-a)} + a $}
%\end{itemize} 
%  &\text{Existe } 0 \in A \text{ tal que }\notag \\
%  &\text{ para todos los elementos  } a \in A \notag \\ 
%  &\text{ se tiene que } a+0=a && .\\ 
%  & \text{Para cada uno de los elementos  } a \in A  \notag \\
%  &\text{ se tiene que existe un elemento (el mismo u otro) } \overline(a) \in A \notag \\ 
%  &\text{ tal que } a + \overline{a} = 0 =  \overline{a} + a  && \dct{Opuest}{opuesto de la suma}.
%\end{description}
%\newslide
%\subsubsection{Axiomas de la multiplicación}
%\item[Axiomas de la multiplicación] 
%\begin{description}
%\item
  \dctc{ClaMulR}{Clausurativa de la multiplicación}{$a \cdot b \in \R$}{Esta propiedad exige que para cualquier valor de $a$ y de $b$ la multiplicación siempre existe} 
%\item
  \dctc{ConMulR}{Conmutativa de la multiplicación}{$a \cdot b=b \cdot a$}{En este caso no importa el orden de los múltiplos}
%\item
  \dctc{AsoMulR}{Asociativa de la multiplicación}{$(a \cdot b) \cdot c=a \cdot (b \cdot c)$}{En este caso se pueden eliminar los paréntesis, así: $a \cdot b \cdot c$} 
%\item
  \dctb{IdentR}{Modulativa de la multiplicación o identidad}{
Existe $1 \in \R$ tal que  para todos los elementos  $a \in \R$ se tiene que $a \cdot 1=a=1 \cdot a$} 
%\item 
 \dctb{InversR}{Opuesto de la  multiplicación o inversa}{
Para cada uno de los elementos  $ a \in \R$, diferente de 0, se tiene que existe un elemento (el mismo u otro) $a^{-1} \in \R$ tal que $a  \cdot  a^{-1} = 1 =  a^{-1}  \cdot  a $} 
%\end{description}
%\end{description}
%\subsubsection{Axiomas distributivos}
%\begin{description}
%\item 
 \dctb{DistrDR}{Distributiva a derecha}{$(b + c) \cdot a=(b \cdot a) + (c \cdot a)$}
%\item
  \dctb{DistrIR}{Distributiva a izquierda}{$a \cdot (b + c)=(a \cdot b) + (a \cdot c)$}
%\end{description}
%\end{itemize}
}
%Lss propiedades DistrD y DistrI se pueden remplazar por la propiedades 
%Como los reaSi además cumple la propiedad ConMul entonces DistrD es equivalente a DistrI y pueden denotarse solamente como 
%
%\dctb{DistrR}{Distributiva}{DistrD y DistrI}

%\begin{prop}

Además, para todo $a,b \in \R$ se tiene que:

%\begin{description}
\teorema{
  \dctb{MulCeroR}{Multiplicación por cero}{$0 \cdot a = 0 = a \cdot 0$} 
  \dctb{MulOpuesR}{Multiplicar por opuesto}{${(-b)} \cdot a = {(-(b \cdot a))}$} 
  }
%\end{description}
%\end{prop}
\begin{proof} Primero vamos a probar que $0 \cdot a = 0 $.
\begin{align*}
0 \cdot a &= 0 \cdot a + 0  && \text{ModulR}.\\
& = 0 \cdot a + (0 \cdot a + (-({0 \cdot a})))   && \text{OpuestR}.\\
& = (0 \cdot a + 0 \cdot a) + (-({0 \cdot a}))   && \text{AsoSumR}.\\
& = (0 + 0) \cdot a + (-({0 \cdot a}))   && \text{Distr}.\\
& =  0 \cdot a + (-({0 \cdot a}))   && \text{ModulR}.\\
& =  0    && \text{OpuestR}.
\end{align*}


De manera análoga se prueba que $0 = a \cdot 0$. Ahora vamos a probar que $\bar{b} \cdot a = (-({b \cdot a}))$.
\begin{align*}
{(-b)} \cdot a &= {(-b)} \cdot a + 0  && \text{ModulR}.\\
 &= {(-b)} \cdot a + (b \cdot a + (-({b \cdot a})))  && \text{OpuestR}.\\
% &= \bar{1} \cdot a + (1 \cdot a + \bar{b \cdot a})  && \text{Ident}.\\
 &= ({(-b)} \cdot a + b \cdot a) + (-({b \cdot a}))  && \text{AsoSumR}.\\
 &= ({(-b)}  + b) \cdot a + (-({b \cdot a}))  && \text{DistrR}.\\
 &= 0 \cdot a + (-({b \cdot a}))  && \text{OpuestR}.\\
 &= 0 + (-({b \cdot a}))  && \text{MulCeroR}.\\
 &= (-({b \cdot a}))  && \text{ModulR}.
\end{align*}
\end{proof}
\perspectiva{Usualmente entre los matemáticos no se llega a este nivel de detalle en las demostraciones, porque ellos escriben para otros matemáticos que infieren algunos sencillos pasos intermedios. Sin embargo, usualmente los ingenieros trabajan con máquinas a las que es necesario describir cada detalle. Estas demostraciones, así de detalladas, son un buen ejercicio para los ingenieros.}

Debido a las propiedades vistas de los números reales, se puede simplificar la notación de la siguiente forma
\begin{itemize}
\item $a-b$ denota $a+(-{b})$.
\item $ab$ denota $a \cdot b$.
\item $a/b$ o $\frac{a}{b}$ denota $a \cdot (b^{-1})$.
\end{itemize}
Además se deducen estas dos propiedades para todo $a,b,c \in \R$.

\teorema{
  \dctb{SumResR}{Sumando opuesto a ambos lados}{Si $a+b=c$ entonces $a=c-b$} 
  \dctb{MulDivR}{Multiplicando inverso a ambos lados}{Si $ab=c$ y $b \neq 0$ entonces $a=c/b$} 
}

Se recomienda hacer ejercicios de despejar ecuaciones para repasar.




\newslide
\section{Notación de matrices}

Una \de{matriz} de \de{tamaño} $m \times n$ es de la forma 
\begin{align*}
A
&=\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots&\vdots&\ddots&\vdots\\
a_{m1} & a_{m2} & \cdots & a_{mn}  \\	
\end{bmatrix}\\
\intertext{y se puede ver como un vector-$n$ (renglón) de vectores-$m$}
A&=[\ve{v_1} \ \ve{v_2} \ \ldots \ \ve{v_n}]
\end{align*}
en donde 

\[\ve{v_1}=\bmatriz{a_{11}\\a_{21}\\ \vdots \\ a_{m1} },
\ve{v_2}=\bmatriz{a_{12}\\a_{22}\\ \vdots \\ a_{m2} },
\ldots,
\ve{v_n}=\bmatriz{a_{1n}\\a_{2n}\\ \vdots \\ a_{mn} }
\]



%\subsection{Nociones y definiciones básicas}

%La matriz 
%\begin{align*}
%A
%&=\begin{bmatrix}
%a_{11} & a_{12} & \cdots & a_{1n} \\
%a_{21} & a_{22} & \cdots & a_{2n} \\
%\vdots&\vdots&\ddots&\vdots\\
%a_{m1} & a_{m2} & \cdots & a_{mn}  \\	
%\end{bmatrix}\\ 
%\end{align*}
%Una matriz se puede denotar 
%\[A=(a_{ij})_{m \times n}.\]
%Si el tamaño de la matriz es claro por el contexto entonces la notación se simplifica a 
%\[A=(a_{ij}).\]
%
Cada uno de los valores $a_{ij}$ de llama \dn{entrada $ij$ de la matriz $A$} o \dn{elemento $ij$ de la matriz $A$} y puede ser cualquier numero real, $a_{ij} \in \R$ (a los números reales los vamos a llamar \dn{escalares}). 

A continuación se presenta cómo definir una matriz en Scilab 
%\begin{codi}
\begin{verbatim}
 A=[1 1 1 175
 1 -2 0 0
 0 1 -2 0]
\end{verbatim}   
%\end{codi}
o así
%\codigo{A=[1  1  1   175;   1 -2  0     0;   0  1 -2     0]}
\verb!A=[1 1 1 175;1 -2 0 0; 0 1 -2 0]!, \\
o por columnas
\verb!A=[[1;1;0],[1;-2;1],[1;0;-2],[175;0;0]]!

El elemento del renglón $i$ y columna $j$  de una matriz $A$ se puede denotar por  \[(A)_{ij}=a_{ij}.\] En Scilab se denota con \verb!A(i,j)!.

Si vemos un vector como un punto, Una matriz se puede ver como un conjunto de puntos numerados. Por ejemplo la siguiente matriz 
\setcounter{MaxMatrixCols}{25}
\[\bsmatriz{-1&-1.5&-1&-0.5&-1&-1&1&1&1.5&1&0.5&1&1&1&0.5&0&0&0.2&0&-0.2&0&0&0.5&0\\
0.5&0&-0.5&0&0.5&1&1&0.5&0&-0.5&0&0.5&1&1.5&1.5&1.8&1.9&2.1&2.3&2.1&1.9&1&0.5&0} 
\]
corresponde a los siguientes puntos numerados
\begin{center}
{\psset{xunit=2.0cm,yunit=2.0cm}
\begin{pspicture}(-2,-1)(2.0,3.0)\grilla
\rput(-1,0.6){1}
\rput(-1.5,0.1){2}
\rput(-1,-0.4){3}
\rput(-0.5,0.1){4}
\rput(-1,0.4){5}
\rput(-1,1.1){6}
\rput(1,0.9){7}
\rput(1,0.4){8}
\rput(1.5,0.1){9}
\rput(1,-0.4){10}
\rput(0.5,0.1){11}
\rput(1,0.6){12}
\rput(1,1.1){13}
\rput(1,1.6){14}
\rput(.5,1.6){15}
\rput(0,1.7){16}
\rput(0.1,2){17}
\rput(0.2,2.2){18}
\rput(0,2.4){19}
\rput(-0.2,2.2){20}
\rput(-0.1,2){21}
\rput(0,1.1){22}
\rput(0.5,0.6){23}
\rput(0,0.1){24}
\psdots[dotsize=0.12](-1,0.5) 
\psdots[dotsize=0.12](-1.5,0) 
\psdots[dotsize=0.12](-1,-0.5) 
\psdots[dotsize=0.12](-0.5,0) 
\psdots[dotsize=0.12](-1,0.5) 
\psdots[dotsize=0.12](-1,1) 
\psdots[dotsize=0.12](1,1) 
\psdots[dotsize=0.12](1,0.5) 
\psdots[dotsize=0.12](1.5,0) 
\psdots[dotsize=0.12](1,-0.5) 
\psdots[dotsize=0.12](0.5,0) 
\psdots[dotsize=0.12](1,0.5) 
\psdots[dotsize=0.12](1,1) 
\psdots[dotsize=0.12](1,1.5) 
\psdots[dotsize=0.12](.5,1.5) 
\psdots[dotsize=0.12](0,1.8) 
\psdots[dotsize=0.12](0,1.9) 
\psdots[dotsize=0.12](0.2,2.1) 
\psdots[dotsize=0.12](0,2.3) 
\psdots[dotsize=0.12](-0.2,2.1) 
\psdots[dotsize=0.12](0,1.9) 
\psdots[dotsize=0.12](0,1) 
\psdots[dotsize=0.12](0.5,0.5) 
\psdots[dotsize=0.12](0,0) 
\end{pspicture} }
\end{center}

Al unir los puntos en el orden respectivo se obtiene el siguiente dibujo 

\begin{center}
		\includegraphics[scale=0.5]{ciclista_010_solo.eps}\\
\end{center}

%En la siguiente matriz, cada una de las columnas de la matriz 
%$\bsmatriz{
%-1 &-1.5&-1  \\
%0.5&0   &-0.5
%}$ representa los primeros un puntos del dibujo.  
%
%%---------------si vector como punto, matriz como unir puntos. (cambiar Lines por marks y escribir números).......
	
		el cual se generó con el siguiente código de Scilab. 
\begin{verbatim}
P=[[-1;0.5],[-1.5;0],[-1;-0.5],[-0.5;0],[-1;0.5],[-1;1],
[1;1],[1;0.5],[1.5;0],[1;-0.5],[0.5;0],[1;0.5],[1;1],
[1;1.5],[0.5;1.5],[0;1.8],[0;1.9],[0.2;2.1],[0;2.3],
[-0.2;2.1],[0;1.9],[0;1],[0.5;0.5],[0;0]]
plot2d(0,0,-1,"010"," ",[-1.6,-0.6,1.6,2.6]) 
xpoly(P(1,:),P(2,:),"lines")
\end{verbatim}   



%En el anterior ejemplo vimos que un vector puede ser representado gráficamente como un punto, pero también se puede representar como una flecha. 



\perspectiva{De pronto podrá extrañar que, en este texto al igual que otras referencias del curso, la noción de matriz no se da de forma rigurosa. Esto se debe a que al definir la matriz en términos de funciones distrae más de lo que aporta al objetivo principal de este curso. %  En la mayoría de textos del curso definen la matriz con otras palabras que no se han definido previamente y que el lector tiene igual noción que la de una matriz, ya que la noción de matriz o de conjunto está basada en la experiencia que tiene el lector al respecto, que es la experiencia que debe tener un bachiller, en todo caso con los ejemplos realizados en clase se aclara esta noción.
Una matriz de números reales se puede definir como % un conjunto finito de números reales doblemente indexado $\{a_{ij} \in \R \mid i \in \{1,2,\ldots,m\}, j \in \{1,2,\ldots,n \} \}$ el cual corresponde a la 
una función $f:\{1,2,\ldots,m\} \times \{1,2,\ldots,n \} \ra \R$}
%\newslide
\subsection{Tamaño de una matriz}
El \dn{tamaño} de una matriz se denota $m \times n$  (con $m \in \N_1$ y $n \in \N_1$)   en donde $m$ representa el \dn{número de renglones} y $n$ representa el \dn{número de columnas}. Cuando se desea hacer énfasis en el tamaño de una matriz, este se escribe como subíndice $(A)_{m \times n}$. %(no confundir esta notación con $(A)_{ij}$).
El conjunto de todas las matrices de tamaño $m \times n$ se denota $\ds{\R^{m \times n}}$. 


En el ejemplo de la matriz de puntos del ciclista, $m=2$ es la dimensión de cada punto y $n=24$ es la cantidad de puntos.

 En Scilab el tamaño de la matriz $A$ se define de alguna de las siguientes formas.  
\begin{itemize} 
\item \verb![m,n]=size(A) //tamaño reng. (m) y col. (n)!
\item \verb!m=size(A,"r") //tamaño sólo renglones (m)!
\item \verb!n=size(A,"c") //tamaño sólo columnas (n)! 
\end{itemize}
En algunas demostraciones usaremos la notación $size((A)_{m \times n})=m \times n$.  Para indicar que dos matrices $A$ y $B$ tienen el \de{mismo tamaño} podemos escribir $size(A)=size(B)$, es decir que ambas matrices tienen el mismo número de columnas y el mismo número de renglones.

Las matrices de tamaño $m \times 1$ en este texto corresponden a los vectores-$m$ cuyo tamaño es $m$. En otros textos también se consideran como vectores las matrices de $1 \times n$. %En Scilab no existen vectores como tal.    

Hay sólo un problema al hacer corresponder las matrices de $1 \times 1$ con los escalares, ya que al multiplicarlos por una matriz dan diferente. En Scilab las matrices de $1 \times 1$ se consideran como escalares.

%\newslide
\subsection{Algunos tipos de matrices}
La \de{matriz cero} $0_{m \times n}$ es una matriz de tamaño $m \times n$ donde todos sus elementos son cero. Es decir $(0_{m \times n})_{ij}=0$. Si no es importante el tamaño de la matriz se puede denotar $\ve{\ve{0}}$ o simplemente como $0$. En Scilab se escribe \verb!zeros(m,n)!.

%\newslide
Una matriz en la que $m=n$ se llama \de{cuadrada de orden $n$}. Los elementos $a_{ij}$ con  $i=j$ se dice que están en la \de{diagonal principal}.
% y la suma de estos elementos se llama la traza y se denota por $tr(A)$. En Scilab se escribe \verb!trace(A)!. 
La \de{matriz identidad} $I_n$ es una matriz cuadrada de orden $n$ que tiene unos en la diagonal principal y ceros en el resto de elementos. Si no es importante el tamaño de la matriz se puede denotar simplemente como $I$. En Scilab se escribe \verb!eye(n,n)!.

Una matriz cuadrada se llama \de{triangular superior} si todas sus componentes abajo de la diagonal son cero. Es una matriz \de{triangular inferior} si todos sus componentes arriba de la diagonal son cero. Una matriz se llama \de{diagonal} si todos los elementos que no están sobre la diagonal son cero. 


Definir una matriz implica definir dos cosas, el tamaño y cada uno de sus elementos.

%\newslide
\subsection{Igualdad de matrices}


Dos matrices $A$ y $B$ son \de{iguales} si tienen el mismo tamaño y  cada uno de sus entradas son iguales. Dicho de otra forma 
%si $m_A \times n_A$ es el tamaño de la matriz $A$ y $m_B \times n_B$ es el tamaño de la matriz $B$ entonces
  $A=B$ si y sólo si 
  \begin{itemize}
  \item $size(A)=m \times n=size(B)$ 
  \item $(A)_{ij}=(B)_{ij}$ con $i \in \{1,\ldots,m\}$ y $j \in \{1,\ldots,n\}$.
  \end{itemize}
  
  En Scilab para comparar igualdad de matrices o escalares se puede usar \verb!a==b!. Hay que tener cuidado que algunas veces un resultado que debería dar cero, en Scilab y en otros programas da un número muy pequeño pero diferente de cero. Para compara diferencia en Scilab se puede usar \verb!a~=b! o \verb!a<>b!.
  
  

Se recomienda ver \cite[Pg 154 y 155]{NJ99}




\newslide
%\subsection{Nociones y definiciones}

%
%En Scilab se puede definir una matriz $A$ así
%
%%\begin{codi}
%\begin{verbatim}
% A=[1 1 1 175
% 1 -2 0 0
% 0 1 -2 0]
%\end{verbatim}   
%%\end{codi}
%o así
%
%%\codigo{A=[1  1  1   175;   1 -2  0     0;   0  1 -2     0]}
%
%\verb!A=[1 1 1 175;1 -2 0 0; 0 1 -2 0]!
%
%
%En Scilab se puede representar el elemento $a_{i,j}$ de la matríz $A$ así
%
%%\codigo{A(i,j)}
%\verb!A(i,j)!
\subsection{Submatrices y vectores}

Una matriz se puede \dn{subdividir} o \dn{partir} en matrices más pequeñas creando \dn{submatrices}. Cada submatriz  es una parte rectangular de una matriz en la cual se restringen los renglones y las columnas.  En Scilab una submatriz de la matriz $A$ que restringida a los renglones 1 y 2  y a las columnas de la 1 a la 3 se define así \verb!A(1:2,1:3)!. Esta misma submatriz la podemos representar como  $(A)_{1 \ldots 2,1 \ldots 3}$.
 
 




 
 %, a esta última columna columna hace referencia la palabra `aumentada'.


Una \de{matriz renglón} o simplemente \de{renglón} es una matriz donde el número de renglones es $1$. %Cada renglón de una matriz es un vector renglón, %y esta submatriz se  denota como $\overline{a}=(A)_{i,\ldots}$ o $\overline{a}=(A)_{\text{renglón i}}$. 
En Scilab la submatriz renglón $i$ de una matriz $A$ es  \verb!A(i,:)!.
Si $\overline{r_1}$, $\overline{r_2}$ y $\overline{r_3}$ son matrices renglón entonces se pueden \dn{concatenar verticalmente} estos renglones para formar una matriz
\[A= \begin{bmatrix}
\overline{r_1}\\
\overline{r_2}\\
\overline{r_3}\\	
\end{bmatrix}\]
En Scilab se puede escribir así \verb!A=[r1;r2;r3]!.


De manera similar, una \de{matriz columna} o simplemente \de{columna} o un \de{vector} es una matriz donde el número de columnas es $1$. Cada columna de una matriz es un vector. 
%Esta submatriz se denota $\overline{a}=(A)_{\ldots,j}$ o $\overline{a}=(A)_{\text{columna j}}$. 
En Scilab la submatriz columna $j$ de una matriz $A$ es  \verb!A(:,j)!.
 Si $\overline{c_1}$, $\overline{c_2}$ y $\overline{c_3}$ son matrices columna entonces se pueden \dn{concatenar horizontalmente} estos renglones para formar una matriz
\[A= \begin{bmatrix}
\overline{c_1} & \overline{c_2} & \overline{c_3}	
\end{bmatrix}\]
En Scilab se escribe así \verb!A=[c1,c2,c3]!.


En Scilab se puede usar el símbolo  \verb!$!  para indicar el último valor.
Por ejemplo  
% \codigo{A(:,\$)} 
 \verb!A(:,$)! 
indica la última columna y  \verb!A(:,$)! indica el último renglón.

%Los vectores se identifican con matrices columna o matrices renglones, de tal modo que todas las definiciones asociadas a matrices se aplican a los vectores. %pero hay definiciones asociadas a vectores que no aplican a cualquier matriz.

%Los vectores usualmente se identifican con letras minúsculas con una línea superior ($\overline{v}$) o una flecha ($\vec{v}$), pero esta notación no diferencia si es renglón o columna (lo cual no será necesario en algunos contextos). Un vector de $n$ elementos se puede llamar \de{n-vector}. El conjunto de todos los n-vectores se simboliza $\R^{n}$.  El \de{vector cero} es un vector cuyos elementos son cero y se denota $\overline{0}$. 

Se recomienda ver \cite[pg 162 y 163]{NJ99}

\newslide
\section{Operación transposición}
Si $A$ es una matriz de $m \times n$, entonces la \de{transpuesta de $A$}, denotada por $A^T$, es la matriz de $n \times m$ que se obtiene al intercambiar los renglones y las columnas, es decir 

\defOpMat{A^{T}}{Transpuesta de una matriz $A$}
{siempre}
{n \times m \text{ en donde }  m \times n =size(A)}
{(A)_{ji}}  
{}\verb!A'!

%Las siguientes son algunas propiedades de la transpuesta.
La transpuesta cumple la siguiente propiedad

\teorema{
  \dctb{TraTra}{Una matriz es igual a la transpuesta de su transpuesta}{$(A^T)^T=A$} 
%  \dctb{TraSum}{La transpuesta de una suma}{$(A+B)^T=A^T+B^T$} 
%  \dctb{TraProEsc}{Transpuesta del producto escalar por matriz}{$(kA)^T=k(A^T)$} 
%  \dctb{TraMul}{Transpuesta de la multiplicación de matrices}{$(AB)^T=B^TA^T$} 
}


%\[(A^T)_{ij}=(A)_{ji}\] En Scilab se denota \verb!A'!.

Una matriz $A$ es \de{simétrica} si es igual a su transpuesta, $A=A^T$. 

Se recomienda ver \cite{NJ99} en las páginas 175 y 176.

\newslide

\section{Operación suma de matrices y vectores}

%\subsubsection{Suma de matrices}
En el conjunto de las matrices, la suma no es exactamente una operación, %en el sentido del álgebra, 
ya que no esta definida entre matrices de tamaños diferentes. Por eso 
para hablar de la operación suma entre matrices hay que restringirnos a matrices del mismo tamaño $m \times n$. 

Si $A, B \in \R^{m \times n}$ ($A$ y $B$  pertenecen al conjunto de las matrices de tamaño ${m \times n}$) entonces la \de{suma de matrices} $A+B$ es la matriz obtenida al sumar los elementos de $A$ con los correspondientes elementos de $B$. En resumen

%\defOpMat{A+B}{Suma de dos matrices $A$ y $B$}
%{$A+B$ existe si y sólo si  $size(A)=size(B)$}
%{su tamaño es  $size(A+B):=size(A)=size(B)$}
%{sus elementos son  $(A+B)_{ij}:=(A)_{ij}+(B)_{ij}$}
%{en Scilab se escribe \verb!A+B!}
\defOpMat{A+B}{Suma de dos matrices $A$ y $B$}
{si y solo si $size(A)=size(B)$}
{size(A)=size(B)}
{(A)_{ij}+(B)_{ij}}
{}\verb!A+B!

% \[A+B \text{ existe si y sólo si } size(A)=size(B) \text{ y su tamaño es } size(A+B):=size(A)=size(B) \text{ y sus elementos son } (A+B)_{ij}:=(A)_{ij}+(B)_{ij} \text{ y } (A)_{m \times n}+(B)_{m \times n}=(A+B)_{m \times n}\]
%La suma de dos matrices $A$ y $B$ existe si y sólo si $size(A)=size(B)$.


%para que cumpla la propiedad ClaSumM----------------------------------------------------- y así podemos evaluar las propiedades del campo para la suma.

Gráficamente la suma de dos vectores se representa para los puntos como desplazando un punto, 
\begin{align*}
\bmatriz{2\\1}+\bmatriz{0.5\\1}=\bmatriz{2.5\\2}
\end{align*}

\begin{center}
\scalebox{1} % Change this value to rescale the drawing.
{%\psset{xunit=2.0cm,yunit=2.0cm}
\begin{pspicture}(0,-0.5)(3,2)
\rput(0,0){\psaxes[linewidth=0.04](0,0)(0,0)(3,2)}
\psdots[dotsize=0.12](2,1)
\psdots[dotsize=0.12](2.5,2)
\psline[linewidth=0.04cm,linestyle=dashed,dash=0.16cm 0.16cm,tbarsize=0.07055555cm 5.0] {|-|}(2,0.75)(2.5,0.75)
\psline[linewidth=0.04cm,linestyle=dashed,dash=0.16cm 0.16cm,tbarsize=0.07055555cm 5.0] {|-|}(1.75,1)(1.75,2)
\end{pspicture}

}\end{center}

y para las flechas la ley del paralelogramo ilustra la ley conmutativa de la suma \cite[2.1]{NJ99}. 
\begin{align*}
\bmatriz{2\\1}+\bmatriz{0.5\\1}=\bmatriz{0.5\\1}+\bmatriz{2\\1}=\bmatriz{2.5\\2}
\end{align*}



\begin{center}
\scalebox{1} % Change this value to rescale the drawing.
{%\psset{xunit=2.0cm,yunit=2.0cm}
\begin{pspicture}(0,-0.5)(3,2)
\rput(0,0){\psaxes[linewidth=0.04](0,0)(0,0)(3,2)}
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(0,0)(2,1)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(0,0)(0.5,1)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(2,1)(2.5,2)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(0.5,1)(2.5,2)

\psdots[dotsize=0.12](2,1)
\psdots[dotsize=0.12](2.5,2)
\psline[linewidth=0.04cm,linestyle=dashed,dash=0.16cm 0.16cm,tbarsize=0.07055555cm 5.0] {->}(0,0)(2.4,1.9)
%\psline[linewidth=0.04cm,linestyle=dashed,dash=0.16cm 0.16cm,tbarsize=0.07055555cm 5.0] {|-|}(1.75,1)(1.75,2)
\end{pspicture} 

}\end{center}



Debido a que la suma de los escalares es asociativa, conmutativa, modulativa y con opuesto, se tiene que estas mismas propiedades se extienden a la suma de vectores y matrices, en donde el módulo es el vector cero o la matriz cero, \cite[Teorema 2.1.1]{NJ99}.


La suma de matrices $A$, $B$ y $C$ de tamaño $m \times n$ cumple las propiedades:

\teorema{ 
\dctb{ConSumM}{Conmutativa de la suma}{$A+B=B+A$}
\dctb{AsoSumM}{Asociativa de la suma}{$(A+B)+C=A+(B+C)$}
\dctb{ModulM}{Modulativa}{Existe $0_{m \times n}$ tal que  para cada matriz  $A$ se tiene que $A+0_{m \times n}=A$}
%\dctb{OpuestM}{Opuesto}{Para cada una de las matrices $A$ se tiene que existe una matriz $-A$ tal que $A+(-A)=0=(-A)+A$}
\dctb{TraSumM}{La transpuesta de una suma}{$(A+B)^T=A^T+B^T$}}
Las primeras cuatro demostraciones son muy parecidas y consiste en generalizar los axiomas, de la suma, del campo $\R$ a las matrices. La siguiente es la prueba de ConSum de las matrices $A$ y $B$ de $m \times n$. Lo primero que hay que probar es que $size(A+B)=size(B+A)$, lo cual se debe a que al conmutar las matrices los tamaños de los sumandos no varían porque son iguales.
 Lo siguiente es mostrar que cada uno de los elementos de ambas matrices coinciden. 
\begin{align*}
(A+B)_{ij}&=(A)_{ij}+(B)_{ij}&&\text{definición de suma matricial}\\
&=(B)_{ij}+(A)_{ij}&&\text{ConSumR}\\
&=(B+A)_{ij}&&\text{definición de suma matricial}
\end{align*} 

Ver \cite[Pag 155 y 156]{NJ99}

  
\newslide
\section{Operaciones opuesto y resta de matrices y vectores}  
 La \de{matriz opuesta} de $A$ es la matriz $-A$ definida  como la matriz de los opuestos. En resumen

%\defOpMat{Opuesto de una matriz $A$}
%{$-A$ siempre existe}
%{su tamaño es  $size(-A):=size(A)$}
%{sus elementos son  $(-A)_{ij}:=-(A)_{ij}$}  
%{en Scilab se escribe \verb!-A!}
\defOpMat{-A}{Opuesto de una matriz $A$}
{siempre}
{size(A)}
{-(A)_{ij}}  
{}\verb!-A!

El opuesto cumple la siguiente propiedad

\teorema{ 
\dctb{OpuestM}{Opuesto}{Para cada una de las matrices $A$ se tiene que existe una matriz $-A$ tal que $A+(-A)=0=(-A)+A$}}

Gráficamente un la operación opuesta sobre una flecha consiste en cambiarle la dirección 

\begin{align*}
-\bmatriz{2\\1}=\bmatriz{-2\\-1}
\end{align*}


\begin{center}
\scalebox{1} % Change this value to rescale the drawing.
{%\psset{xunit=2.0cm,yunit=2.0cm}
\begin{pspicture}(-2,-1)(2,1)
\rput(0,0){\psaxes[linewidth=0.04](0,0)(-2,-1)(2,1)}
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(0,0)(2,1)
\psline[linewidth=0.04cm,linestyle=dashed,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(0,0)(-2,-1)

%\psline[linewidth=0.04cm,linestyle=dashed,dash=0.16cm 0.16cm,tbarsize=0.07055555cm 5.0] {|-|}(1.75,1)(1.75,2)
\end{pspicture} 

}\end{center}



 
% \[(-A)_{ij}=-(A)_{ij} \text{ y } size(-A)=size(A)\]
% La matriz opuesta siempre existe.  
\newslide
La anterior operación permite definir la \de{resta entre matrices} como $A-B:=A+(-B)$.

\defOpMat{A-B}{Resta de dos matrices $A$ y $B$}
{ si y sólo si  $size(A)=size(B)$}
{size(A)=size(B)}
{(A)_{ij}-(B)_{ij}}
{}\verb!A-B!

%\[(A-B)_{ij}:=(A)_{ij}-(B)_{ij} \text{ y } size(A)_{m \times n}-(B)_{m \times n}(A-B)_{m \times n}\] 

%No es posible sumar o restar matrices de tamaños diferentes. En Scilab se escribe de manera natural \verb!A+B!.


Esta operación cumple la siguiente propiedad útil para despejar.

\teorema{
   \dctb{SumRes}{Un sumando pasa a restar}{Si $A+B=C$ entonces $A=C-B$}
}

\begin{align*}
\bmatriz{2\\1}-\bmatriz{0.5\\1}=-\bmatriz{0.5\\1}+\bmatriz{2\\1}=\bmatriz{1.5\\0}
\end{align*}



\begin{center}
\scalebox{1} % Change this value to rescale the drawing.
{%\psset{xunit=2.0cm,yunit=2.0cm}
\begin{pspicture}(0,-0.5)(3,2)
\rput(0,0){\psaxes[linewidth=0.04](0,0)(0,0)(3,2)}
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(0,0)(2,1)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(0,0)(0.5,1)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(2,1)(2.5,2)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(0.5,1)(2.5,2)

\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(2,1)(1.5,0)


\psdots[dotsize=0.12](2,1)
\psdots[dotsize=0.12](2.5,2)
\psline[linewidth=0.04cm,linestyle=dashed,dash=0.16cm 0.16cm,tbarsize=0.07055555cm 5.0] {->}(0.5,1)(2,1)
\psline[linewidth=0.04cm,linestyle=dashed,dash=0.16cm 0.16cm,tbarsize=0.07055555cm 5.0] {->}(0,0.05)(1.5,0.05)
\end{pspicture} 

}\end{center}



\newslide
\section{Operación escalar por matriz y escalar por vector}
%\subsubsection{Producto de escalar por matriz}


Si $A$ es cualquier matriz y $c$ es cualquier escalar, entonces el \de{producto de escalar por matriz} $cA$ es la matriz obtenida al multiplicar $c$ por cada elemento de $A$. Es un error de notación escribir $Ac$ o $A/c$. %aunque Scilab si interpreta \verb!A*c! y \verb!A/c! como $cA$ y $\frac{1}{c}A$ respectivamente.
Resumiendo

\defOpMat{cA}{Producto de escalar $c$ por matriz $A$}
{siempre}
{size(A)}
{c(A)_{ij}}
{}\verb!c*A!
% \[(cA)_{ij}=c(A)_{ij}\]
%En Scilab se usa el asterisco para el producto escalar \verb!c*A!. 

Es importante que c sea un escalar porque si es una matriz realiza una multiplicación matricial. Aunque en general existen las matrices de $1 \times 1$, hay que tener cuidado porque en Scliab se interpretan como escalares. 



Gráficamente el producto escalar por un vector se representa por otro vector en la misma \ds{dirección} pero diferente \ds{longitud}.

\begin{align*}
1.5\bmatriz{2\\1}=\bmatriz{3\\1.5}
\end{align*}


\begin{center}
\scalebox{1} % Change this value to rescale the drawing.
{%\psset{xunit=2.0cm,yunit=2.0cm}
\begin{pspicture}(0,0)(3,2)
\rput(0,0){\psaxes[linewidth=0.04](0,0)(0,0)(3,2)}
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(0,0)(2,1)
\psline[linewidth=0.04cm,linestyle=dashed,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(0,-0.1)(3,1.4)

%\psline[linewidth=0.04cm,linestyle=dashed,dash=0.16cm 0.16cm,tbarsize=0.07055555cm 5.0] {|-|}(1.75,1)(1.75,2)
\end{pspicture} 

}\end{center}



Dos vectores $\ve{u}$ y $\ve{v}$ son \de{paralelos} si hay un escalar $c$ tal que $\ve{u}=c \ve{v}$. 


A diferencia de la suma de matrices, el producto de escalar por matriz es una operación entre dos conjuntos diferentes, el conjunto de los números reales y el conjunto de las matrices. Por este motivo, las propiedades son algo especiales.%, en particular no se tiene la propiedad conmutativa porque no se va a definir la operación matriz por escalar. 
 Sin embargo, las propiedades de la multiplicación entre escalares se pueden nuevamente extender, de tal forma que la multiplicación escalar por matriz cumple las leyes distributiva, asociativa y además $1\ve{u}=\ve{u}$ y $0\ve{u}=\ve{0}$, \cite[Teorema 2.1.1]{NJ99}. Aunque la multiplicación entre escalares es conmutativa esta no se extrapola ya que no se va a definir la multiplicación de matriz por escalar. 

Las siguientes son algunas propiedades del producto escalar por matriz. Asuma que $B$ y $C$ son matrices de los tamaños apropiados y que $a$ y $b$ son escalares.
 
\teorema{ 
  \dctc{EscCero}{Escalar cero}{$0A=(0)_{m \times n}$}{Esta propiedad es diferente a MulCero, porque los operandos son de diferente tipo}
  \dctc{EscUno}{Escalar uno}{$1A=A$}{Esta propiedad es diferente a Ident, porque los operandos son de diferente tipo} 
  \dctc{EscOpues}{Escalar es opuesto}{$(-a)B=a(-B)=-(aB)$}{Esta propiedad es diferente a MulOpues, porque los operandos son de diferente tipo}
  \dctb{EscDisMat}{Escalar distribuye en suma de matrices}{$a(B+C)=aB+aC$} 
  \dctb{MatDisEsc}{Matriz distribuye en suma de escalares}{$(a+b)C=aC+bC$} 
  \dctb{EscAsoMat}{El producto de escalares es Asociativo con Matriz}{$(ab)C=a(bC)$} 
  \dctb{MatAsoEsc}{El producto de matrices es Asociativo con Escalar}{$a(BC)=(aB)C=B(aC)$} 
  \dctb{TraProEsc}{Transpuesta del producto escalar por matriz}{$(cA)^T=c(A^T)$} 
} 


%\newslide
%\section{Combinación lineal de matrices y vectores}
%Se denomina \de{combinación lineal de matrices} $A_1,A_2,\ldots,A_k$ con coeficientes $c_1,c_2,\ldots,c_k$ a la expresión 
%\[c_1 A_1 + c_2 A_2 + \ldots + c_k A_k. \]
%
%Si las matrices son vectores entonces se denomina \de{combinación lineal de vectores} y esto permite generar RECTAS y  PLANOS.
%
%\newslide
%\section{Matriz por vector columna}
%Sea \[A=[\overline{a_1} \  \overline{a_2} \ \ldots \ \overline{a_n}].\] una matriz de $m \times n$ escrita en sus submatrices columnas y sea 
%\[
%\overline{c}=\begin{bmatrix}
%c_1 \\ c_2 \\ \vdots \\ c_n
%\end{bmatrix}
%\] 
%una columna de $n$  elementos, entonces el \de{producto de matriz por columna}  $A\overline{c}$ es la columna resultante de la combinación lineal 
%\[A\overline{c}:=c_1 \overline{a_1} + c_2 \overline{a_2} + \ldots + c_k \overline{a_k}. \]
%En Scilab se escribe \verb!A*C!.
%
\section{Repaso}
Usted debe estar en capacidad de identificar estos conceptos para una matriz $A$: 
numero de renglones,
número de columnas,
tamaño,
entrada (1,2),
entrada (2,1),
es cero,
es cuadrada,
es diagonal,
es identidad,
es triangular superior,
es triangular inferior,
es un vector columna,
es un vector renglón,
columna 2,
renglón 2,
transpuesta,
es simétrica,
-3A. Además de la operación suma.

ejercicio calcular y dibujar $\frac{1}{2} \bsmatriz{2\\1} - 
\frac{1}{4} \bsmatriz{1\\3}$ 

\newslide
\chapter{Combinación lineal o transformaciones matriciales}
\section{Introducción a la combinación lineal de vectores}

%cambiar eje x,y horizontal vertical

Supongamos que tenemos un robot llamado UALI (la versión Chibcha del de la película) en el espacio lejano (lejos de grandes masas como planetas o estrellas), el cual  tan solo se puede mover en linea recta según el propulsor que tenga encendido, por el momento tiene dos propulsores: el propulsor 1 ($Pro_1$) se encarga de mover al robot a lo largo del eje $y_1$ (a una velocidad de 1cm/s)

\begin{center}
{%\center{}

\begin{pspicture}(-1,-1.0)(2,2)\grilla
\rput(0.5,-0.2){$Pro_1$}
\psline[linewidth=0.04cm,arrowsize=0.05cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(0.0,0.0)(1.0,0)
\end{pspicture}} 
\end{center}


El propulsor 2 ($Pro_2$) mueve al robot a lo largo del eje $y_2$ (a la misma velocidad). 

\begin{center}
\begin{pspicture}(-1,-1.0)(2,2)\grilla
\rput(-0.5,0.5){$Pro_2$}
\psline[linewidth=0.04cm,arrowsize=0.05cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(0.0,0.0)(0,1.0)
\end{pspicture} 
\end{center}


Para mover al robot del origen a las coordenadas $\bsmatriz{2\\3}$ (en cm) hay que ordenarle moverse $2$ (segundos) al propulsor $Pro_1$ y $3$ (segundos) al $Pro_2$ lo que resumiremos como $2 Pro_1 + 3 Pro_2$. 

\begin{center}
\begin{pspicture}(-1,-1.0)(3,4)\grilla
\rput(1,-0.2){$2Pro_1$}
\psline[linewidth=0.04cm,arrowsize=0.05cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(0.0,0.0)(2.0,0)
\rput(2.6,1.5){$3Pro_2$}
\psline[linewidth=0.04cm,arrowsize=0.05cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(2.0,0.0)(2,3.0)
\rput(1,2){\rotatebox{60}{$2Pro_1+3Pro_2$}}
\psline[linewidth=0.04cm,arrowsize=0.05cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(0.0,0.0)(2.0,3)
\end{pspicture} 
\end{center}


%Si se desea pintar sobre una línea vertical basta con mover $Mot_2$. Pero si se desea dibujar sobre una línea a 45 grados hay que accionar los dos motores el mismo tiempo. 
%
%\begin{center}
%\begin{pspicture}(-1,-1.0)(2,2)\grilla
%\rput(1,-0.2){$2Mot_1$}
%\psline[linewidth=0.04cm,arrowsize=0.05cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(0.0,0.0)(1.0,0)
%\rput(1.5,1.5){$3Mot_2$}
%\psline[linewidth=0.04cm,arrowsize=0.05cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(0.0,0.0)(0,1.0)
%\end{pspicture} 
%\end{center}


%En el caso de que las líneas a 45 grados sean muy frecuentes 
Se puede colocar otro propulsor $Pro_3$ que mueve al robot al mismo tiempo en el eje $y_1$ (a una velocidad de 1 cm/s) y en el eje $y_2$ (a una velocidad de 1.5 cm/s),  %con este ángulo (a una velocidad de 1cm/s en cada eje). Ahora hay %3 grados de libertad para mover la pluma y 

\begin{center}
\begin{pspicture}(-1,-1.0)(2,3)\grilla
%\rput(1,-0.2){$2Mot_1$}
%\psline[linewidth=0.04cm,arrowsize=0.05cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(0.0,0.0)(2.0,0)
%\rput(2.6,1.5){$3Mot_2$}
%\psline[linewidth=0.04cm,arrowsize=0.05cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(2.0,0.0)(2,3.0)
\rput(1,2){\rotatebox{60}{$Pro_3$}}
\psline[linewidth=0.04cm,arrowsize=0.05cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(0.0,0.0)(1,1.5)
\end{pspicture} 
\end{center}
de tal forma que si se mueve sólo con $Pro_3$ durante un segundo quedará en las coordenadas  $\bsmatriz{1\\1.5}$. Pero si se mueve dos segundos quedará en las coordenadas  $\bsmatriz{2\\3}$. Ahora hay muchas formas de mover al robot para poder llegar a cualquier punto. %las coordenadas $\bsmatriz{2\\3}$. 

Por ejemplo, un forma para mover al robot 3 cm en el eje $y_2$ consiste en activar solamente $Pro_2$ tres segundos, $3Pro_2$. Otra forma consiste en activar solamente $Pro_1$ y $Pro_3$. ¿Cuanto tiempo hay que activar cada propulsor?. Es de aclarar que un tiempo negativo ($-t$) implica que el propulsor se mueve en la dirección opuesta un tiempo $t$.

\begin{center}
\begin{pspicture}(-1,-1.0)(3,4)\grilla
\rput(1,3.2){$-2Pro_1$}
\psline[linewidth=0.04cm,arrowsize=0.05cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(2.0,3.0)(0.0,3)
%\rput(2.6,1.5){$3Mot_2$}
%\psline[linewidth=0.04cm,arrowsize=0.05cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(2.0,0.0)(2,3.0)
\rput(1,2){\rotatebox{60}{$2Pro_3$}}
\psline[linewidth=0.04cm,arrowsize=0.05cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(0.0,0.0)(2.0,3)
\end{pspicture} 
\end{center}


%
%\begin{center}
%\begin{pspicture}(-1,-1.0)(2,2)\grilla
%\rput(1,-0.2){$2Mot_1$}
%\psline[linewidth=0.04cm,arrowsize=0.05cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(0.0,0.0)(1.0,0)
%\rput(1.5,1.5){$3Mot_2$}
%\psline[linewidth=0.04cm,arrowsize=0.05cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(0.0,0.0)(0,1.0)
%\end{pspicture} 
%\end{center}


\perspectiva{En el capítulo anterior, los vectores representaban puntos de un dibujo. En este capítulo, los vectores representan la velocidad y se dibujan mediante flechas.  Los símbolos matemáticos no distinguen entre puntos o flechas, el significado geométrico o físico lo ponemos los humanos. En este capítulo las columnas de la matriz $A$ usualmente serán vectores de flechas y el vector $\ve{c}$ controla cuanto vamos a `usar' cada una de esas flechas.}



%\section{Operación matriz por vector o  combinación lineal de vectores}\label{sec_vectores}

%La \de{suma de dos vectores} corresponde a la suma de cada una de sus componentes
%\[\bmatriz{a_1\\a_2\\ \vdots \\ a_m }+\bmatriz{b_1\\b_2\\ \vdots \\ b_m }=\bmatriz{a_1+b_1\\a_2+b_2\\ \vdots \\ a_m+b_m }.\]
%Gráficamente se representa con la ley del paralelogramo \cite[2.1]{NJ99}. Debido a que la suma de los escalares es asociativa, conmutativa, modulativa y con opuesto, se tiene que estas mismas propiedades se extienden a la suma de vectores, en donde el módulo de los vectores-$m$  es el vector cero de tamaño $m$, \cite[Teorema 2.1.1]{NJ99}.
% 
%
%
%Un \de{escalar por vector} corresponde  multiplicar un escalar por cada uno de las componentes del vector
%\[c\bmatriz{a_1\\a_2\\ \vdots \\ a_m }=\bmatriz{c a_1\\ c a_2\\ \vdots \\ c a_m }.\]
%Gráficamente se representa por un vector en la misma \ds{dirección} pero diferente \ds{longitud}.
%Dos vectores $\ve{u}$ y $\ve{v}$ son \de{paralelos} si hay un escalar $c$ tal que $\ve{u}=c \ve{v}$. Nuevamente las propiedades de la multiplicación entre escalares se pueden extender de tal forma que la multiplicación escalar por vector cumple las leyes distributiva, asociativa y además $1\ve{u}=\ve{u}$ y $0\ve{u}=\ve{0}$, \cite[Teorema 2.1.1]{NJ99}. Aunque la multiplicación por escalares es conmutativa esta no se extrapola ya que no se define la multiplicación de vector por escalar. 
%


\section{Definición de Combinación lineal}
Una \de{combinación lineal} de los vectores-$m$ $\ve{v_1},\ve{v_2},\ldots,\ve{v_n}$ es de la forma
\[c_1\ve{v_1}+c_2\ve{v_2}+\ldots+c_n\ve{v_n}\]
donde $c_1,c_2, \ldots ,c_n$ son escalares llamados \de{coeficientes de la combinación lineal}. Por ejemplo

\scalebox{1} % Change this value to rescale the drawing.
{\psset{xunit=0.9cm,yunit=0.9cm}
\begin{pspicture}(0,-4.79)(17.02,2)%\grilla
\definecolor{color4}{rgb}{0.6,0.6,0.6}
%%%\rput(2.0,-2.77){\psaxes[linewidth=0.04](0,0)(-2,-2)(4,4)}
%%%\usefont{T1}{ptm}{m}{n}
%%%\rput(5.6028123,-2.26){$x_1$}
%%%\usefont{T1}{ptm}{m}{n}
%%%\rput(2.2028124,1.64){$x_2$}
%%%\psdots[dotsize=0.2](1,0.2)
%\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm](6.0,0.23)(0.0,0.23)
%\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm](6.0,-4.77)(0.0,1.23)
\rput(12.0,-2.77){\psaxes[linewidth=0.04,linecolor=color4](0,0)(-2,-2)(5,4)}
%\psbezier[linewidth=0.04,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(1.0,0.27)(1.0,4.77)(14.0,4.77)(14.0,0.27)
%\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm](17.0,0.27)(10.0,0.27)
%\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm](14.0,1.27)(14.0,-4.73)
\usefont{T1}{ptm}{m}{n}
\rput(8,1.5){$c_1\bmatriz{1 \\0} +c_2\bmatriz{1\\1} = \bmatriz{y_1\\y_2} $}
\rput(8,0){$-1\bmatriz{1 \\0}+3 \bmatriz{1\\1} = \bmatriz{2\\3} $}
%\psbezier[linewidth=0.04,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(2.0,0.27)(4.0,3.77)(13.0,3.87)(15.0,0.27)
%\psbezier[linewidth=0.04,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(3.0,-1.73)(3.0,2.77)(14.0,2.77)(14.0,-1.73)
\usefont{T1}{ptm}{m}{n}
\rput(16.412813,-2.26){$y_1$}
\usefont{T1}{ptm}{m}{n}
\rput(12.312813,1.64){$y_2$}
\psline[linewidth=0.04cm,arrowsize=0.05cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(12.0,-2.8)(13,-2.8)
\psline[linewidth=0.04cm,arrowsize=0.05cm 2.0,arrowlength=1.4,arrowinset=0.4,linestyle=dashed]{>->}(12.0,-2.8)(11,-2.8)
%\psline[linewidth=0.04cm,arrowsize=0.05cm 2.0,arrowlength=1.4,arrowinset=0.4,linestyle=dotted]{>->}(15.0,0.2)(14,0.2)
\psline[linewidth=0.04cm,arrowsize=0.05cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(12.0,-2.8)(13,-1.8)
%\psline[linewidth=0.04cm,arrowsize=0.05cm 2.0,arrowlength=1.4,arrowinset=0.4,linestyle=dotted]{>->}(12.0,-2.8)(15,0.2)
\psline[linewidth=0.04cm,arrowsize=0.05cm 2.0,arrowlength=1.4,arrowinset=0.4,linestyle=dashed]{>->}(11.0,-2.8)(14,0.2)
\psdots[dotsize=0.2](14,0.2)
\end{pspicture} 
}

Gráficamente la combinación lineal se puede ver como una sucesión de flechas como se muestra en \cite[Figura 2.5, pg 67]{NJ99}


Se sugiere estudiar de la sección \cite[2.1]{NJ99} y realizar los ejercicios 1,3,5,6 y 7.


%\section{Notación matricial de la combinación lineal}

%Una \de{matriz} de \de{tamaño} $m \times n$ es de la forma 
%\begin{align*}
%A
%&=\begin{bmatrix}
%a_{11} & a_{12} & \cdots & a_{1n} \\
%a_{21} & a_{22} & \cdots & a_{2n} \\
%\vdots&\vdots&\ddots&\vdots\\
%a_{m1} & a_{m2} & \cdots & a_{mn}  \\	
%\end{bmatrix}\\
%\intertext{y se puede ver como un vector-$n$ (renglón) de vectores-$m$}
%A&=[\ve{v_1} \ \ve{v_2} \ \ldots \ \ve{v_n}]
%\end{align*}
%en donde 
%
%\[\ve{v_1}=\bmatriz{a_{11}\\a_{21}\\ \vdots \\ a_{m1} },
%\ve{v_2}=\bmatriz{a_{12}\\a_{22}\\ \vdots \\ a_{m2} },
%\ldots,
%\ve{v_n}=\bmatriz{a_{1n}\\a_{2n}\\ \vdots \\ a_{mn} }
%\]
%
%La suma, el producto por escalar, la igualdad de matrices y la matriz cero se pueden inferir de las respectivas definiciones de los vectores.
%\begin{align*}
%[\ve{v_1} \ \ve{v_2} \ \ldots \ \ve{v_n}]+[\ve{u_1} \ \ve{u_2} \ \ldots \ \ve{u_n}]&=[\ve{v_1}+\ve{u_1} \ \ \ve{v_2}+\ve{u_2} \ \ \ldots \ \ \ve{v_n}+\ve{u_n}]\\
%c[\ve{v_1} \ \ve{v_2} \ \ldots \ \ve{v_n}]&=[c \ve{v_1} \ c \ve{v_2} \ \ldots \ c \ve{v_n}]\\
%[\ve{v_1} \ \ve{v_2} \ \ldots \ \ve{v_n}]=[\ve{u_1} \ \ve{u_2} \ \ldots \ \ve{u_n}] &\Ra [\ve{v_1}=\ve{u_1} \ \ \ve{v_2}=\ve{u_2} \ \ \ldots \ \ \ve{v_n}=\ve{u_n}]\\
%\ve{\ve{0}}&=[\ve{0} \ \ve{0} \ \ldots \ \ve{0} ] 
%\end{align*}
%
%Además, se extrapolan las propiedades de los vectores, \cite[Teorema 3.1.1]{NJ99}. 
%
%La \de{transpuesta} de una matriz $A$ intercambia los renglones por las columnas, la operación de \de{transposición} de denota $A'$. Una matriz $A$ es \de{simétrica} si es igual a su transpuesta, $A=A'$.  En \cite[Teorema 3.2.9]{NJ99} se presentan algunas propiedades de la transposición.
%
%
\section{Operación matriz por vector}
Usando la notación matricial, la combinación lineal se denota como el producto Matriz por vector.
%\[c_1\ve{v_1}+c_2\ve{v_2}+\ldots+c_k\ve{v_k}=
%c_1\bmatriz{a_{11}\\a_{21}\\ \vdots \\ a_{m1} }+
%c_2\bmatriz{a_{12}\\a_{22}\\ \vdots \\ a_{m2} }+
%\ldots +
%c_n\bmatriz{a_{1n}\\a_{2n}\\ \vdots \\ a_{mn} }
%\]
%se puede escribir como 
%\[[\ve{v_1} \ \ve{v_2} \ \ldots \ \ve{v_n}]\bmatriz{c_1\\c_2\\ \vdots \\ c_m }=\begin{bmatrix}
%a_{11} & a_{12} & \cdots & a_{1n} \\
%a_{21} & a_{22} & \cdots & a_{2n} \\
%\vdots&\vdots&\ddots&\vdots\\
%a_{m1} & a_{m2} & \cdots & a_{mn}  \\	
%\end{bmatrix}
%\bmatriz{c_1\\c_2\\ \vdots \\ c_m }
%\]
%es decir
\begin{align*}
\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots&\vdots&\ddots&\vdots\\
a_{m1} & a_{m2} & \cdots & a_{mn}  \\	
\end{bmatrix}
\bmatriz{c_1\\c_2\\ \vdots \\ \vdots \\ c_n }
:=&c_1\bmatriz{a_{11}\\a_{21}\\ \vdots \\ a_{m1} }+
c_2\bmatriz{a_{12}\\a_{22}\\ \vdots \\ a_{m2} }+
\ldots +
c_n\bmatriz{a_{1n}\\a_{2n}\\ \vdots \\ a_{mn} }\\
=&\begin{bmatrix}
a_{11} c_1 + a_{12} c_2 + \cdots + a_{1n} c_n \\
a_{21} c_1 + a_{22} c_2 + \cdots + a_{2n} c_n \\
\vdots\\%&\vdots&\ddots&\vdots\\
a_{m1} c_1 + a_{m2} c_2 + \cdots + a_{mn}  c_n \\	
\end{bmatrix}
\intertext{que es equivalente a }
A \ve{c}:=&c_1\ve{v_1}+c_2\ve{v_2}+\cdots+c_n\ve{v_n}
\end{align*}
en donde
\[A=\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots&\vdots&\ddots&\vdots\\
a_{m1} & a_{m2} & \cdots & a_{mn}  \\	
\end{bmatrix}, 
\ve{c}=\bmatriz{c_1\\c_2\\ \vdots \\ \vdots \\ c_n },
\ve{v_i}=\bmatriz{a_{1i}\\a_{2i}\\ \vdots \\ a_{mi} },
i \in \{1,2,\ldots,n\}\]

Al vector $\ve{c}$ algunos autores lo llaman \de{valor} y al resultado $A\ve{c}$ lo llaman \de{covalor}. A continuación se muestra el mismo ejemplo anterior escrito como producto de matriz por vector.


\scalebox{1} % Change this value to rescale the drawing.
{\psset{xunit=0.9cm,yunit=0.9cm}
\begin{pspicture}(0,-4.79)(17.02,2)%\grilla
\definecolor{color4}{rgb}{0.6,0.6,0.6}
\rput(2.0,-2.77){\psaxes[linewidth=0.04](0,0)(-2,-2)(4,4)}
\usefont{T1}{ptm}{m}{n}
\rput(5.6028123,-2.26){$x_1$}
\usefont{T1}{ptm}{m}{n}
\rput(2.2028124,1.64){$x_2$}
\psdots[dotsize=0.2](1,0.2)
%\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm](6.0,0.23)(0.0,0.23)
%\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm](6.0,-4.77)(0.0,1.23)
\rput(12.0,-2.77){\psaxes[linewidth=0.04,linecolor=color4](0,0)(-2,-2)(5,4)}
%\psbezier[linewidth=0.04,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(1.0,0.27)(1.0,4.77)(14.0,4.77)(14.0,0.27)
%\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm](17.0,0.27)(10.0,0.27)
%\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm](14.0,1.27)(14.0,-4.73)
\usefont{T1}{ptm}{m}{n}
\rput(8,1.5){${\bmatriz{1 & 1\\0&1}} \bmatriz{c_1\\c_2} = \bmatriz{y_1\\y_2} $}
\rput(8,0){${\bmatriz{1 & 1\\0&1}} \bmatriz{-1\\3} = \bmatriz{2\\3} $}
%\psbezier[linewidth=0.04,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(2.0,0.27)(4.0,3.77)(13.0,3.87)(15.0,0.27)
%\psbezier[linewidth=0.04,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(3.0,-1.73)(3.0,2.77)(14.0,2.77)(14.0,-1.73)
\usefont{T1}{ptm}{m}{n}
\rput(16.412813,-2.26){$y_1$}
\usefont{T1}{ptm}{m}{n}
\rput(12.312813,1.64){$y_2$}
\psline[linewidth=0.04cm,arrowsize=0.05cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(12.0,-2.8)(13,-2.8)
\psline[linewidth=0.04cm,arrowsize=0.05cm 2.0,arrowlength=1.4,arrowinset=0.4,linestyle=dashed]{>->}(12.0,-2.8)(11,-2.8)
%\psline[linewidth=0.04cm,arrowsize=0.05cm 2.0,arrowlength=1.4,arrowinset=0.4,linestyle=dashed]{>->}(15.0,0.2)(14,0.2)
\psline[linewidth=0.04cm,arrowsize=0.05cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(12.0,-2.8)(13,-1.8)
%\psline[linewidth=0.04cm,arrowsize=0.05cm 2.0,arrowlength=1.4,arrowinset=0.4,linestyle=dashed]{>->}(12.0,-2.8)(15,0.2)
\psline[linewidth=0.04cm,arrowsize=0.05cm 2.0,arrowlength=1.4,arrowinset=0.4,linestyle=dashed]{>->}(11.0,-2.8)(14,0.2)
\psdots[dotsize=0.2](14,0.2)
%\psbezier[linewidth=0.04,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4,linestyle=dotted]{>->}(1.0,0.27)(1.0,1)(14.0,1)(14.0,0.27)
\end{pspicture} 
}


Resumiendo

\defOpMat{A\ve{c}}{Producto de matriz $A$ por vector-$n$ $\ve{c}$}
{si el número de columnas de $A$ es igual a $n$ que es el tamaño del vector $\ve{c}$}
{m \times 1, \text{donde $m$ el número de renglones de $A$}}
{c_1(A)_{i1}+c_2(A)_{i2}+\cdots+c_n(A)_{in} \text{ donde $j=1$}}
{}\verb!A*c!

Las propiedades de matriz por vector serán un caso particular de las propiedades de la multiplicación entre matrices que se presentan más adelante.

Se sugiere estudiar de la sección \cite[2.5]{NJ99} las paginas 104 a 106 y realizar los ejercicios 1,2,11,12,13. %Además, de la sección \cite[3.1]{NJ99} las paginas 154 a 157 y realice los ejercicios 1 al 9.

\
%\chapter{Multiplicación Matricial}

\section{Multiplicación matricial como muchas combinaciones lineales}
%--------------------Una matriz no siempre se asocia a un a transformación, por ejemplo podemos definir las columnas de una 

Supongamos que las columnas de la matriz $B=[\ve{p_1} \ \ve{p_2} \ \ldots \ \ve{p_n}]$ corresponde los puntos de un dibujo. Y sea $A$ la una matriz con la cual se va a transformar cada uno de los puntos. Entonces se define 
%desea hacer la transformación de cada uno de los puntos Pero para cada uno de esos puntos si es posible transformarlos con la transformación $T_A$ dada por la matriz $A$, entonces
%Las anteriores transformaciones matriciales usualmente no se aplican a un punto aislado sino a una serie de puntos que representan un dibujo, los cuales corresponden a las columnas de una matriz. 
%\[B=[\ve{v_1} \ \ve{v_2} \ \ldots \ \ve{v_n}] \]
%Para realizar una transformación matricial $A$ al dibujo, es necesario realizar la transformación matricial de cada vector, 
 la \de{multiplicación matricial} de la siguiente forma. 

\[AB=A[\ve{p_1} \ \ve{p_2} \ \ldots \ \ve{p_n}]:=[A\ve{p_1} \ A\ve{p_2} \ \ldots \ A\ve{p_n}]\] 


%Sumando a la primera componente un múltiplo de la segunda
\[\begin{bmatrix}
1 & 1  \\
0 & 1  
\end{bmatrix}
\bmatriz{
-1 &-1.5&-1  \\
0.5&0   &-0.5
}=
%\bmatriz{
%1 & 1  \\
%0 & 1  }
%\bmatriz{
%\bmatriz{
%1 & 1  \\
%0 & 1  }
%\bmatriz{-1 \\0.5}
%&
%\bmatriz{
%1 & 1  \\
%0 & 1  }
%\bmatriz{-1.5\\ 0}   
%&
%\bmatriz{
%1 & 1  \\
%0 & 1  }
%\bmatriz{-1  \\-0.5}
%}=
\bmatriz{
-0.5 &-1.5&-1.5  \\
0.5&0   &-0.5
}
\]
		\includegraphics[scale=0.5,viewport=0 90 600 300,clip]{cicla_2_rap_a.eps}



En resumen

\defOpMat{AB}{Producto de la matriz $A \in \R^{m \times r}$ por la matriz $B \in \R^{r \times n}$}
{si el número de columnas de $A$ es igual $r$ que es el número de renglones de $B$}
{m \times n}%, \text{donde $m$ es el número de renglones de} \text{$A$ y $n$ es el número de columnas de $B$}}
{(A)_{i1}(B)_{1j}+(A)_{i2}(B)_{2j}+\cdots+(A)_{ir}(B)_{rj} }
{}\verb!A*B!



%Sea $A$ una matriz de $m \times r$ y $B=[\overline{b_1} \ \overline{b_2} \ \ldots \ \overline{b_n} ]$ una matriz de $r \times n$ escrita es submatrices columnas, entonces el producto entre dos matrices $AB$ es la matriz $m \times n$ formada al encadenar los productos de la matriz $A$ por cada una de las columnas $B_i$ 
%\[AB=[A \overline{b1} \ A \overline{b_2} \ \ldots \ A \overline{b_n}]\]
%En Scilab se escribe \verb!A*B!.
%Una pregunta frecuente es porque no se define el producto de matrices al estilo de la suma de matrices, elemento a elemento, la razon es porque no se utiliza en esta materia. %Sin embargo, en Scilab se escribe el producto elemento a elemento \verb!A.*B!. 

%--------colocar la imagen de cada transformación.


La multiplicación matricial no es conmutativa pero si es asociativa y distributiva. Además la multiplicación por una matriz cero da cero y por la matriz identidad da la misma. \cite[Teorema 3.1.2]{NJ99}
%Como el producto de matriz por vector se puede ver como un caso particular de la multiplicación entre matrices, a continuación sólo vamos a hablar de la multiplicación entre matrices.
Sin embargo, es de resaltar que la multiplicación de matrices no está definida entre cualquier par de matrices. %no se puede construir un álgebra ni siquiera para un conjunto de matrices de un tamaño determinado, amenos que las matrices sean cuadradas donde si se cumple la propiedad ClaMul. 
Para las siguientes propiedades se asumen que las matrices $A$, $B$ y $C$ tienen los tamaños adecuados para poder realizar las operaciones y no necesariamente son cuadradas.


% La multiplicación de matrices cumple las propiedades:

\teorema{ 
  \dctb{AsoMul}{Asociativa de la mult.}{$(A B) C=A (B C)$}
  \dctc{Ident}{Identidad}{Existe la matriz $I_n$ tal que  para cada matriz $A$ cuadrada de orden $n$ se tiene que $A  I_n =A=A I_n$}{Esta propiedad se cumple al restringimos a matrices cuadradas de orden $n$}
 \dctb{DistrD}{Distributiva a derecha}{$(B + C)  A=(B  A) + (C A)$}
 \dctb{DistrI}{Distributiva a izquierda}{$A (B + C)=(A B) + (A C)$}
% La matrices también cumplen las propiedades
   \dctb{MulCero}{Multiplicación por cero}{$0 A = 0 = A 0$}
   \dctb{MulOpues}{Multiplicar por opuesto}{$(-B) A = -(B A)$}
  \dctb{TraMul}{Transpuesta de la multiplicación de matrices}{$(AB)^T=B^TA^T$} 
}

%\newslide
  Las demostraciones de las propiedades de la multiplicación de matrices son más interesantes que las de la suma de matrices. A continuación se presenta la demostración de DistrI (distributiva a izquierda) de las matrices $A$ de $m \times r$ y $B, C$ de $r \times n$, la cual se presenta en \cite{Ant06}[demo. Teorema 1.4.1.d]. Lo primero es demostrar que los tamaños de las matrices son iguales  $size(A(B+C))=size(AB+AC)$.
 \begin{align*} 
 &size(A_{m \times r}(B_{r \times n}+C_{r \times n}))\\&=size(A_{m \times r}(B+C)_{r \times n})&&\text{def. suma matricial}
 \\&=size((A(B+C))_{m \times n})&&\text{def. mult. matricial}
 \intertext{es $m \times n$, además}
 &size(A_{m \times r}B_{r \times n}+A_{m \times r}C_{r \times n}))\\&=size((AB)_{m \times n}+(AC)_{m \times n})&&\text{def. mult matricial}
 \\&=size((AB+AC)_{m \times n})&&\text{def. suma matricial}
 \intertext{que también es $m \times n$. Ahora hay que probar que cada uno de los elementos de la matrices son iguales.}
\end{align*}
\begin{align*}
&(AB+AC)_{ij}\\
&=\{(AB)_{ij}\}+\{(AC)_{ij}\}&&\text{def. de suma matricial}\\
&=\{(A)_{i1}(B)_{1j}+(A)_{i2}(B)_{2j}+ \cdots +(A)_{ir}(B)_{rj}\}\\
&\ \ +\{(A)_{i1}(C)_{1j}+(A)_{i2}(C)_{2j}+ \cdots +(A)_{ir}(C)_{rj}\}&&\text{def. de mult. matricial}\\
&=\{(A)_{i1}(B)_{1j}+(A)_{i1}(C)_{1j}\}\\
&\ \ +\{(A)_{i2}(B)_{2j}+(A)_{i2}(C)_{2j}\}\\ 
&\ \ + \cdots\\ 
&\ \ +\{(A)_{ir}(B)_{rj}+(A)_{ir}(C)_{rj}\} &&\text{ConSum del campo $\R$}\\
&=\{(A)_{i1}((B)_{1j}+(C)_{1j})\}\\
&\ \ +\{(A)_{i2}((B)_{2j}+(C)_{2j})\}\\ 
&\ \ + \cdots\\ 
&\ \ +\{(A)_{ir}((B)_{rj}+(C)_{rj})\} &&\text{Distr del campo $\R$}\\
&=\{(A)_{i1}(B+C)_{1j}\}\\
&\ \ +\{(A)_{i2}(B+C)_{2j}\}\\ 
&\ \ + \cdots\\ 
&\ \ +\{(A)_{ir}(B+C)_{rj})\} &&\text{def. suma matricial}\\
&=(A(B+C))_{ij}&&\text{def mult. matricial}
 \end{align*}
 
 Es de resaltar que la multiplicación de matrices en general no es conmutativa, pero se pueden encontrar \de{matrices que conmutan}, %en particular las matriz identidad ($I$) y la matriz de ceros ($0$) conmutan con las matrices con las que se puedan multiplicar. %De la propiedad Ivers se hablará más adelante. 
El producto de matriz por columna se puede ver como un caso particular de la multiplicación de matrices.


\section{Potencia positiva de una matriz} \label{PotN}
Para una matriz cuadrada $A$ se definen las potencias: $A^0=I$, para $n \in \N_1$, $A^n=AA \ldots A$ $n$ veces 

Las siguientes son algunas propiedades de las potencias. Asumiendo que $A$ es cuadrada, que $c$ es un escalar distinto se cero y que $r,s \in \N_0$, se tienen las siguientes propiedades.

\teorema{
  \dctb{SumaExpN}{Suma de Exponentes}{$A^r A^s=A^{r+s}$}
  \dctb{ExpExpN}{Producto de exponentes}{$((A)^{r})^s=A^{rs}$}
  \dctb{ExpEscMatN}{El escalar sale con exponente}{$(cA)^{r}=c^{r}(A)^{r}$}
}




Se sugiere estudiar de la sección \cite[3.1]{NJ99} las páginas  160 en adelante y realizar los ejercicios del 10 en adelante. 


\section{Transformación matricial}

Vimos que la operación matriz por vector ($A \ve{x}$) transforma un sólo vector. Además vimos como la multiplicación matricial ($A[ \ve{v_1}\ \ve{v_2}\ \ldots \ve{v_n}\ ]$) puede transformar muchos vectores al tiempo. Ahora vamos a centrarnos en como la matriz $A$ transforma. Es decir podemos inferir que hace la matriz $A$ sin importar si los puntos que transforma son de una bicicleta, una casa o un carro.


Dado un vector-$n$ fijo $\ve{c}$ (no variable) la combinación lineal $c_1 \ve{v_1} + c_2 \ve{v_2} + \cdots + c_n \ve{v_n}$ o en su respectiva operación matriz por vector generan un vector-$m$. 
Pero si $\ve{x}$ es un vector-$n$ variable la combinación lineal $x_1 \ve{v_1}+x_2 \ve{v_2}+ \cdots +x_n \ve{v_n}$ o su respectiva operación matriz por vector
$A_{m \x n} \ve{x}$ % si el vector $\ve{x}$ es variable entonces  %de $n$ vectores-$m$ 
%\[\begin{bmatrix}
%a_{11} & a_{12} & \cdots & a_{1n} \\
%a_{21} & a_{22} & \cdots & a_{2n} \\
%\vdots&\vdots&\ddots&\vdots\\
%a_{m1} & a_{m2} & \cdots & a_{mn}  \\	
%\end{bmatrix}
%\bmatriz{c_1\\c_2\\ \vdots \\ \vdots \\ c_n }\]
se pueden ver como una trasformación del espacio $\R^n$ (llamado \de{dominio}) al espacio $\R^m$ (llamado \de{codominio}), 
\begin{align*}
T_A:\R^n &\ra \R^m
\intertext{ dada por }
 T_A \parr{
\bmatriz{x_1\\x_2\\ \vdots \\ \vdots \\ x_n }}
%A\ve{c}%=T\left(\bmatriz{c_1\\c_2\\ \vdots \\ \vdots \\ c_n }\right)
&=\begin{bmatrix}
a_{11} x_1 + a_{12} x_2 + \cdots + a_{1n} x_n \\
a_{21} x_1 + a_{22} x_2 + \cdots + a_{2n} x_n \\
\vdots\\%&\vdots&\ddots&\vdots\\
a_{m1} x_1 + a_{m2} x_2 + \cdots + a_{mn}  x_n \\	
\end{bmatrix}
\end{align*}
%\[ T_A(\ve{c})=\begin{bmatrix}
%a_{11} & a_{12} & \cdots & a_{1n} \\
%a_{21} & a_{22} & \cdots & a_{2n} \\
%\vdots&\vdots&\ddots&\vdots\\
%a_{m1} & a_{m2} & \cdots & a_{mn}  \\	
%\end{bmatrix}
%\bmatriz{c_1\\c_2\\ \vdots \\ \vdots \\ c_n }
%%A\ve{c}%=T\left(\bmatriz{c_1\\c_2\\ \vdots \\ \vdots \\ c_n }\right)
%=\begin{bmatrix}
%a_{11} c_1 + a_{12} c_2 + \cdots + a_{1n} c_n \\
%a_{21} c_1 + a_{22} c_2 + \cdots + a_{2n} c_n \\
%\vdots\\%&\vdots&\ddots&\vdots\\
%a_{m1} c_1 + a_{m2} c_2 + \cdots + a_{mn}  c_n \\	
%\end{bmatrix}
%\]

Recordemos que este resultado corresponde a \[T_A(\ve{x})=A\ve{x}\]

(ver \cite[Fig 5.2]{NJ99}). Si $T_A(\ve{x})=\ve{y}$ algunos autores llaman  $\ve{x}$ el \de{valor} y $\ve{y}$  el \de{covalor}.

Retomando el ejemplo anterior, $T_A$ transforma el vector $\ve{x}$ en  $\ve{y}=T_A(\ve{x})$.




\scalebox{1} % Change this value to rescale the drawing.
{\psset{xunit=0.9cm,yunit=0.9cm}
\begin{pspicture}(0,-4.79)(17.02,2)%\grilla
\definecolor{color4}{rgb}{0.6,0.6,0.6}
\rput(2.0,-2.77){\psaxes[linewidth=0.04](0,0)(-2,-2)(4,4)}
\usefont{T1}{ptm}{m}{n}
\rput(5.6028123,-2.26){$x_1$}
\usefont{T1}{ptm}{m}{n}
\rput(2.2028124,1.64){$x_2$}
\psdots[dotsize=0.2](1,0.2)
%\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm](6.0,0.23)(0.0,0.23)
%\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm](6.0,-4.77)(0.0,1.23)
\rput(12.0,-2.77){\psaxes[linewidth=0.04,linecolor=color4](0,0)(-2,-2)(4,4)}
%\psbezier[linewidth=0.04,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(1.0,0.27)(1.0,4.77)(14.0,4.77)(14.0,0.27)
%\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm](17.0,0.27)(10.0,0.27)
%\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm](14.0,1.27)(14.0,-4.73)
\usefont{T1}{ptm}{m}{n}
\rput(8,1.5){${\bmatriz{1(x_1) + 1(x_2)\\0(x_1)+1(x_2)}}  = \bmatriz{y_1\\y_2} $}
\rput(8,0){${\bmatriz{1(-1) + 1(3)\\0(-1)+1(3)}}  = \bmatriz{2\\3} $}
%\psbezier[linewidth=0.04,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(2.0,0.27)(4.0,3.77)(13.0,3.87)(15.0,0.27)
%\psbezier[linewidth=0.04,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(3.0,-1.73)(3.0,2.77)(14.0,2.77)(14.0,-1.73)
\usefont{T1}{ptm}{m}{n}
\rput(16.412813,-2.5){$y_1$}
\usefont{T1}{ptm}{m}{n}
\rput(12.312813,1.64){$y_2$}
\psline[linewidth=0.04cm,arrowsize=0.05cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(12.0,-2.8)(13,-2.8)
%\psline[linewidth=0.04cm,arrowsize=0.05cm 2.0,arrowlength=1.4,arrowinset=0.4,linestyle=dashed]{>->}(12.0,-2.8)(11,-2.8)
%\psline[linewidth=0.04cm,arrowsize=0.05cm 2.0,arrowlength=1.4,arrowinset=0.4,linestyle=dashed]{>->}(15.0,0.2)(14,0.2)
\psline[linewidth=0.04cm,arrowsize=0.05cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(12.0,-2.8)(13,-1.8)
%\psline[linewidth=0.04cm,arrowsize=0.05cm 2.0,arrowlength=1.4,arrowinset=0.4,linestyle=dashed]{>->}(12.0,-2.8)(15,0.2)
%\psline[linewidth=0.04cm,arrowsize=0.05cm 2.0,arrowlength=1.4,arrowinset=0.4,linestyle=dashed]{>->}(11.0,-2.8)(14,0.2)
\psdots[dotsize=0.2](14,0.2)

\psline[linestyle=dashed](0,-4.8)(0,1.2)
\psline[linestyle=dashed](2.05,-4.8)(2.05,1.2)
\psline[linestyle=dashed](4,-4.8)(4,1.2)
\psline[linestyle=dashed](6,-4.8)(6,1.2)

\psline[linestyle=dotted](0,-4.8)(6,-4.8)
\psline[linestyle=dotted](0,-2.8)(6,-2.8)
\psline[linestyle=dotted](0,-0.8)(6,-0.8)
\psline[linestyle=dotted](0,1.2)(6,1.2)

\psline[linestyle=dashed](14,-4.8)(16,-2.8)
\psline[linestyle=dashed](12,-4.8)(16,-0.8)
\psline[linestyle=dashed](10,-4.8)(16,1.2)
\psline[linestyle=dashed](10,-2.8)(14,1.2)
\psline[linestyle=dashed](10,-0.8)(12,1.2)

\psline[linestyle=dotted](10,-4.8)(16,-4.8)
\psline[linestyle=dotted](10,-2.8)(16,-2.8)
\psline[linestyle=dotted](10,-0.8)(16,-0.8)
\psline[linestyle=dotted](10,1.2)(16,1.2)

\usefont{T1}{ptm}{m}{n}
\rput(16,-2){$'T(x_1)'$}
\usefont{T1}{ptm}{m}{n}
\rput(15.7,0.64){$'T(x_2)'$}


\end{pspicture} 
}

En la gráfica anterior se observa como se transforman los eje $x_1$ y $x_2$







Como la operación de matriz por vector $A \ve{x}$ da un vector $\ve{y}$, entonces podemos ver esta operación como que $A$ transforma a $\ve{x}$ en $\ve{y}$. A la izquierda presentamos el espacio de los $\ve{x}$ y a la derecha el espacio de los $\ve{y}$.

%\perspectiva{
Hasta el momento tenemos tres representaciones equivalentes: 
\begin{itemize}
\item combinación lineal $-1\bmatriz{1 \\0}+3 \bmatriz{1\\1}$,
\item operación matriz por vector ${\bmatriz{1 & 1\\0&1}} \bmatriz{-1\\3}$ y
\item transformación matricial $\bmatriz{1(x_1) + 1(x_2)\\0(x_1)+1(x_2)}$. 
\end{itemize}
De esta forma si, por ejemplo, nos referirnos a una combinación lineal (por ejemplo, $-1\bsmatriz{1 \\0}+3 \bsmatriz{1\\1}$) como una operación matriz por vector estamos haciendo referencia a la única operación matriz por vector correspondiente (por ejemplo, ${\bsmatriz{1 & 1\\0&1}} \bsmatriz{-1\\3}$). Por otro lado, en una transformación matricial $T_A(\ve{x})=\ve{y}$ asumimos que los vectores $\ve{x}$ y $\ve{y}$ son variables y $A$ es una matriz de escalares. Por eso una transformación matricial se representa 
\[T_A(\ve{x})=\begin{bmatrix}
a_{11} x_1 + a_{12} x_2 + \cdots + a_{1n} x_n \\
a_{21} x_1 + a_{22} x_2 + \cdots + a_{2n} x_n \\
\vdots\\%&\vdots&\ddots&\vdots\\
a_{m1} x_1 + a_{m2} x_2 + \cdots + a_{mn}  x_n \\	
\end{bmatrix}   
\text{ asociada a la matriz }
A=\begin{bmatrix}
a_{11}  & a_{12}  & \cdots & a_{1n}  \\
a_{21}  & a_{22}  & \cdots & a_{2n}  \\
\vdots&\vdots&\ddots&\vdots\\
a_{m1}  & a_{m2}  & \cdots & a_{mn}   \\	
\end{bmatrix}\]   
\[\text{Por ejemplo, } T_A(\ve{x})=\bsmatriz{1(x_1) + 1(x_2)\\0(x_1)+1(x_2)} \text{ está asociado a } A=\bsmatriz{1 & 1\\0&1}\].
Por lo tanto cada transformación matricial está  asociado a una (y sólo una) matriz (o a un conjunto de vectores-$m$). Por lo tanto si nos referimos a una matriz como una transformación matricial, estamos haciendo referencia a la transformación asociada a esa matriz.

 
 
\section{Transformaciones de matrices elementales}

A continuación veremos algunas combinaciones lineales de \de{matriciales elementales}, las cuales consisten en una matriz identidad a la cual se le ha realizdo una (y sólo una) de las siguientes tres operaciones matriciales.% $\bsmatriz{1&0\\0&1}$
\begin{itemize}
\item ($eR_i \ra R_i$) Multiplicar los elementos de un renglón por un escalar $e$.
\item ($R_i \lra R_j$) Intercambiar dos renglones.
\item ($R_i+eR_j \ra R_i$) Sumar un múltiplo escalar $e$ de un renglón a otro renglón.
\end{itemize}



\begin{description}
\item[Identidad] 

%\[\begin{bmatrix}
%1 & 0  \\
%0 & 1  
%\end{bmatrix}
%\bmatriz{x_1\\y_1}
%=x_1\bmatriz{1\\0}
%+y_1\bmatriz{0\\1}
%=\bmatriz{x_1\\0}
%+\bmatriz{0\\y_1}
%=\bmatriz{x_1\\y_1}
%%=\bmatriz{x_2\\y_2}
%\] 
La combinación lineal de dos vectores, uno en el eje $y_1$ $\ve{v_1}=\bsmatriz{1\\0}$ y otro en el eje $y_2$ $\ve{v_2}=\bsmatriz{0\\1}$, está dada por 
%\[c_1 \bmatriz{1\\0} + c_2 \bmatriz{0\\1} \text{, ó } \bmatriz{1&0\\0&1}\bmatriz{c_1\\c_2} \]
%Como $A=\bsmatriz{1&0\\0&1}$ entonces 
\[\bmatriz{1&0\\0&1}\bmatriz{c_1\\c_2}=c_1 \bmatriz{1\\0} + c_2 \bmatriz{0\\1}= \bmatriz{c_1\\0} + \bmatriz{0\\c_2} = \bmatriz{c_1\\c_2}\]
En este caso da el mismo vector $\ve{c}$, por este motivo la matriz $\bsmatriz{1&0\\0&1}$ se llama la matriz identidad de $2 \times 2$.


\[\begin{bmatrix}
1 & 0  \\
0 & 1  
\end{bmatrix}
\bmatriz{
-1 &-1.5&-1  \\
0.5&0   &-0.5
}=
\bmatriz{
-1 &-1.5&-1  \\
0.5&0   &-0.5
}
\]
		\includegraphics[scale=0.5,viewport=0 90 600 300,clip]{cicla_2_id.eps}


\item[Escalamiento del primer componente] con $k=2$
\[\begin{bmatrix}
k & 0  \\
0 & 1  
\end{bmatrix}
\bmatriz{x_1\\x_2}
=x_1\bmatriz{k \\0}
+x_2\bmatriz{0\\1}
=\bmatriz{k \ x_1\\x_2}
%=\bmatriz{x_2\\y_2}
\]



\[\begin{bmatrix}
2 & 0  \\
0 & 1  
\end{bmatrix}
\bmatriz{
-1 &-1.5&-1  \\
0.5&0   &-0.5
}=
\bmatriz{
-2 &-3&-2  \\
0.5&0   &-0.5
}\]
		\includegraphics[scale=0.5,viewport=0 90 650 300,clip]{cicla_2_2x.eps}


\item[Escalamiento del segundo componente] con $k=0.5$

\[\begin{bmatrix}
1 & 0  \\
0 & k  
\end{bmatrix}
\bmatriz{x_1\\x_2}
=\bmatriz{x_1\\k \ x_2}
%=\bmatriz{x_2\\y_2}
\]

\[\begin{bmatrix}
1 & 0  \\
0 & 0.5  
\end{bmatrix}
\bmatriz{
-1 &-1.5&-1  \\
0.5&0   &-0.5
}=
\bmatriz{
-1 &-1.5&-1  \\
0.25&0   &-0.25
}
\]
		\includegraphics[scale=0.5,viewport=0 90 600 300,clip]{cicla_2_5y.eps}


\item[Intercambio de componentes]

\[\begin{bmatrix}
0 & 1  \\
1 & 0  
\end{bmatrix}
\bmatriz{x_1\\x_2}
=\bmatriz{x_2\\ x_1}
%=\bmatriz{x_2\\y_2}
\]

\[\begin{bmatrix}
0 & 1  \\
1 & 0  
\end{bmatrix}
\bmatriz{
-1 &-1.5&-1  \\
0.5&0   &-0.5
}=
\bmatriz{
0.5&0   &-0.5 \\
-1 &-1.5&-1  
}
\]
		\includegraphics[scale=0.5,viewport=0 90 600 300,clip]{cicla_2_inv.eps}

\item[Sumando a la primera componente un múltiplo de la segunda]  con $k=1$

\[\begin{bmatrix}
1 & k  \\
0 & 1  
\end{bmatrix}
\bmatriz{x_1\\x_2}
=\bmatriz{x_1+(k \ x_2)\\x_2}
%=\bmatriz{x_2\\y_2}
\]


La combinación lineal del vector del eje x con el vector a 45 grados es
%\[c_1 \bmatriz{1\\0}  + c_3 \bmatriz{1\\1} \text{, ó } \bmatriz{1&1\\0&1}\bmatriz{c_1\\c_3} \]
%y para la matriz $A=\bsmatriz{1&1\\0&1}$, $T_A$ queda 
\[\bmatriz{1&1\\0&1}\bmatriz{c_1\\c_3}=c_1 \bmatriz{1\\0} + c_3 \bmatriz{1\\1}=\bmatriz{c_1+c_3\\c_3}\]


\[\begin{bmatrix}
1 & 1  \\
0 & 1  
\end{bmatrix}
\bmatriz{
-1 &-1.5&-1  \\
0.5&0   &-0.5
}=
\bmatriz{
-0.5 &-1.5&-1.5  \\
0.5&0   &-0.5
}
\]
		\includegraphics[scale=0.5,viewport=0 90 600 300,clip]{cicla_2_rap.eps}


\item[Sumando a la segunda componente un múltiplo de la primera]  con $k=1$

\[\begin{bmatrix}
1 & 0  \\
k & 1  
\end{bmatrix}
\bmatriz{x_1\\x_2}
=\bmatriz{x_1\\x_2+(k\ x_1)}
%=\bmatriz{x_2\\y_2}
\]

\[\begin{bmatrix}
1 & 0  \\
1 & 1  
\end{bmatrix}
\bmatriz{
-1 &-1.5&-1  \\
0.5&0   &-0.5
}=
\bmatriz{
-1 &-1.5&-1  \\
-0.5&-1.5&-1.5
}\]
		\includegraphics[scale=0.5,viewport=0 90 550 240,clip]{cicla_2_sub.eps}


\end{description}

Ejercicio. ¿Cómo queda la figura al aplicarle la transformación \[\bmatriz{0.5 & -0.5 \\ 0.5 & 0.5}\]

\section{Transformaciones con $\R^3$}

\begin{description}
\item[De $\R^3$ en $\R^2$.]
Ahora le concatenamos a los dos vectores de la identidad el vector
 $\ve{v_3}=\bsmatriz{1\\1}$ de tal forma que ahora la transformación correspondiente convierte vectores de $\R^3$ en vectores de $\R^2$. 
%\[c_1 \bmatriz{1\\0} + c_2 \bmatriz{0\\1} + c_3 \bmatriz{1\\1} \text{, ó } \bmatriz{1&0&1\\0&1&1}\bmatriz{c_1\\c_2\\c_3} \]
%y para la matriz $A=\bsmatriz{1&0&1\\0&1&1}$, $T_A$ queda 
\[x_1 \bmatriz{1\\0} + x_2 \bmatriz{0\\1} + x_3 \bmatriz{1\\1}=\bmatriz{1&0&1\\0&1&1}\bmatriz{x_1\\x_2\\x_3}=\bmatriz{x_1+x_3\\x_2+x_3}\]
En particular si queremos dos veces el primer vector, una vez el segundo vector y cuatro veces el tercer vector, la combinación lineal se expresa 
\[2 \bmatriz{1\\0} + 1 \bmatriz{0\\1} + 4 \bmatriz{1\\1}=\bmatriz{1&0&1\\0&1&1}\bmatriz{2\\1\\4}=\bmatriz{6\\5}\]
%\scalebox{1} % Change this value to rescale the drawing.
%{\psset{xunit=0.9cm,yunit=0.9cm}
%\begin{pspicture}(0,-3.358125)(17.180937,3.358125)
%\rput(2.1809375,-1.0403125){\psaxes[linewidth=0.04,labels=none](0,0)(-2,-2)(4,4)}
%\usefont{T1}{ptm}{m}{n}
%\rput(0.40234375,-3.1303124){$x_1$}
%\usefont{T1}{ptm}{m}{n}
%\rput(5.702344,-0.6303125){$x_2$}
%%\psline[linewidth=0.04cm,linestyle=dashed,dash=0.16cm 0.16cm](0.1809375,-3.1003125)(3.1809375,2.9996874)
%\rput(12.180938,-1.0403125){\psaxes[linewidth=0.04](0,0)(-2,-2)(5,4)}
%%\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm](14.180938,2.9996874)(14.180938,-3.0003126)
%\usefont{T1}{ptm}{m}{n}
%\rput(8.472343,1.0096875){$T_A$}
%%\psline[linewidth=0.04cm](0.1809375,-3.0003126)(3.1809375,0.0)
%\usefont{T1}{ptm}{m}{n}
%\rput(2.3023438,3.1696875){$x_3$}
%\psline[linewidth=0.04cm](1.0809375,-1.9003125)(1.2809376,-2.1003125)
%\psline[linewidth=0.04cm](0.0809375,-2.9003124)(0.2809375,-3.1003125)
%\psline[linewidth=0.04cm](2.0809374,-0.9003125)(2.2809374,-1.1003125)
%\psline[linewidth=0.04cm](3.0809374,0.0996875)(3.2809374,-0.1003125)
%\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(7.6590624,0.44859377)(9.259063,0.44859377)
%\usefont{T1}{ptm}{m}{n}
%\rput(11.102344,2.4696875){$y_2$}
%\usefont{T1}{ptm}{m}{n}
%\rput(16.602345,-2.0303125){$y_1$}
%\end{pspicture} 
%}
\scalebox{1} % Change this value to rescale the drawing.
{
\begin{pspicture}(0,-3.358125)(18,5)%\grilla
\rput(2.1809375,-1.0403125){
%\rput(12.0,-2.77){\psaxes[linewidth=0.04](0,0)(-2,-2)(5,4)}
\psaxes[linewidth=0.04,labels=none](0,0)(-2,-2)(4,4)}
\usefont{T1}{ptm}{m}{n}
\rput(0.40234375,-3.1303124){$x_1$}
\usefont{T1}{ptm}{m}{n}
\rput(5.702344,-0.6303125){$x_2$}
%\psline[linewidth=0.04cm,linestyle=dashed,dash=0.16cm 0.16cm](0.1809375,-3.1003125)(3.1809375,2.9996874)
\definecolor{color4}{rgb}{0.6,0.6,0.6}
\rput(12.180938,-1.0403125){\psaxes[linewidth=0.04,linecolor=color4](0,0)(-2,-2)(6,6)}
%\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm](14.180938,2.9996874)(14.180938,-3.0003126)
\usefont{T1}{ptm}{m}{n}
\rput(8.472343,1.0096875){$T_{\bsmatriz{1&0&1\\0&1&1}}\parr{\bsmatriz{2\\1\\4}}$}
\psline[linewidth=0.04cm](0.1809375,-3.0003126)(3.1809375,0.0)
\usefont{T1}{ptm}{m}{n}
\rput(2.3023438,3.1696875){$x_3$}
\psline[linewidth=0.04cm](1.0809375,-1.9003125)(1.2809376,-2.1003125)
\psline[linewidth=0.04cm](0.0809375,-2.9003124)(0.2809375,-3.1003125)
\psline[linewidth=0.04cm](2.0809374,-0.9003125)(2.2809374,-1.1003125)
\psline[linewidth=0.04cm](3.0809374,0.0996875)(3.2809374,-0.1003125)
%\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm](2.581062,-3.0994067)(5.581062,3.000594)
%\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm](1.781062,-3.0994067)(4.781062,3.000594)
%\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm](0.98106265,-3.0994067)(3.981062,3.000594)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(7.6590624,0.44859377)(9.259063,0.44859377)
\usefont{T1}{ptm}{m}{n}
\rput(11.102344,2.4696875){$y_2$}
\usefont{T1}{ptm}{m}{n}
\rput(16.602345,-2.0303125){$y_1$}

\psline[linewidth=0.04cm,linestyle=dashed,dash=0.16cm 0.16cm](2.3,-1)(0.3,-3)
\psline[linewidth=0.04cm,linestyle=dashed,dash=0.16cm 0.16cm](3.3,-1)(1.3,-3)
\psline[linewidth=0.04cm,linestyle=dashed,dash=0.16cm 0.16cm](0.3,-3)(1.3,-3)
\psline[linewidth=0.04cm,linestyle=dashed,dash=0.16cm 0.16cm](2.3,-1.1)(3.3,-1.1)
\psline[linewidth=0.04cm,linestyle=dashed,dash=0.16cm
 0.16cm](1.3,-3)(1.3,1)


\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(12.2,-1)(13.2,-1)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(12.2,-1)(12.2,0)
\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(12.2,-1)(13.2,0)

\psline[linewidth=0.04cm,linestyle=dashed,dash=0.16cm
 0.16cm](12.2,-1)(14.2,-1)
\psline[linewidth=0.04cm,linestyle=dashed,dash=0.16cm
 0.16cm](14.2,-1)(14.2,0)
\psline[linewidth=0.04cm,linestyle=dashed,dash=0.16cm
 0.16cm](14.2,0)(18.2,4)




\psdots[dotsize=0.2](1.3,1)
\psdots[dotsize=0.2](18.2,4)


\end{pspicture} 
}

\item[De $\R^2$ en $\R^3$.]
 Ahora veamos la combinación lineal de dos vectores en $R^3$ (vectores-3).
 La combinación lineal de dos vectores, uno en el eje $y_1$ $\ve{v_1}=\bsmatrizB{1\\0\\0}$ y otro en el eje $y_2$ $\ve{v_2}=\bsmatrizB{0\\1\\0}$, está dada por 
%\[c_1 \bmatriz{1\\0\\0} + c_2 \bmatriz{0\\1\\0} \text{, ó } \bmatriz{1&0\\0&1\\0&0}\bmatriz{c_1\\c_2} \]
%Como $A=\bsmatrizB{1&0\\0&1\\0&0}$ entonces 
\[x_1 \bmatriz{1\\0\\0} + x_2 \bmatriz{0\\1\\0}=\bmatriz{1&0\\0&1\\0&0}\bmatriz{x_1\\x_2}= %\bmatriz{x_1\\0\\0} + \bmatriz{0\\x_2\\0} = 
\bmatriz{x_1\\x_2\\0}\]
%En este caso NO da el mismo vector $\ve{c}$, por este motivo la matriz $\bsmatrizB{1&0\\0&1\\0&0}$ No se llama la matriz identidad. De aquí concluimos que las matrices identidad siempre son cuadradas.

Dibuje el punto $\bmatriz{3\\2}$ en $\R^2$ y $T_{\bsmatriz{1&0\\0&1\\0&0}}\parr{\bmatriz{3\\2}}$ en $\R^3$.

\item[De $\R^3$ en $\R^3$.]
 La combinación lineal de los dos vectores anteriores con el vector $\ve{v_3}=\bsmatrizB{1\\1\\0}$ es
%\[c_1 \bmatriz{1\\0\\0} + c_2 \bmatriz{0\\1\\0} + c_3 \bmatriz{1\\1\\0} \text{, ó } \bmatriz{1&0&1\\0&1&1\\0&0&0}\bmatriz{c_1\\c_2\\c_3} \]
%y para la matriz $A=\bsmatrizB{1&0&1\\0&1&1\\0&0&0}$, $T_A$ queda 
\[x_1 \bmatriz{1\\0\\0} + x_2 \bmatriz{0\\1\\0} + x_3 \bmatriz{1\\1\\0}=\bmatriz{1&0&1\\0&1&1\\0&0&0}\bmatriz{x_1\\x_2\\x_3}=\bmatriz{x_1+x_3\\x_2+x_3\\0}\]

Dibuje el punto $\bmatriz{3\\2\\1}$ en $\R^3$ y $T_{\bsmatriz{1&0&1\\0&1&1\\0&0&0}}\parr{\bmatriz{3\\2\\1}}$ también en $\R^3$.


\end{description}




%¿Cómo quedaría la combinación lineal, la matriz y la transformación respectivas para la combinación lineal del vector del eje x con el vector a 45 grados? 
%es
%\[c_1 \bmatriz{1\\0}  + c_3 \bmatriz{1\\1} \text{, ó } \bmatriz{1&1\\0&1}\bmatriz{c_1\\c_3} \]
%y para la matriz $A=\bsmatriz{1&1\\0&1}$, $T_A$ queda \[T_A(\ve{c})=c_1 \bmatriz{1\\0} + c_3 \bmatriz{1\\1}=\bmatriz{c_1+c_3\\c_3}\]
%



%Es evidente que sólo se pueden combinar vectores del mismo espacio


%Por ejemplo, la siguiente matriz representa los primeros puntos de un dibujo en dos dimensiones.
%$\bmatriz{
%-1 &-1.5&-1  &-0.5&-1 \\%&-1&1&1  &1.5&1   &0.5&1  &1&1  &0.5&0  &0  &0.2&0  &-0.2&0  &0&0.5&0\\
%0.5&0   &-0.5&0   &0.5%&1 &1&0.5&0  &-0.5&0  &0.5&1&1.5&1.5&1.8&1.9&2.1&2.3&2.1 &1.9&1&0.5&0
%}$
%
%
%
%%Se sugiere estudiar la sección \cite[5.1]{NJ99} excepto el ejemplo 2 y el teorema 1 y realice los ejercicios del 25 al 35. De manera opcional se puede mirar \cite[Sec. 4.2]{Ant06}
%
%
%
%
%%----------------- mostrar que si Av=v para todo v entonces A=I
%
%
%
%
%
%
%
%--------------
%
%----- introducir el espacio generado por un conjunto de vectores, graficar el generado por un vector, y por dos vectores no paralelos. definir recta y plano
%
%----- Producto punto
%
%----- TL
%



\newslide
\section{Multiplicación matricial como composición de transformaciones matriciales}

Suponga que $A \in \R^{m \times r}$ ($A$ pertenece al conjunto de las marices de tamaño $m \times r$), que $B \in \R^{r \times n}$ y que $\ve{c}$ es un vector-$n$. 
Debido a que la multiplicación matricial es asociativa se tiene la siguiente igualdad al considerar los vectores como matrices columna.
\[A(B\ve{c})=(AB)\ve{c}\] 
Recordemos que una matriz se puede ver como una transformación matricial. ¿Cómo se puede ver las transformaciones en este caso?
Para el lado izquierdo de la anterior ecuación se puede ver la \dn{composición} de dos transformaciones matriciales (la cual se ilustra en \cite[Figura 5.27 pg 357]{NJ99}), mientras que el lado derecho se puede ver como una transformación matricial de la matriz resultante del producto $AB$.
\[T_A (T_B (\ve{c}))=T_{AB}(\ve{c})\]
Esto se puede interpretar como que la composición de transformaciones matriciales es la transformación dada por el producto de matrices. Lo cual queda más claro con el siguiente ejemplo. 
Sabemos que la matriz $A=\bsmatriz{0 & 1  \\1 & 0}$ es la matriz que intercambia componentes y que $B=\bsmatriz{2 & 0  \\0 & 1}$ es la matriz que escalona (duplica) la primera componente. $T_A (T_B (\ve{c}))$ significa que primero se duplica la primera componente y luego se intercambian las componente. En la siguiente figura  primero se muestra la imagen original, luego escalonada en la primera componente y finalmente se intercambia las componentes.

		\includegraphics[scale=0.3]{ciclista_003_comp_b.eps}

Por otro lado para graficar $T_{AB}$ hay que calcular el producto
\[AB=\bmatriz{0 & 1  \\1 & 0}\bmatriz{2 & 0  \\0 & 1}=\bmatriz{0 & 1  \\2 & 0}\]
la corresponde a una transformación que realiza las dos operaciones en un sólo paso.
En la siguiente figura se tienes 	las imágenes de la composición 	$T_B (T_A (\ve{c}))=T_{BA}(\ve{c})$ lo cual muestra que ni la composición ni la multiplicación son conmutativas.
		
		\includegraphics[scale=0.3]{ciclista_004_comp_a.eps}

%Colocar producto?????????????
Se sugiere mirara \cite[pag. 239-243]{Ant06}. De manera opcional se puede ojear la sección \cite[5.5]{NJ99} y se recomienda realizar los ejercicios 8 9 y 10. De manera opcional se pueden hacer los ejercicios del 1 al 17. 





\section{Resumen}

\begin{itemize}
\item combinación lineal $c_1\bmatriz{a_{11}\\a_{21}\\ \vdots \\ a_{m1} }+
c_2\bmatriz{a_{12}\\a_{22}\\ \vdots \\ a_{m2} }+
\ldots +
c_n\bmatriz{a_{1n}\\a_{2n}\\ \vdots \\ a_{mn} }$ donde $a_{ij}$ y $c_k$ son escalares y da un vector-$m$,
\item operación matriz por vector $\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots&\vdots&\ddots&\vdots\\
a_{m1} & a_{m2} & \cdots & a_{mn}  \\	
\end{bmatrix}
\bmatriz{c_1\\c_2\\ \vdots \\ \vdots \\ c_n }$ donde $a_{ij}$ y $c_k$ son escalares y da un vector-$m$,
\item transformación matricial $\begin{bmatrix}
a_{11} x_1 + a_{12} x_2 + \cdots + a_{1n} x_n \\
a_{21} x_1 + a_{22} x_2 + \cdots + a_{2n} x_n \\
\vdots\\%&\vdots&\ddots&\vdots\\
a_{m1} x_1 + a_{m2} x_2 + \cdots + a_{mn}  x_n \\	
\end{bmatrix}$ donde los $a_{ij}$ son escalares y los $x_k$ son variables. 
\end{itemize}


En esta sección usted debe estar en capacidad de encontrar el covalor de la transformación de un valor numérico o de un valor arbitrario. Además, dada una transformación matricial (es decir una matriz) debe saber cual es el dominio y el codominio. También debe identificar aquellas transformaciones matriciales que: sólo escalona alguna componente, sólo intercambia componentes, suman una componente a otra. Por último debe componer transformaciones matriciales.

\newslide


%\newslide
\chapter{Solución de sistemas de ecuaciones lineales o transformaciones inversas}
%\section{Solución de sistemas de ecuaciones lineales}

Borrador de las notas de clase (Bajo licencia GPL), enviar las sugerencias a  gmunoz@udistrital.edu.co.  


Regresando a nuestro ejemplo del plotter, sale una pregunta de modo natural. ¿Cuánto tiempo debo prender los motores para poder llegar a unas coordenadas determinadas? para el caso de tener un motor en cada eje la respuesta es sencilla, se complica un poco para cuando están solamente $Mot_1$ y $Mot_3$ y se complica aun más cuando están los tres motores. En este capítulo veremos una forma sistemática para resolver esta pregunta. 

\section{Matrices extendidas}
En el capítulo ase multiplicó una matríz $A \in \R^{m \times n}$ por un vector-$n$ $\ve{c}$ %y se calculaba 
%la transformación $T_A(\ve{c})$ que es 
y se obtenía el vector-$m$ $A \ve{c}$. En este capítulo vamos a buscar si existe un vector-$m$ $x$ tal que al ser multiplicado por la matriz $A$ de un vector $b$. Esto se puede representar de las siguientes maneras 
%se conoce la matriz $A \in \R^{m \times n}$ y el vector-$m$ $\ve{b}$ y debemos encontrar (si es que existen) los vectores-$n$ $\ve{x}$ tales que 
%son transformados en $\ve{b}$ %
%\[A \ve{x}=\ve{b}\] 

%Cuando vimos combinación lineal calculábamos el valor del vector-$m$ $\ve{y}$ a partir del producto $A \ve{u} = \ve{y}$, donde $A$ es una matriz de $m \x n$ y $\ve{u}$ es un vector-$n$. En esta sección se va a cambiar la incógnita, ahora hay que calcular el vector-$n$ $\ve{x}$ que cumpla el producto 
%\parbox{11cm}
{\begin{align*}
%T_A( \ve{x} )&= \ve{b},
%\intertext{que resulta ser}
A \ve{x} &= \ve{b},
\intertext{al remplazar por las componentes queda}
\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots&\vdots&\ddots&\vdots\\
a_{m1} & a_{m2} & \cdots & a_{mn}  \\	
\end{bmatrix}
\bmatriz{x_1\\x_2\\ \vdots \\ \vdots \\ x_n }
&=\bmatriz{b_1\\b_2\\ \vdots \\ b_m }
\intertext{que es lo mismo que}
\bmatriz{a_{11} x_1 + a_{12} x_2 + \ldots + a_{1n} x_n  \\
a_{21} x_1 + a_{22} x_2 + \ldots + a_{2n} x_n  \\
\vdots\\
a_{m1} x_1 + a_{m2} x_2 + \ldots + a_{mn} x_n  }
&=\bmatriz{b_1 \\ b_2 \\ \vdots \\ b_m}
\intertext{y como dos vectores son iguales si sus elementos son iguales entonces}
a_{11} x_1 + a_{12} x_2 + \ldots + a_{1n} x_n &= b_1 \\
a_{21} x_1 + a_{22} x_2 + \ldots + a_{2n} x_n &= b_2 \\
&\vdots\\
a_{m1} x_1 + a_{m2} x_2 + \ldots + a_{mn} x_n &= b_m \\
\end{align*}}
En particular este último se conocen como un \de{sistema de ecuaciones lineales} con  $m$ ecuaciones y $n$ variables. Las componentes de $A$ se conocen como los \de{coeficientes del sistema lineal}, las componentes del vector-$n$ $\ve{x}$ se llaman \de{variables} y las componentes del vector-$m$ $\ve{b}$ se llaman \de{términos constantes}. 


Un sistema de ecuaciones se puede representar con la \de{matriz extendida} de la siguiente forma
\[[A : b]\]
o lo que es lo mismo
\[\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1n} &: &b_1 \\
a_{21} & a_{22} & \cdots & a_{2n} &: &b_2\\
\vdots&\vdots&\ddots&\vdots\\
a_{m1} & a_{m2} & \cdots & a_{mn}  &: &b_m\\	
\end{bmatrix}\]
Pero para usar esta notación es necesario fijar el orden de las variables, en este caso es $x_1,x_2, \ldots x_n$. Lo cual personalmente escribo sobre la matriz extendida, aunque esto no se acostumbra en la literatura referenciada para el curso.

\begin{center}
\parbox{8cm}{ 
\begin{align*}
&\ \ \ \ \begin{matrix}
x_1&\ x_2&\ \cdots &\ x_n
\end{matrix}\\
&\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1n} &: &b_1 \\
a_{21} & a_{22} & \cdots & a_{2n} &: &b_2\\
\vdots&\vdots&\ddots&\vdots\\
a_{m1} & a_{m2} & \cdots & a_{mn}  &: &b_m\\	
\end{bmatrix}
\end{align*}
}
\end{center}

%Observe que las variables son los coeficientes de la combinación lineal vistos en el capítulo anterior. Sin embargo, cuando hablemos de coeficientes en este capítulo seguramente estamos haciendo referencia a los coeficientes del sistema lineal, es decir a los elementos de $A$.  

%\perspectiva{Usualmente se usan las símbolos $x$ y $y$ para las variables, pero estas letras también tienen otros usos. Es el contexto el que permite inferir cuales símbolos son las variables.}

%\perspectiva{El `mismo' objeto matemático tiene ahora cuatro significados con características propias:
%\begin{itemize}
%\item como transformación matricial, la cual tiene dominio y codominio, 
%\item como operación matriz por vector, la cual cumple propiedades asociativa, distributiva entre otras, 
%\item como matriz extendida, la cual vamos a clasificar como matriz escalón, escalonada o escalón reducido y sobre la cual realizaremos algunas operaciones elemnetales.   
%\item como sistema de ecuaciones, al cual se le va a buscar una solución. 
%\end{itemize}
%Debido a que a estos significados corresponde el mismo objeto matemático entonces podemos aplicar las características de cualquiera de ellos a los otros, por ejemplo podemos encontrar la solución de una transformación matricial o determinar el dominio y el codominio a una matriz extendida. }
%



Cada renglón $i$ de la matriz extendida corresponde a una \de{ecuación lineal}
\[a_{i1} x_1 + a_{i2} x_2 + \ldots + a_{in} x_n = b_i\]


En una matriz cualquiera, un \de{renglón cero} sólo tiene ceros. Un \de{renglón no cero} tiene al menos un elemento diferente de cero. Lo mismo se define en las columnas. El primer elemento no cero de un renglón no cero se llama \de{elemento delantero}.


\section{Clasificación de matrices extendidas}

%Un vector $\ve{x}$ que cumple la ecuación $A \ve{x} = \ve{b}$ se llama \de{solución particular} del sistema lineal y el conjunto de todas las soluciones particulares se llama \de{conjunto solución}. Una solución particular escrita en forma genérica se llama \de{solución general}.


Hay unos sistemas de ecuaciones más sencillos de resolver que otros, primero vamos a caracterizar los más sencillos de resolver y después  vamos a convertir los demás sistemas a los más sencillos de resolver. Para esto necesitamos algunas definiciones que no sólo se pueden aplicar a las matrices extendidas, sino en general a todas las matrices \cite[1.1]{NJ99}.



Una matriz está en forma (renglón) \de{escalón} si cumple las siguientes dos propiedades.


-----------Explicar como simplifica cada propiedad el sistema de ecuaciones

\begin{enumerate}[(E1)]
\item Todos los renglones cero están en la parte inferior de la matriz o si no hay renglones de ceros.
\item Para cada elemento delantero $a_{i,j}$ se tiene que el elemento delantero del siguiente renglón $a_{i+1,k}$ (si lo hay) debe estar a la derecha (es decir $j < k$).
% renglón $i$ el elemento delantero del siguiente renglón $i+1$ (si lo hay) se encuentra a la derecha del renglón . 
\end{enumerate}
A los elementos delanteros de una matriz escalón se les llama \de{pivotes}.  Las \de{columnas pivote} %$v_1,v_2, \ldots ,v_i$ 
y \de{renglones pivote}  %$r_1,r_2, \ldots ,r_i$ 
son aquellos que tienen un elemento pivote. 

----\teorema{En cada columna y en cada renglón hay máximo un pivote}


----\teorema{El número de pivotes es menor que el número de columnas y de renglones}

  
En [Gro06] se define una matriz \de{escalonada} (por renglones) como la matriz que está en forma renglón escalón y además cumple la siguiente propiedad.
\begin{enumerate}[(E3)]
\item Cada pivote es $1$ (y se llama \de{1 delantero}).
\end{enumerate}

Una matriz está en forma \de{escalón reducida} o \de{escalonada reducida} si es una matriz escalonada por renglones y además cumple la siguiente propiedad.
\begin{enumerate}[(E4)]
\item Arriba y abajo del pivote hay ceros.
\end{enumerate}


%Las columnas con pivote se llaman \de{columnas pivote} $v_1,v_2, \ldots ,v_i$ %y las otras se llaman \de{columnas ???????} $l_1,l_2, \ldots ,l_j$. 
%Si una matriz tiene todas las columnas pivote se llama \de{linealmente independiente} de lo contrario la matriz se llama \de{linealmente dependiente}.

%\section{Solución de sistemas de ecuaciones}

Un vector $\ve{x}$ que cumple la ecuación $A \ve{x} = \ve{b}$ se llama \de{solución particular} del sistema lineal y el conjunto de todas las soluciones particulares se llama \de{conjunto solución}. Una solución particular escrita en forma genérica se llama \de{solución general}.


Los siguientes ejemplos permiten comprender como se relacionan la anterior clasificación de matrices extendidas con las soluciones de los sistemas de ecuaciones.

\newslide
\section{Ejemplo con la matriz identidad}
Queremos encontrar cuales vectores-$2$ $\ve{x}$ cumplen que
%son enviados por la transformación dada por matriz identidad de $2 \times 2$, $T_A=\bsmatriz{1 & 0\\0&1}$ al punto $\bsmatriz{2\\3}$ esto se puede escribir de las siguientes formas

\begin{align}
%\tag{$T_A(\ve{x})=\ve{b}$}&T_{\bsmatriz{1 & 0\\0&1}}(\bmatriz{x_1\\x_2})=\bmatriz{2\\3}\\ 
\tag{$A\ve{x}=\ve{b}$}&\bmatriz{1 & 0\\0&1} \bmatriz{x_1\\x_2}=\bmatriz{2\\3}\\ 
\tag{Matriz extendida}&\bmatriz{1 & 0&:&2\\0&1&:&3} \\ 
\tag{Sistema de ecuaciones}&\matriz{1x_1 &+& 0x_2&=&2\\0x_1&+&1x_2&=&3}
\end{align} 

\subsection{Solución del sistema lineal}
Resolviendo el sistema de ecuaciones obtenemos que $x_1=2$ y $x_2=3$. 
\begin{align}
\tag{una solución particular}&\bmatriz{2\\3}\\ 
\tag{la solución general}&\bmatriz{2\\3}\\ 
\tag{el conjunto solución}&\set{\bmatriz{2\\3}}% 
\end{align}
en este caso la solución particular y la general coinciden porque la solución es única.

\subsection{Clasificación de la matriz}
En la matriz extendida vemos que cumple:
\begin{itemize}
\item (E1) porque no tiene renglones de ceros, 
\item (E2) porque la entrada $2,1$ es cero,
\item (E3) porque los elementos delanteros $1,1$ y $2,2$ son uno,
\item (E4) porque la entrada $1,2$ también es cero.
\end{itemize}

Como cumple (E1) y (E2) la matriz está en forma escalón y podemos identificar los elementos delanteros, en este caso en el primer renglón el elemento delantero (o pivote) es la entrada $(1,1)$. Este elemento delantero está asociado a la variable $x_1$ que fue la variable que se despejó en la primera ecuación. 

En el segundo renglón el elemento delantero (o pivote) es la entrada $(2,2)$. Este elemento delantero está asociado a la variable $x_2$ que fue la variable que se despejó en la segunda ecuación. %(Este párrafo se redactó igual que el anterior para facilitar la comparación entre los renglones de la matriz)

Todos los renglones son pivotes pero sólo las dos primeras columnas son pivotes, las mismas columnas que están asociadas a variables, entonces todas las columnas asociadas a variables son pivotes.

Como cumple (E3) la matriz está en forma escalonada lo cual facilita el despejar las variables $x_1$ y $x_2$.

Como también cumple (E4) la matriz está en forma escalón reducida y esto hace que el sistema de ecuaciones ya está despejado.

%En general vamos a concluir que ``En el `i' renglón el elemento delantero (o pivote) es la entrada $i,?$. Este elemento delantero está asociado a la variable $x_?$ que fue la variable que se despejó en la `i' ecuación.

\subsection{Representación gráfica}
Para representar gráficamente la solución vamos a considerar la variable $x_1$ como el eje horizontal y $x_2$ como el eje vertical. Luego se colocan los puntos que resuelven cada una de las ecuaciones.

\scalebox{1} % Change this value to rescale the drawing.
{
%\psset{xunit=0.50cm,yunit=0.50cm}
\begin{pspicture}(0,-3.02)(6.02,3.02)
\rput(2.0,-1.0){\psaxes[linewidth=0.04](0,0)(-2,-2)(4,4)}
\usefont{T1}{ptm}{m}{n}
\rput(5.4214063,-1.99){$x_1$}
\usefont{T1}{ptm}{m}{n}
\rput(1.2214062,2.61){$x_2$}
\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm](4.0,-3.0)(4.0,3.0)
\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm](6.0,2.0)(0.0,2.0)
\end{pspicture} 
}

La primera ecuación $1x_1+0x_2=2$ corresponde a la línea vertical que cruza el eje de $x_1$ por 2. La segunda ecuación $0x_1+1x_2=3$ corresponde a la línea horizontal que cruza el eje de $x_2$ por 3. La solución del sistema es donde se cruzan las dos líneas.
 

\subsection{Solución con valores arbitrarios}

Si en vez del vector-$2$ numérico $\ve{b}=\bsmatriz{2\\3}$ tenemos el vector-$2$ arbitrario $\ve{b}=\bsmatriz{b_1\\b_2}$ 

\begin{align}
%\tag{$T_A(\ve{x})=\ve{b}$}&T_{\bsmatriz{1 & 0\\0&1}}(\bmatriz{x_1\\x_2})=\bmatriz{2\\3}\\ 
\tag{$A\ve{x}=\ve{b}$}&\bmatriz{1 & 0\\0&1} \bmatriz{x_1\\x_2}=\bmatriz{b_1\\b_2}\\ 
\tag{Matriz extendida}&\bmatriz{1 & 0&:&b_1&\\0&1&:&&b_2} \\ 
\tag{Sistema de ecuaciones}&\matriz{1x_1 &+& 0x_2&=&b_1\\0x_1&+&1x_2&=&b_2}
\end{align} 
Se han desfasado los valores arbitrarios $b_1$ y $b_2$ en la matriz extendida ya que será útil más adelante.

De las ecuaciones se obtiene directamente que  $x_1=b_1$ y $x_2=b_2$ y por lo tanto para cada valor $b_1$ y $b_2$ se tiene
\begin{align}
\tag{una solución particular}&\bmatriz{b_1\\b_2}\\ 
\tag{la solución general}&\bmatriz{b_1\\b_2}\\ 
\tag{el conjunto solución}&\set{\bmatriz{b_1\\b_2}}% 
\end{align}
en este caso también la solución particular y la general coinciden porque la solución es única. Además la matriz $\bmatriz{1 & 0\\0&1}$ se llama la identidad porque el valor y el covalor son idénticos.

Observe que en este caso al cambiar los términos constantes por los arbitrarios no se afectaron ni los pivotes ni la forma de despejar las variables.

\newslide
\section{Ejemplo de matriz que no cumple (E4)}
 Ahora repetiremos el ejercicio pero para encontrar el vector-$2$, $\ve{x}$  que cumple
 %con la transformación dada por la matriz $\bsmatriz{1 & 1\\0&1}$
\begin{align}
%\tag{$T_A(\ve{x})=\ve{b}$}&T_{\bsmatriz{1 & 1\\0&1}}(\bmatriz{x_1\\x_2})=\bmatriz{2\\3}\\ 
\tag{$A\ve{x}=\ve{b}$}&\bmatriz{1 & 1\\0&1} \bmatriz{x_1\\x_2}=\bmatriz{2\\3}\\ 
\tag{Matriz extendida}&\bmatriz{1 & 1&:&2\\0&1&:&3} \\ 
\tag{Sistema de ecuaciones}&\matriz{1x_1 &+&   1x_2&=&2\\0x_1&+&1x_2&=&3}  
\end{align} 

\subsection{Solución del sistema lineal}
Este sistema de ecuaciones es un poco más interesante que el anterior, de manera inmediata vemos que $x_2=3$ y resolviendo la primera ecuación obtenemos 
\begin{align*}
x_1+x_2=2\\
x_1=2-x_2\\
x_1=2-3\\
x_1=-1
\end{align*}
\begin{align}
\tag{una solución particular}&\bmatriz{-1\\3}\\ 
\tag{la solución general}&\bmatriz{-1\\3}\\ 
\tag{el conjunto solución}&\set{\bmatriz{-1\\3}}% 
\end{align}
también en este caso la solución particular y la general coinciden porque la solución es única.

\subsection{Clasificación de la matriz}
En la mariz extendida vemos que cumple:
\begin{itemize}
\item (E1) porque no tiene renglones de ceros, 
\item (E2) porque la entrada $2,1$ es cero,
\item (E3) porque los elementos delanteros $1,1$ y $2,2$ son uno,
%\item (E4) porque la entrada $1,2$ también es cero.
\end{itemize}
(E4) no se cumple porque la entrada $1,2$ no es cero.

Como cumple (E1) y (E2) la matriz está en forma escalón y podemos identificar los elementos delanteros, en este caso en el primer renglón el elemento delantero (o pivote) es la entrada $1,1$. Este elemento delantero está asociado a la variable $x_1$ que fue la variable que se despejó en la primera ecuación. 

En el segundo renglón el elemento delantero (o pivote) es la entrada $2,2$. Este elemento delantero está asociado a la variable $x_2$ que fue la variable que se despejó en la segunda ecuación.

Todos los renglones son pivotes pero sólo las dos primeras columnas son pivotes, las mismas columnas que están asociadas a variables, entonces todas las columnas asociadas a variables son pivotes.

Como cumple (E3) la matriz está en forma escalonada lo cual facilita el despejar las variables $x_1$ y $x_2$.

Como no cumple (E4) la matriz no está en forma escalón reducida y esto hace que sea necesario realizar algunos pasos adicionales para despejar $x_2$.

%En general vamos a concluir que ``En el `i' renglón el elemento delantero (o pivote) es la entrada $i,?$. Este elemento delantero está asociado a la variable $x_?$ que fue la variable que se despejó en la `i' ecuación.

\subsection{Representación gráfica}
La representación gráfica de la solución de las ecuaciones se representa en la siguiente gráfica.

\scalebox{1} % Change this value to rescale the drawing.
{
\begin{pspicture}(0,-3.02)(6.02,3.02)
\rput(2.0,-1.0){\psaxes[linewidth=0.04](0,0)(-2,-2)(4,4)}
\usefont{T1}{ptm}{m}{n}
\rput(5.4214063,-1.99){$x_1$}
\usefont{T1}{ptm}{m}{n}
\rput(1.2214062,2.61){$x_2$}
\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm](6.0,2.0)(0.0,2.0)
\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm](6.0,-3.0)(0.0,3.0)
\end{pspicture} 
}


Donde la segunda ecuación $0x_1+1x_2=3$ queda igual a la línea horizontal que cruza el eje de $x_2$ por 3. Pero la primera ecuación $1x_1+1x_2=2$ cambia, para dibujarla podemos despejar cualquier variable por ejemplo $x_1=2-x_2$ y dar valores a $x_2$ y encontrar los respectivos valores de $x_1$, por ejemplo $\bsmatriz{2\\0}$ y $\bsmatriz{1\\1}$. Con dos puntos es suficiente porque se dibuja la recta que pasa por esos dos puntos. 

En la siguiente figura se relaciona el punto obtenido (a la izquierda) con la combinación lineal de las columnas de la matriz (a la derecha).


\scalebox{1} % Change this value to rescale the drawing.
{\psset{xunit=0.9cm,yunit=0.9cm}
\begin{pspicture}(0,-4.79)(17.02,2)%\grilla
\definecolor{color4}{rgb}{0.6,0.6,0.6}
\rput(2.0,-2.77){\psaxes[linewidth=0.04](0,0)(-2,-2)(4,4)}
\usefont{T1}{ptm}{m}{n}
\rput(5.6028123,-2.26){$x_1$}
\usefont{T1}{ptm}{m}{n}
\rput(2.2028124,1.64){$x_2$}
\psdots[dotsize=0.2](1,0.2)
%\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm](6.0,0.23)(0.0,0.23)
%\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm](6.0,-4.77)(0.0,1.23)
\rput(12.0,-2.77){\psaxes[linewidth=0.04,linecolor=color4](0,0)(-2,-2)(5,4)}
%\psbezier[linewidth=0.04,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(1.0,0.27)(1.0,4.77)(14.0,4.77)(14.0,0.27)
%\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm](17.0,0.27)(10.0,0.27)
%\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm](14.0,1.27)(14.0,-4.73)
\usefont{T1}{ptm}{m}{n}
\rput(8,1.5){${\bmatriz{1 & 1\\0&1}} \bmatriz{x_1\\x_2} = \bmatriz{y_1\\y_2} $}
\rput(8,0){${\bmatriz{1 & 1\\0&1}} \bmatriz{-1\\3} = \bmatriz{2\\3} $}
%\psbezier[linewidth=0.04,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(2.0,0.27)(4.0,3.77)(13.0,3.87)(15.0,0.27)
%\psbezier[linewidth=0.04,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(3.0,-1.73)(3.0,2.77)(14.0,2.77)(14.0,-1.73)
\usefont{T1}{ptm}{m}{n}
\rput(16.412813,-2.26){$y_1$}
\usefont{T1}{ptm}{m}{n}
\rput(12.312813,1.64){$y_2$}
\psline[linewidth=0.04cm,arrowsize=0.05cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(12.0,-2.8)(13,-2.8)
\psline[linewidth=0.04cm,arrowsize=0.05cm 2.0,arrowlength=1.4,arrowinset=0.4,linestyle=dashed]{>->}(12.0,-2.8)(11,-2.8)
\psline[linewidth=0.04cm,arrowsize=0.05cm 2.0,arrowlength=1.4,arrowinset=0.4,linestyle=dashed]{>->}(15.0,0.2)(14,0.2)
\psline[linewidth=0.04cm,arrowsize=0.05cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(12.0,-2.8)(13,-1.8)
\psline[linewidth=0.04cm,arrowsize=0.05cm 2.0,arrowlength=1.4,arrowinset=0.4,linestyle=dashed]{>->}(12.0,-2.8)(15,0.2)
\psline[linewidth=0.04cm,arrowsize=0.05cm 2.0,arrowlength=1.4,arrowinset=0.4,linestyle=dashed]{>->}(11.0,-2.8)(14,0.2)
\psdots[dotsize=0.2](14,0.2)
\end{pspicture} 
}

Recordemos como esta matriz transforma los puntos del dibujo.

		\includegraphics[scale=0.5,viewport=0 90 600 300,clip]{cicla_2_rap.eps}

Y veamos como transforma las rectas



%\subsection{Transformación de las líneas}
%Ya sabemos que donde se cruzan las dos líneas ($\bsmatriz{-1\\3}$) es transformado en $\bsmatriz{2\\3}$, $T_A(\bsmatriz{-1\\3})=\bsmatriz{2\\3}$. Pero ¿cómo son trasformados el resto de los puntos de cada línea? basta con transformar dos valores de cada linea que soluciona el sistema de ecuaciones y trazar una nueva línea por los respectivos covalores. Para la primera ecuación, los puntos $\bsmatriz{-1\\3}$ y $\bsmatriz{1\\1}$, quedan transformados en 
%\[\bmatriz{1 & 1\\0&1} \bmatriz{-1\\3}=\bmatriz{2\\3},\ \ 
%\bmatriz{1 & 1\\0&1} \bmatriz{1\\1}=\bmatriz{2\\1}
%\]
%Para la segunda ecuación, los puntos $\bsmatriz{-1\\3}$ y $\bsmatriz{0\\3}$, quedan transformados en 
%\[\bmatriz{1 & 1\\0&1} \bmatriz{-1\\3}=\bmatriz{2\\3},\ \ 
%\bmatriz{1 & 1\\0&1} \bmatriz{0\\3}=\bmatriz{3\\3}
%\]
%
%La siguiente figura muestra la transformación de la rectas, podemos ver como la línea horizontal es desplazada a la derecha tres unidades (pero vuelva a caer sobre la misma línea por lo tanto no se distinguen) y la línea inclinada se convierte en una línea vertical. En general las líneas horizontales encima del eje se desplazan a la derecha y entre más lejos del eje se desplazan más.  

%La siguiente figura relaciona los puntos $\bsmatriz{x_1\\x_2}$ con los puntos $\bsmatriz{y_1\\y_2}$ al ser multiplicados por la matriz $\bsmatriz{1 & 1\\0&1}$.
%\[ \bmatriz{1 & 1\\0&1} \bmatriz{x_1\\x_2} = \bmatriz{y_1\\y_2} \]
\scalebox{1} % Change this value to rescale the drawing.
{\psset{xunit=0.9cm,yunit=0.9cm}
\begin{pspicture}(0,-4.79)(17.02,4)%\grilla
\rput(2.0,-2.77){\psaxes[linewidth=0.04](0,0)(-2,-2)(4,4)}
\usefont{T1}{ptm}{m}{n}
\rput(5.6028123,-2.26){$x_1$}
\usefont{T1}{ptm}{m}{n}
\rput(2.2028124,1.64){$x_2$}
\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm](6.0,0.23)(0.0,0.23)
\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm](6.0,-4.77)(0.0,1.23)
\rput(12.0,-2.77){\psaxes[linewidth=0.04](0,0)(-2,-2)(5,4)}
\psbezier[linewidth=0.04,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(1.0,0.27)(1.0,4.77)(14.0,4.77)(14.0,0.27)
\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm](17.0,0.27)(10.0,0.27)
\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm](14.0,1.27)(14.0,-4.73)
\usefont{T1}{ptm}{m}{n}
%\rput(6.3728123,4.58){$T_{\bsmatriz{1 & 1\\0&1}}$}
\psbezier[linewidth=0.04,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(2.0,0.27)(4.0,3.77)(13.0,3.87)(15.0,0.27)
\psbezier[linewidth=0.04,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(3.0,-1.73)(3.0,2.77)(14.0,2.77)(14.0,-1.73)
\usefont{T1}{ptm}{m}{n}
\rput(16.412813,-2.26){$y_1$}
\usefont{T1}{ptm}{m}{n}
\rput(12.312813,1.64){$y_2$}
\psline[linewidth=0.04cm,arrowsize=0.05cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(12.0,-2.7)(13,-2.7)
\psline[linewidth=0.04cm,arrowsize=0.05cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(12.0,-2.8)(13,-1.8)
\end{pspicture} 
}

\subsection{Solución con valores arbitrarios}
Si en vez del vector-$2$ numérico $\ve{b}=\bsmatriz{2\\3}$ tenemos el vector-$2$ arbitrario $\ve{b}=\bsmatriz{b_1\\b_2}$ entonces 

\begin{align}
%\tag{$T_A(\ve{x})=\ve{b}$}&T_{\bsmatriz{1 & 1\\0&1}}(\bmatriz{x_1\\x_2})=\bmatriz{2\\3}\\ 
\tag{$A\ve{x}=\ve{b}$}&\bmatriz{1 & 1\\0&1} \bmatriz{x_1\\x_2}=\bmatriz{b_1\\b_2}\\ 
\tag{Matriz extendida}&\bmatriz{1 & 1&:&b_1&\\0&1&:&&b_2} \\ 
\tag{Sistema de ecuaciones}&\matriz{1x_1 &+& 1x_2&=&b_1\\0x_1&+&1x_2&=&b_2}  
\end{align} 

Observe que en la matriz extendida los valores arbitrarios están desfasados, esto será de utilidad más adelante. 

De manera inmediata concluimos que $x_2=b_2$ y despejando concluimos que $x_1=b_1-3$. De esta forma obtenemos
\begin{align}
\tag{una solución particular}&\bmatriz{b_1-3\\b_2}\\ 
\tag{la solución general}&\bmatriz{b_1-3\\b_2}\\ 
\tag{el conjunto solución}&\set{\bmatriz{b_1-3\\b_2}}% 
\end{align}


Observe que en este caso al cambiar los términos constantes no se afectaron ni los pivotes ni la forma de despejar las variables.


%-----------------------colocar todas las soluciones

\newslide
\section{Ejemplo de matriz con parámetros}
%----------------completar los enunciados de los ejemplos
Vamos ahora a encontrar un vector-$3$ $\ve{x}$
%con la transformación dada por la matriz $\bsmatriz{1 & 0 & 1\\0&1&1}$
\begin{align}
%\tag{$T_A(\ve{x})=\ve{b}$}&T_{\bsmatriz{1 &0& 1\\0&1&1}}(\bmatriz{x_1\\x_2\\x_3})=\bmatriz{2\\3}\\ 
\tag{$A\ve{x}=\ve{b}$}&\bmatriz{1 &0& 1\\0&1&1} \bmatriz{x_1\\x_2\\x_3}=\bmatriz{2\\3}\\ 
\tag{Matriz extendida}&\bmatriz{1 &0& 1&:&2\\0&1&1&:&3} \\ 
\tag{Sistema de ecuaciones}&\matriz{1x_1 &+& 0x_2 &+&1 x_3&=&2\\0x_1&+&1x_2&+&1x_3&=&3}  
\end{align} 
Este sistema de ecuaciones es todavía más interesante que el anterior, para empezar no podemos concluir nada de manera inmediata. Por eso antes de encontrar las soluciones vamos a estudiar la matriz extendida para ver que cumple.
\subsection{Clasificación de la matriz}
\begin{itemize}
\item (E1) porque no tiene renglones de ceros, 
\item (E2) porque la entrada $2,1$ es cero y la $2,2$ no,
\item (E3) porque los elementos delanteros son uno,
\item (E4) porque la entrada $1,2$ también es cero.
\end{itemize}
%(E4) no se cumple porque la entrada $1,2$ no es cero. Lo cual es similar al ejemplo anterior.

Como cumple (E1) y (E2) la matriz está en forma escalón y podemos identificar los elementos delanteros, en este caso en el primer renglón el elemento delantero (o pivote) es la entrada $1,1$. Este elemento delantero está asociado a la variable $x_1$ la cual se puede despejar en términos de $x_3$ (ya que el coeficiente de $x_2$ es cero). \[x_1=2-1x_3\] 

En el segundo renglón el elemento delantero (o pivote) es la entrada $2,2$. Este elemento delantero está asociado a la variable $x_2$ la cual se puede despejar en términos de $x_3$ (ya que el coeficiente de $x_1$ es cero). \[x_2=3-1x_3\]

En este caso no hay un renglón de la matriz (o una ecuación) que permita despejar $x_3$. Esto se debe a que $x_3$ no es una variable delantera, es decir que no hay pivote en la columna de $x_3$. Por este motivo $x_3$ puede tomar cualquier valor real $x_3 \in \R$, en este caso decimos que $x_3$ es una \de{variable libre} o un \de{parámetro} y por lo tanto hay infinitas soluciones. Algunos autores le asignan otra letra al parámetro por ejemplo \[x_3=t\]

Como cumple (E3) la matriz está en forma escalonada lo cual facilita el despejar las variables $x_1$ y $x_2$.

Como cumple (E4) la matriz está en forma escalón reducida y esto hace que sea inmediato despejar $x_2$.

\subsection{Solución del sistema lineal}

Ahora si despejamos las variables obtenemos las siguientes soluciones.

----\teorema{Si la matriz de coeficientes tiene columnas sin pivote (es decir que tiene parámetros) entonces el sistema no puede tener solución única}

-----\teorema{Si la matriz de coeficientes tiene más columnas que renglones entonces el sistema de ecuaciones no puede tener solución única}

\begin{align}
\tag{una solución particular, con $t=0$}&\bmatriz{2\\3\\0}\\ 
\tag{la solución general}&\bmatriz{2-t\\3-t\\0+t}\\ 
\tag{el conjunto solución}&\set{\bmatriz{2-t\\3-t\\0+t} \mid t \in \R }% 
\end{align}
En este caso la solución no es única y ya no coinciden la solución particular con la general.


\subsection{Representación gráfica}
Ahora el vector $\ve{x}$ está en 3 dimensiones y por lo tanto la figura se complica un poco en las 2 dimensiones de esta hoja y sólo podemos presentar algunas proyecciones desarrolladas en Scilab. Las dos ecuaciones $x_1=2-x_3$ y $x_2=3-x_3$ corresponden a planos, los cuales se presentan en la siguiente figura a la izquierda. La figura de la derecha incluye la recta del conjunto solución $\set{\bsmatriz{2-t\\3-t\\0+t} \mid t \in \R }$

\begin{center}
	\includegraphics[scale=0.5]{CrucePlanos.eps}
	\includegraphics[scale=0.5]{CrucePlanosRecta.eps}
\end{center}

Para obtener estas figuras se realizó la siguiente código en Scilab

\begin{verbatim}
t1=[[2;0;0],[2;3;0],[-2;3;4],[-2;0;4]]
t2=[[-2;3;0],[2;3;0],[2;0;3],[-2;0;3]]
x=[t1(1,:)',t2(1,:)']
y=[t1(2,:)',t2(2,:)']
z=[t1(3,:)',t2(3,:)']
tcolor = [2 3]';
plot3d(x,y,list(z,tcolor));
param3d([2 -1],[3 0],[0 3])
\end{verbatim}

%\subsection{Transformación de las líneas}
%Este ejemplo se diferencia del anterior en que el dominio es $\R^3$, además la solución no es un punto sino la recta formada por el conjunto de puntos $\set{\bsmatriz{2-t\\3-t\\0+t} \mid t \in \R }$, de tal forma que \[T_A\parr{\bmatriz{2-t\\3-t\\0+t}}=\bmatriz{2\\3} \text{ para todo } t \in \R \].
%
%%La anterior notación implica que para cada valor  $\ve{v} \in \set{\bsmatriz{2-t\\3-t\\0+t} \mid t \in \R }$ se tiene que $T_A(\ve{v}) \in \set{\bsmatriz{2\\3}}$.
% 
%Para graficar la solución de la primera ecuación despejamos $x_1$, 
%$x_1 = 2 - x_3$ 
%y se le dan valores arbitrarios a $x_2$ y $x_3$ lo cual genera un plano. 
%\scalebox{.5} % Change this value to rescale the drawing.
%{
%\begin{pspicture}(0,-3.358125)(17.180937,3.358125)
%\rput(2.1809375,-1.0403125){\psaxes[linewidth=0.04,labels=none](0,0)(-2,-2)(4,4)}
%\usefont{T1}{ptm}{m}{n}
%\rput(0.40234375,-3.1303124){$x_1$}
%\usefont{T1}{ptm}{m}{n}
%\rput(5.702344,-0.6303125){$x_2$}
%\psline[linewidth=0.04cm,linestyle=dashed,dash=0.16cm 0.16cm](0.1809375,-3.1003125)(3.1809375,2.9996874)
%\rput(12.180938,-1.0403125){\psaxes[linewidth=0.04](0,0)(-2,-2)(5,4)}
%\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm](14.180938,2.9996874)(14.180938,-3.0003126)
%\usefont{T1}{ptm}{m}{n}
%\rput(8.472343,1.0096875){$T_A$}
%\psline[linewidth=0.04cm](0.1809375,-3.0003126)(3.1809375,0.0)
%\usefont{T1}{ptm}{m}{n}
%\rput(2.3023438,3.1696875){$x_3$}
%\psline[linewidth=0.04cm](1.0809375,-1.9003125)(1.2809376,-2.1003125)
%\psline[linewidth=0.04cm](0.0809375,-2.9003124)(0.2809375,-3.1003125)
%\psline[linewidth=0.04cm](2.0809374,-0.9003125)(2.2809374,-1.1003125)
%\psline[linewidth=0.04cm](3.0809374,0.0996875)(3.2809374,-0.1003125)
%\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm](2.581062,-3.0994067)(5.581062,3.000594)
%\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm](1.781062,-3.0994067)(4.781062,3.000594)
%\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm](0.98106265,-3.0994067)(3.981062,3.000594)
%\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(7.6590624,0.44859377)(9.259063,0.44859377)
%\usefont{T1}{ptm}{m}{n}
%\rput(11.102344,2.4696875){$y_2$}
%\usefont{T1}{ptm}{m}{n}
%\rput(16.602345,-2.0303125){$y_1$}
%\end{pspicture} 
%}
%
%Para encontrar en que es transformado el plano, con ayuda de la gráfica tomamos tres valores no \dn{colineales} y se dibujan los covalores en el codominio. Los tres puntos del plano no colineales son $\bsmatriz{2\\3\\0}$, $\bsmatriz{1\\2\\1}$ y $\bsmatriz{2\\0\\0}$.
%\[T_A\parr{\bmatriz{2\\3\\0}}=\bmatriz{2\\3},T_A\parr{\bmatriz{1\\2\\1}}=\bmatriz{2\\3},T_A\parr{\bmatriz{2\\0\\0}}=\bmatriz{2\\0}\]
%Los cuales son transformados en dos puntos $\bsmatriz{2\\3}$ y $\bsmatriz{2\\0}$ que genera una línea.
%
%Para la segunda ecuación despejamos $x_2$, $x_2=3-x_3$ y se dibuja el plano
%
%\scalebox{.5} % Change this value to rescale the drawing.
%{
%\begin{pspicture}(0,-3.358125)(17.180937,3.358125)
%\rput(2.1809375,-1.0403125){\psaxes[linewidth=0.04,labels=none](0,0)(-2,-2)(4,4)}
%\usefont{T1}{ptm}{m}{n}
%\rput(0.40234375,-3.1303124){$x_1$}
%\usefont{T1}{ptm}{m}{n}
%\rput(5.702344,-0.6303125){$x_2$}
%\psline[linewidth=0.04cm,linestyle=dashed,dash=0.16cm 0.16cm](5.1590624,-1.0514063)(2.1590624,1.9485937)
%\rput(12.180938,-1.0403125){\psaxes[linewidth=0.04](0,0)(-2,-2)(5,4)}
%\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm](10.159062,1.9485937)(17.059063,1.9485937)
%\usefont{T1}{ptm}{m}{n}
%\rput(8.472343,1.0096875){$T_A$}
%\psline[linewidth=0.04cm](0.1809375,-3.0003126)(3.1809375,0.0)
%\usefont{T1}{ptm}{m}{n}
%\rput(2.3023438,3.1696875){$x_3$}
%\psline[linewidth=0.04cm](1.0809375,-1.9003125)(1.2809376,-2.1003125)
%\psline[linewidth=0.04cm](0.0809375,-2.9003124)(0.2809375,-3.1003125)
%\psline[linewidth=0.04cm](2.0809374,-0.9003125)(2.2809374,-1.1003125)
%\psline[linewidth=0.04cm](3.0809374,0.0996875)(3.2809374,-0.1003125)
%\psline[linewidth=0.04cm,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(7.6590624,0.44859377)(9.259063,0.44859377)
%\usefont{T1}{ptm}{m}{n}
%\rput(11.112344,2.4696875){$y_2$}
%\usefont{T1}{ptm}{m}{n}
%\rput(16.612343,-2.0303125){$y_1$}
%\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm](4.3590627,-1.8514062)(1.3590626,1.1485938)
%\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm](3.5590625,-2.6514063)(0.5590625,0.34859377)
%\end{pspicture} }
%



%
%
%En el plano encontramos tres valores no colineales $\bsmatriz{2\\3\\0}$, $\bsmatriz{1\\2\\1}$ y $\bsmatriz{0\\3\\0}$ y determinamos sus covalores
%\[T_A\parr{\bmatriz{2\\3\\0}}=\bmatriz{2\\3},T_A\parr{\bmatriz{1\\2\\1}}=\bmatriz{2\\3},T_A\parr{\bmatriz{0\\3\\0}}=\bmatriz{0\\3}\] lo cual genera una línea horizontal que cruza el eje $x_2$ en 3.
%
%
%

\subsection{Solución con valores arbitrarios}
Si en vez del vector-$2$ numérico $\ve{b}=\bsmatriz{2\\3}$ tenemos el vector-$2$ arbitrario $\ve{b}=\bsmatriz{b_1\\b_2}$ entonces $x_1=b_1-t$, $x_2=b_2-t$ y $x_3=t$.  

\begin{align}
\tag{una solución particular, con t=0}&\bmatriz{b_1\\b_2\\0}\\ 
\tag{la solución general}&\bmatriz{b_1-t\\b_2-t\\0+t}\\ 
\tag{el conjunto solución}&\set{\bmatriz{b_1-t\\b_2-t\\0+t} \mid t \in \R}% 
\end{align}

Observe que en este caso al cambiar los términos constantes no se afectaron ni los pivotes ni la forma de despejar las variables.



\newslide
\section{Ejemplo de matriz con renglón de ceros y con codominio en $\R^3$}
%Vamos ahora  a encontrar los valores que son transformados en el covalor  de $\R^3$ $\bsmatriz{2\\3\\0}$ por la transformación dada por la matriz $\bsmatriz{1 & 0 \\0&1\\0&0}$. Recordemos que el dominio de esta transformación es $\R^2$ y el codominio es $\R^3$.
Encontrmos nuevamente un vecotr-$2$ $\ve{x}$ 
\begin{align}
%\tag{$T_A(\ve{x})=\ve{b}$}&T_{\bsmatriz{1 &0\\0&1\\0&0}}(\bmatriz{x_1\\x_2})=\bmatriz{2\\3\\0}\\ 
\tag{$A\ve{x}=\ve{b}$}&\bmatriz{1 &0\\0&1\\0&0} \bmatriz{x_1\\x_2}=\bmatriz{2\\3\\0}\\ 
\tag{Matriz extendida}&\bmatriz{1 &0&:&2\\0&1&:&3\\0&0&:&0} \\ 
\tag{Sistema de ecuaciones}&\matriz{1x_1 &+& 0x_2 &=&2\\ 0x_1&+&1x_2&=&3\\ 0x_1&+&0x_2&=&0}  
\end{align} 
Este sistema de ecuaciones nos recuerda el de la matriz identidad, ya que tan sólo se le adicionó una ecuación trivial $0=0$. Sin embargo, al final del ejemplo, veremos una diferencia interesante.
\subsection{Clasificación de la matriz}
\begin{itemize}
\item (E1) porque el renglones de ceros está al final, 
\item (E2) porque las entradas $2,1$ y $3,1$ son cero y la $2,2$ no lo es,
\item (E3) porque los elementos delanteros $1,1$ y $2,2$ son uno,
\item (E4) porque la entrada $1,2$ también es cero.
\end{itemize}
%(E4) no se cumple porque la entrada $1,2$ no es cero. Lo cual es similar al ejemplo anterior.

\subsection{Solución del sistema lineal}
Despejando obtenemos que $x_1=2$, $x_2=3$, $0=0$. Podemos ignorar que $0=0$ y escribimos las soluciones
\begin{align}
\tag{una solución particular}&\bmatriz{2\\3}\\ 
\tag{la solución general}&\bmatriz{2\\3}\\ 
\tag{el conjunto solución}&\set{\bmatriz{2\\3} }% 
\end{align}
%Si en vez del vector-$2$ numérico $\ve{b}=\bsmatriz{2\\3}$ tenemos el vector-$2$ arbitrario $\ve{b}=\bsmatriz{b_1\\b_2}$ entonces las soluciones son
%\begin{align}
%\tag{una solución particular}&\bmatriz{b_1\\b_2\\0}\\ 
%\tag{la solución general}&\bmatriz{b_1-r\\b_2-r\\0-r}\\ 
%\tag{el conjunto solución}&\set{\bmatriz{b_1-r\\b_2-r\\0-r} \mid r \in \R }% 
%\end{align}
%
%Observe que en este caso al cambiar los términos constantes tampoco se afectaron ni los pivotes ni la forma de despejar las variables.
%
%--------------------------------
%
%Como cumple (E1) y (E2) la matriz está en forma escalón y podemos identificar los elementos delanteros, en este caso en el primer renglón el elemento delantero (o pivote) es la entrada $1,1$. Este elemento delantero está asociado a la variable $x_1$ la cual se puede despejar en términos de $x_3$ (ya que el coeficiente de $x_2$ es cero). \[x_1=2-1x_3\] 
%
%En el segundo renglón el elemento delantero (o pivote) es la entrada $2,2$. Este elemento delantero está asociado a la variable $x_2$ la cual se puede despejar en términos de $x_3$ (ya que el coeficiente de $x_1$ es cero). \[x_2=3-1x_3\]
%
%En este caso no hay un renglón de la matriz (o una ecuación) que permita despejar $x_3$. Esto se debe a que $x_3$ no es una variable delantera, es decir que no hay pivote en la columna de $x_3$. Por este motivo $x_3$ puede tomar cualquier valor real $x_3 \in \R$, en este caso decimos que $x_3$ es un \de{parámetro} y por lo tanto hay infinitas soluciones. Algunos autores le asignan otra letra al parámetro por ejemplo $x_3=t$
%
%Como cumple (E3) la matriz está en forma escalonada lo cual facilita el despejar las variables $x_1$ y $x_2$.
%
%Como cumple (E4) la matriz está en forma escalón reducida y esto hace que sea inmediato despejar $x_2$.
%
%Ahora si despejamos las variables obtenemos las siguientes soluciones.
%
%\begin{align}
%\tag{una solución particular}&\bmatriz{2\\3\\0}\\ 
%\tag{la solución general}&\bmatriz{2-r\\3-r\\r}\\ 
%\tag{el conjunto solución}&\set{\bmatriz{2-r\\3-r\\r} \mid r \in \R }% 
%\end{align}
%En este caso la solución no es única y ya no coinciden la solución particular con la general.
%
%------------------------------

%-----------graficar solución y transformación
\subsection{Representación con valores arbitrarios}\label{ejem_no_genera}
Si en vez del vector-$2$ numérico $\ve{b}=\bsmatriz{2\\3\\0}$ tenemos el vector-$2$ arbitrario $\ve{b}=\bsmatriz{b_1\\b_2\\b_3}$ entonces al despejar obtenemos $x_1=b_1$, $x_2=b_2$, $0=b_3$. Vamos a mirar en detalle que significa que $0=b_3$. 

A primera vista podríamos concluir que $b_3$ es cero, pero antes habíamos dicho que $b_3$ era un número arbitrario, entonces entramos en una contradicción. Como $\ve{b}=\bsmatriz{b_1\\b_2\\b_3}$ puede tener cualquier valor vamos a considerar otro valor diferente de $\bsmatriz{2\\3\\0}$, por ejemplo consideremos ahora el valor $\bsmatriz{2\\3\\4}$. En este caso obtenemos
%\begin{align}
%\tag{$T_A(\ve{x})=\ve{b}$}&T_{\bsmatriz{1 &0& 1\\0&1&1}}(\bmatriz{x_1\\x_2\\x_3})=\bmatriz{2\\3}\\ 
%\tag{$A\ve{x}=\ve{b}$}&\bmatriz{1 &0& 1\\0&1&1} \bmatriz{x_1\\x_2\\x_3}=\bmatriz{2\\3}\\ 
%\tag{Matriz extendida}&\bmatriz{1 &0& 1&:&2\\0&1&1&:&3} \\ 
%\tag{Sistema de ecuaciones}&\matriz{1x_1 &+& 0x_2 &+&1 x_3&=&2\\0x_1&+&1x_2&+&1x_3&=&3}  
%\end{align} 
\begin{align}
%\tag{$T_A(\ve{x})=\ve{b}$}&T_{\bsmatriz{1 &0\\0&1\\0&0}}(\bmatriz{x_1\\x_2})=\bmatriz{2\\3\\4}\\ 
\tag{$A\ve{x}=\ve{b}$}&\bmatriz{1 &0\\0&1\\0&0} \bmatriz{x_1\\x_2}=\bmatriz{2\\3\\4}\\ 
\tag{Matriz extendida}&\bmatriz{1 &0&:&2\\0&1&:&3\\0&0&:&4} \\ 
\tag{Sistema de ecuaciones}&\matriz{1x_1 &+& 0x_2 &=&2\\ 0x_1&+&1x_2&=&3\\ 0x_1&+&0x_2&=&4}  
\end{align} 
La última ecuación es $0=4$! esto si es definitivamente una contradicción, lo cual significa que ningún valor de $\ve{x}$ es solución del sistema de ecuaciones, dicho de otro modo ningún valor de $\R^2$  corresponde al covalor $\bsmatriz{2\\3\\4}$. En este caso %no hay ni solución particular ni solución general y el conjunto solución es el conjunto vacío $\{\}$.
las soluciones quedan
\begin{align}
\tag{una solución particular}&\text{no hay}\\ 
\tag{la solución general}&\text{no hay}\\ 
\tag{el conjunto solución}&\text{conjunto vacío }\set{ }% 
\end{align}

Es de resaltar que en este caso al cambiar los términos constantes SI se afectaron los pivotes, ya que en los ejemplos anteriores la última columna de la matriz extendida (correspondiente al término independiente) nunca era un pivote, pero en este ejemplo sí. Claro que si $b_3=0$, entonces la última columna no tiene pivote y por lo tanto el sistema sí tiene solución. Pero si $b_3 \neq 0$ entonces la última columna tiene pivote y el sistema no tiene solución.

%            \newslide
%\section{Ejemplo de matriz que no cumple (E3), con parámetros y renglón de ceros}

En general, para una matriz $A \in \R^{m \times n}$, si el sistema de ecuaciones $A \ve{x}=\ve{b}$ tiene solución para cualquier vector arbitrario $\ve{b}$ entonces se dice que \de{las columnas de $A$ generan a $\R^m$} o simplemente que \de{$A$ genera a $\R^m$}. De manera análoga se define que \de{los renglones de $A$ generan a $\R^n$} si \de{$A^T$ genera a $\R^n$.} 

\teorema{Una matriz $A \in R^{m \times n}$ genera a $\R^m$ si y sólo si $A$ tiene $m$ pivotes.}
\teorema{Una matriz $A \in R^{m \times n}$ en forma escalón genera a $\R^m$ si y sólo si $A$ no tiene renglones de ceros.}
\teorema{Una matriz $A \in R^{m \times n}$ que tiene más renglones que columnas no puede generar a $\R^m$.}

\section{Resumen}

De los anteriores ejemplos podemos aprender lo siguiente \linebreak
\teorema{En una matriz extendida en forma escalón:
\begin{itemize}
 \item si la última columna es pivote entonces no tiene solución y la matriz se llama \de{inconsistente}.
 \item si la ultima columna no tiene pivote entonces sí tiene solución y la matriz se llama \de{consistente}. En estas matrices:
 \begin{itemize}
  \item si no tiene parámetros la \de{solución es única}.
  \item si tiene parámetros tiene \de{infinitas soluciones}.
 \end{itemize}
\end{itemize}}  
%Una matriz extendida no tiene solución si la última columna es pivote, en este caso la matriz extendida es \de{inconsistente}. De lo contrario es \de{consistente} y tiene \de{solución única} si no tiene variables parámetros. Una matriz extendida consistente con parámetros tiene \de{infinitas soluciones}.}

Además, en esta sección se debe identificar si una matriz extendida tiene las propiedades (E1), (E2), (E3) y (E4) y concluir si una matriz está en forma escalón, escalonada o escalón reducido. Se debe encontrar los pivotes y determinar los renglones y columnas con pivotes y las variables delanteras y los parámetros. Determinar si el sistema de ecuaciones asociado es inconsistente o consistente con solución única o con infinitas soluciones.  También se debe relacionar la matriz extendida con el producto de matriz por vector.% o con una transformación matricial en busca de los valores dado un covalor.

\section{Multiplicación de matrices elementales por matrices extendidas} 


En la sección de la operación de multiplicación entre matrices se vieron unos ejemplos de como se alteraban los puntos de un dibujo al ser multiplicados por matrices elementales. En esta sección veremos como se alteran las matrices extendidas y sus soluciones al ser multiplicadas por matrices elementales.

\begin{description}
%\item[Identidad]  
%\[\begin{bmatrix}
%1 & 0  \\
%0 & 1  
%\end{bmatrix}
%\bmatriz{x_1\\y_1}
%=x_1\bmatriz{1\\0}
%+y_1\bmatriz{0\\1}
%=\bmatriz{x_1\\0}
%+\bmatriz{0\\y_1}
%=\bmatriz{x_1\\y_1}
%%=\bmatriz{x_2\\y_2}
%\]

\item[Escalamiento del primer componente con $k=2$]  
Recordemos como se afecta cada columna en esta operación

\[\begin{bmatrix}
k & 0  \\
0 & 1  
\end{bmatrix}
\bmatriz{x_1\\x_2}
=x_1\bmatriz{k \\0}
+x_2\bmatriz{0\\1}
=\bmatriz{k \ x_1\\x_2}
%=\bmatriz{x_2\\y_2}
\]

y para $k=2$ la matriz extendida es transformada de la siguiente forma
\begin{align*}
%T_{\bsmatriz{1 & 1\\0&1}}(\bmatriz{1 & 1&:&2\\0&1&:&3})
&\bmatriz{2 & 0\\0&1}\bmatriz{1 & 1&:&2\\0&1&:&3}
=\bmatriz{2 & 2&:&4\\0&1&:&3}
\intertext{la cual corresponde al sistema de ecuaciones}
&\matriz{2x_1 &+& 2x_2&=&4\\0x_1&+&1x_2&=&3}
\end{align*}
La segunda ecuación nuevamente es la línea horizontal que cruza el eje de $x_2$ en 3. La primera ecuación se multiplico por dos a ambos lados de la ecuación, lo que no afecta la solución, por lo tanto queda nuevamente la misma ecuación. Entonces la intersección de las dos líneas es la misma  $\bsmatriz{3\\-1}$ 




\item[Escalamiento del segundo componente]  
En el escalamiento del segundo componente sucede algo similar al escalamiento del primer componente y por lo tanto no se altera la solución del sistema de ecuaciones.

\item[Intercambio de componentes]
%\[\begin{bmatrix}
%0 & 1  \\
%1 & 0  
%\end{bmatrix}
%\bmatriz{x_1\\y_1}
%=\bmatriz{y_1\\ x_1}
%%=\bmatriz{x_2\\y_2}
%\]

Al ser multiplicado por $\bsmatriz{0 & 1 \\1 & 0}$ la matriz extendida  intercambian los renglones 
\begin{align*}
%T_{\bsmatriz{1 & 1\\0&1}}(\bmatriz{1 & 1&:&2\\0&1&:&3})
&\bmatriz{0 & 1\\1&0}\bmatriz{1 & 1&:&2\\0&1&:&3}
=\bmatriz{0&1&:&3\\1 & 1&:&2}
\intertext{lo cual corresponde a intercambiar las ecuaciones}
&\matriz{0x_1&+&1x_2&=&3\\1x_1 &+& 1x_2&=&2}
\end{align*}

Por supuesto, esto no genera ningún cambio en la solución del sistema de ecuaciones.


\item[Sumando a la primera componente un múltiplo ($k=1$) de la segunda]  
Recordemos que con esta operación cada vector es transformado de la siguiente forma
\[\begin{bmatrix}
1 & k  \\
0 & 1  
\end{bmatrix}
\bmatriz{x_1\\x_2}
=\bmatriz{x_1+(k \ x_2)\\x_2}
%=\bmatriz{x_2\\y_2}
\]

Con $k=1$, la transformación del la matriz extendida es
\begin{align*}
%T_{\bsmatriz{1 & 1\\0&1}}(\bmatriz{1 & 1&:&2\\0&1&:&3})
&\bmatriz{1 & 1\\0&1}\bmatriz{1 & 1&:&2\\0&1&:&3}
=\bmatriz{1 & 2&:&5\\0&1&:&3}
\intertext{la cual corresponde al sistema de ecuaciones}
&\matriz{1x_1 &+& 2x_2&=&5\\0x_1&+&1x_2&=&3}
\end{align*}
La segunda ecuación nuevamente es la línea horizontal que cruza el eje de $x_2$ en 3. La primera ecuación resulta ser la suma de las ecuaciones originales. Para dibujarla despejamos $x_1$ \[x_1=5-2x_2\] y tabulamos algunos valores, por ejemplo si $x_2=0$, $x_1=5$ pero si $x_2=1$ entonces $x_1=3$. Por lo tanto a segunda línea pasa por los valores $\bsmatriz{5\\0}$ y $\bsmatriz{3\\1}$. 


\scalebox{1} % Change this value to rescale the drawing.
{
\psset{xunit=0.50cm,yunit=0.50cm}
\begin{pspicture}(0,-4.79)(17.02,4.79)
\rput(2.0,-2.77){\psaxes[linewidth=0.04](0,0)(-2,-2)(4,4)}
\usefont{T1}{ptm}{m}{n}
\rput(5.621406,-2.26){$x_1$}
\usefont{T1}{ptm}{m}{n}
\rput(2.6214063,0.84){$x_2$}
\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm](6.0,0.23)(0.0,0.23)
\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm](6.0,-4.77)(0.0,1.23)
\rput(12.0,-2.77){\psaxes[linewidth=0.04](0,0)(-2,-2)(5,4)}
\psbezier[linewidth=0.04,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{>->}(1.0,0.27)(1.0,4.77)(11.0,4.77)(11.0,0.27)
\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm](17.0,0.27)(10.0,0.27)
\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm](17.0,-2.73)(10.0,0.77)
\usefont{T1}{ptm}{m}{n}
%\rput(6.391406,4.58){$T_A$}
\end{pspicture} 
}


Sin embargo, se puede apreciar que al sumar las dos ecuaciones se genera una nueva ecuación cuya solución pasa entre las soluciones las dos ecuaciones originales.



\item[Sumando a la segunda componente un múltiplo de la primera]  
%\[\begin{bmatrix}
%1 & 0  \\
%k & 1  
%\end{bmatrix}
%\bmatriz{x_1\\y_1}
%=\bmatriz{x_1\\y_1+(k\ x_1)}
%%=\bmatriz{x_2\\y_2}
%\]

Al sumar al segundo componente un múltiplo del primero sucede algo similar que el caso anterior y por lo tanto no se altera la solución del sistema de ecuaciones.


\end{description}
Los anteriores ejemplos permiten definir las \de{operaciones elementales} %las cuales permiten realizar modificaciones en una matriz extendida manteniendo la misma solución y posición de los pivotes.
%ilustrar un resultado importante para poder encontrar la solución (si la hay) de cualquier sistema de ecuaciones. 


\teorema{%Al multiplicar una matriz elemental por una matriz extendida no se cambia la solución}
 \dctb{$cR_i \rightarrow R_i$}{Renglón por constante}{Al multiplicar un reglón de una matriz extendida por una constante diferente de cero 
 %las posiciones de los pivote 
 no se altera la solución}
%  \codigo{A(i,:)=c*A(i,:)}
%  \verb!A(i,:)=c*A(i,:)!
  \dctb{$R_i \leftrightarrow R_j$}{Intercambia renglones}{Al intercambiar dos reglones de una matriz extendida
  % las posiciones de los pivote 
  no se altera la solución} 
%  A continuación se define la función \verb!interc! en Scilab, la cual tiene tres parámetros: la matriz $A$, el renglón $i$ y el renglón $j$; y devuelve la matriz $B$. Está función  copia $A$ en $B$ y luego intercambia en $B$ los renglones $i$ e $j$ de $A$. 
%
%%  \codigo{deff("B=interc(A,i,j)",["B=A";// %"B(j,:)=A(i,:)";"B(i,:)=A(j,:)"])}
%  
%\begin{verbatim}
%deff("B=interc(A,i,j)",["B=A"; 
%"B(j,:)=A(i,:)";"B(i,:)=A(j,:)"])
%\end{verbatim}  
%Si la función se usa de la siguiente forma intercambia los renglones  1 y 3 de $A$.
%
%\verb!A=interc(A,1,3)!
  \dctb{$R_j + k R_i \rightarrow R_j$}{Suma  múltiplo de un renglón a otro}{En una matriz extendida, al sumar un múltiplo de un reglón a otro 
  %las posiciones de los pivote 
  no se altera  la solución} 
%\verb!{A(j,:)=A(j,:)+c*A(i,:)!
}
Además, cada uno de estos resultados se puede obtener por medio de una multiplicación matricial de \de{matrices elementales}, que se construyen al realizar una operación elemental sobre la matriz identidad.

Dos matrices ($A$ y $B$) son \de{equivalentes} si una ($A$) se puede obtener de la otra ($B$) a partir de aplicar operaciones elementales entre renglones y se denota $A \sim B$.

%\teorema{Las siguientes operaciones elementales entre renglones no alteran el conjunto solución de una matriz extendida
%\begin{description}
%\item[Intercambio] Intercambie dos renglones ($R_i \lra R_j$).
%\item[escalamiento] Multiplique un renglón por una constante distinta de cero ($cR_i \ra R_i$).
%\item[Sumar múltiplo] Sume un múltiplo de un renglón a otro ($R_i + cR_j \ra R_i$).
%\end{description}}


%\chapter{Solución de cualquier matriz extendida}
\section{Eliminación de Gauss}
%Debido a que con las operaciones elementales se puede transformar cualquier matriz extendida, entonces 
En esta sección vamos a usar las operaciones elementales para transformar cualquier matriz extendida en matrices en forma escalón o escalón reducido por medio de la eliminación de Gauss. \cite{NJ99}




%A partir de las operaciones elementales se puede convertir cualquier matriz en forma escalón reducida con el algoritmo de Gauss que se transcribe de \cite{NJ99}

\teorema{Los siguientes cuatro pasos permiten convertir una matriz en forma escalón y el último paso la convierte en forma escalón reducida.
\begin{enumerate}[(1)]
\item Vaya a la columna no cero extrema izquierda.
\item Si el primer renglón tiene un cero en la columna del paso (1), intercámbielo con uno que tenga un elemento no cero en la misma columna.
\item Obtenga ceros abajo del elemento delantero, sumando múltiplos adecuados del renglón superior a los renglones debajo de él.
\item Cubra el renglón superior y repita el mismo proceso comenzando por el paso (1) aplicado a la submatriz restante. Repita este proceso con el resto de los renglones.
\item Comenzando con el último renglón no cero, avance hacia arriba: para cada renglón obtenga un 1 delantero e introduzca ceros arriba de él, sumando múltiplos adecuados a los renglones correspondientes. 
\end{enumerate}}

\teorema{Cualquier par de matrices extendidas equivalentes, en forma escalón, tienen las mismas posiciones pivotes, por lo tanto coinciden sus columnas pivotes y los parámetros}% Una columna de una matriz que no está en forma escalón es libre o pivote si esa columna es libre o pivote en una matriz escalón equivalente.}


\teorema{Una matriz extendida es equivalente a muchas matrices en forma escalón pero sólo a una matriz escalón reducida.} %Además, todas las matrices escalón equivalentes tienen las mismas columnas pivote y libres.}

\teorema{La matriz de coeficientes $A$ de tamaño $n \times n$ tiene $n$ pivotes si y sólo si $A$ es equivalente a la identidad.}

\teorema{Si una matriz en forma escalón $A \in \R^{m \times n}$ genera a $R^m$ entonces cualquier matriz equivalente genera a $R^m$}

En una matriz extendida cualquiera $A$ una columna, o su variable asociada, es \de{pivotes} (o \de{parámetro}) si esa misma columna es pivote (o \de{parámetro} respectivamente) en una matriz extendida en forma escalón $B$ equivalente ($A \sim B$).

Se recomienda estudiar \cite[Secciones 1.1 y 1.2]{NJ99}
y hacer sus ejercicios.

%\section{Ejemplo de despejar sistema de ecuaciones}
\section{Independencia lineal}
En \cite[Ejemplo 1.2.16]{NJ99} se presenta como se aplica la eliminación de Gauss de tal forma que la matriz
\[\bmatriz{
0 & 3&- 6&-4&-3&: &-5\\
-1& 3&-10&-4&-4&: &-2\\
4 &-9& 34& 0& 1&: &-21\\
 2&-6& 20& 2& 8&: &-8}
\]
queda transformada en la matriz en forma escalón
\[\bmatriz{
-1& 3&-10&-4&-4&:&- 2\\
0 & 3&- 6&-4&-3&:&- 5\\
0 & 0&  0&-12& -12&:&-24\\
 0& 0&  0& 0& 6&:&- 0}
\]
y en la matriz en forma escalón reducida 
\[\bmatriz{
1 & 0&  4& 0& 0&:& -3\\
0 & 1&- 2& 0& 0&:& 1\\
0 & 0&  0& 1& 0&:& 2\\
 0& 0&  0& 0& 1&:& 0}
\]
Observe que las posiciones pivote de las dos últimas matrices son las mismas, aunque los valores cambian. Por este motivo en las \underline{tres} matrices las columnas pivotes son la primera, la segunda, la cuarta y la quinta. La tercera columna corresponde a un parámetro. \[x_3=t\] Despejando las variables obtenemos:
\[\bmatriz{x_1\\x_2\\x_3\\x_4\\x_5}=\bmatriz{-3-4t\\1+2t\\0+t\\2+0t\\0+0t}\]

\teorema{Las columnas que corresponden a parámetros se pueden escribir como una combinación lineal de las columnas pivote de su izquierda (dependen de las columnas pivote). Por el contrario, las columnas pivote no se pueden escribir  como combinación lineal de las otras columnas pivote (no dependen de las otras columnas pivote)} 


%\subsection{Independencia lineal}
Las columnas de una matriz $A$ son \de{linealmente dependientes} (\de{L. D.}) si una de ellas se puede escribir como combinación lineal de las otras, en este caso decimos que $A$ es linealmente dependiente. Si ninguna columna se puede escribir como combinación lineal de las otras entonces son \de{linealmente independientes} (\de{L. I.}). En este caso decimos que $A$ es linealmente independiente.

\teorema{Las columnas de una matriz $A$ son \de{linealmente dependientes} si y sólo si el sistema homogéneo $[A:0]$ tiene infinitas soluciones. Las columnas de una matriz $A$ son \de{linealmente independientes} si y sólo si el sistema homogéneo $[A:0]$ tiene solución única.}

Es de resaltar que cuando decimos que los vectores $\ve{v_1}$, $\ve{v_2}$, \ldots $\ve{v_n}$ son  L. I. nos referimos a que la matriz $[\ve{v_1}$, $\ve{v_2}$, \ldots $\ve{v_n}]$ es L. I., lo mismo aplica para L. D. 

Los renglones de una matriz $A$ son linealmente independientes (o dependientes) si las columnas de $A^T$ son linealmente independientes (o dependientes respectivamente). En este caso se dice que %$A^T$ es linealmente independiente (o dependiente) o que 
 $A$ es linealmente independiente por renglones (o dependiente por renglones).
 
 

Por ejemplo, para la tercera matriz se tiene que 
\[4\bmatriz{1\\0\\0\\0}-2\bmatriz{0\\1\\0\\0}=\bmatriz{4\\-2\\0\\0}\]
para la segunda matriz se tiene
\[4\bmatriz{-1\\0\\0\\0}-2\bmatriz{3\\3\\0\\0}=\bmatriz{-10\\-6\\0\\0}\]
y para la primera matriz
\[4\bmatriz{0\\-1\\4\\2}-2\bmatriz{3\\3\\-9\\-6}=\bmatriz{-6\\-10\\34\\20}\]
Los coeficientes ($4$ y $-2$) coinciden en las tres porque los tres sistemas son equivalentes (ya que uno se obtiene del otro aplicando operaciones elementales de \cite[Ejemplo 1.2.16]{NJ99}) y los sistemas equivalentes tienen la misma solución $\bsmatriz{x_1\\x_2}=\bsmatriz{-4\\-2}$.

De aquí se concluye que las columnas de la matriz $A=\bsmatriz{0 & 3 \\-1 & 3 \\ 4 & 9 \\ 2 & -6 }$ son linealmente independientes. %(o $A$ es linealmente independiente). 
Pero los renglones de $A$ son linealmente dependientes %o que $A$ es linealmente dependiente por renglones, 
porque el cuarto renglón es un múltiplo escalar del segundo. 
% \[A=\bmatriz{0 & 3 \\-1 & 3 \\ 4 & 9 \\ 2 & -6 }\] 
 Las columnas de    
\[\bmatriz{0 & 3 & -6 \\-1 & 3 & -10 \\ 4 & 9 & 34 \\ 2 & -6 & 20 }\]  son linealmente dependientes, al igual que las columnas de  \[\bmatriz{
0 & 3&- 6&-4&-3&\\
-1& 3&-10&-4&-4&\\
4 &-9& 34& 0& 1&\\
 2&-6& 20& 2& 8&}
\]

\teorema{Si una matriz $A$ es linealmente independiente, entonces si se eliminan columnas también es linealmente independiente. Por otro lado si una matriz $A$ es linealmente dependiente, entonces si se aumentan columnas también es linealmente dependiente.}

---\teorema{Una matriz que tiene más columnas que renglones no puede ser linealmente independiente}

En \cite[Ejemplo 1.2.18]{NJ99} es similar a \cite[Ejemplo 1.2.16]{NJ99} pero se presentan dos parámetros.


\section{Generadores y bases de $\R^m$}

En la sección \ref{ejem_no_genera} habíamos visto un  sistema de ecuaciones que era inconsistente para valores arbitrarios de los términos constantes $\ve{b}$, lo cual permitió obtener algunas definiciones y teoremas que repetimos a continuación.

  En general, para una matriz $A \in \R^{m \times n}$, si el sistema de ecuaciones $A \ve{x}=\ve{b}$ tiene solución para cualquier vector arbitrario $\ve{b}$ entonces se dice que \de{las columnas de $A$ generan a $\R^m$} o simplemente que \de{$A$ genera a $\R^m$}. De manera análoga se define que \de{los renglones de $A$ generan a $\R^n$} si \de{$A^T$ genera a $\R^n$.} 


%\teorema{Para una matriz $A \in \R^{m \times n}$ si $A$ tiene $m$ pivotes entonces $A$ genera a $\R^m$}
\teorema{Una matriz $A \in R^{m \times n}$ genera a $\R^m$ si y sólo si $A$ tiene $m$ pivotes.}
\teorema{Una matriz $A \in R^{m \times n}$ en forma escalón genera a $\R^m$ si y sólo si $A$ no tiene renglones de ceros.}
\teorema{Una matriz $A \in R^{m \times n}$ que tiene más renglones que columnas no puede generar a $\R^m$.}


%Observe que para que una matriz $A \in R^{m \times n}$ genere a $\R^m$ se requiere que  

Esto permite definir una \de{base} de $\R^m$ como un conjunto de vectores L.I. y que genera a $\R^m$. Si las columnas de una matriz $A$ forman una base de $\R^m$ entonces decimos que $A$ es una base de $\R^m$.

\teorema{Las bases de $\R^m$ son matrices cuadradas de orden $m$}


------------


Incluir definición de rectas planos cuando se ven los parámetros
%\section{Ejemplo de despejar sistema de ecuaciones con solución arbitraria}
%
%\[\bmatriz{
%2 & -18& 20&-4&-12&:&a\\
%-1& 3&-10&2&-2&:&b}
%\]

%\section{semi-inversas e inversas}


calculo de la pseudo inversa cuando no hay parámetros y como soluciona algunos problemas cuando se requiere despejar varios valores

\chapter{Sistemas homogéneos y núcleo de la transformación}

 
%\chapter{Conjunto de vectores vs Transformaciones matriciales}




%\section{subespacios de $\R^n$}


%
% ---------------------- pasar eso a ecuaciones.
% 
%El  \dn{renglón} $i$ está asociado con la i-esima ecuación y en Scilab se selecciona así 
%
%%\codigo{A(i,:)}
%\verb!A(i,:)!
%
% Una \dn{columna} $j$ está asociada con la variable j-esima y en Scilab  se describe así 
% 
%% \codigo{A(:,j)} 
% \verb!A(:,j)! 
% 
% La columna de la derecha  tiene los términos constantes. Para ver esta columna en Scilab se puede usar el símbolo  \verb!$!  que se utiliza para indicar el último valor.
% 
%% \codigo{A(:,\$)} 
% \verb!A(:,$)! 
%
%
% 
% %, a esta última columna columna hace referencia la palabra `aumentada'.
%


\section{Imagen de una transformación matricial y subespacios generados en $\R^n$}

Respecto al ejemplo ???????? nos podemos preguntar si con esos motores podemos dibujar toda la hoja, y si no, entonces que parte de la hoja es la que podemos dibujar.
%la imagen corresponde a todos los puntos que se pueden pintar con esos motores, y el rango corresponde al menor número de motores que son necesarios para poder pintar lo mismo.

%-------------sobre 1a1 iso LI genera espacio

Para una transformación de la matriz $A=[\ve{v_1},\ve{v_2}, \ldots ,\ve{v_n}]$
\[T_A:\R^n \ra \R^m\]
\[T_A\left(\bmatriz{c_1\\c_2\\ \vdots \\ \vdots \\ c_n }\right)=c_1\ve{v_1}+c_2\ve{v_2}+ \ldots +c_n\ve{v_n}\]


Recordemos que $\R^n$ se llama el {dominio} de $T_A$ y que $\R^m$ es el {codominio} de $T_A$. Vamos a definir la \de{imagen} de $T_A$ como el 
%subconjunto del codominio al cual es transformado algún elemento del dominio. Es decir 
%El \de{subespacio generado por los vectores-$m$} $\ve{v_1},\ve{v_2}, \ldots ,\ve{v_n}$ 
 {conjunto} de vectores que se pueden escribir como combinación lineal de $\ve{v_1},\ve{v_2}, \ldots ,\ve{v_n}$, también se le llama el \de{subespacio generado}
 por las columnas de $A$, denotado por $\< \ve{v_1},\ve{v_2}, \ldots ,\ve{v_n} \>$ o por $Gen\{\ve{v_1},\ve{v_2}, \ldots ,\ve{v_n}\}$, otro nombre es el \de{subespacio columna} de $A$, ($Col(A)$). A continuación se presenta los diferentes nombres del mismo conjunto.
%\[  Im(T_A):=
%\{ \] 

%Al subespacio generado  por los vectores-$m$ de una matriz $A=[\ve{v_1} \ \ve{v_2} \ \ldots \ \ve{v_n} ]$ 
.
\begin{align*}&Im(T_A)=\{T_A(\ve{c}) \mid \ve{c} \in \R^m\}=\{A\ve{c} \mid \ve{c} \in \R^m\}=Gen\{\ve{v_1},\ve{v_2}, \ldots ,\ve{v_n}\}\\&=\< \ve{v_1},\ve{v_2}, \ldots ,\ve{v_n} \> =Col(A)=\{c_1\ve{v_1}+c_2\ve{v_2}+\ldots+c_n\ve{v_n}\mid
c_1,c_2, \ldots ,c_n \in \R \}\end{align*}

Una transformación matricial $T_A$ se dice que es \de{sobre} si la imagen es el codominio, ?????????en este caso también se dice que las columnas de $A$ \de{generan el espacio} $\R^m$.???????????NO?????????

%Si $Gen\{\ve{v_1},\ve{v_2}, \ldots ,\ve{v_n}\}=\R^m$ se dice que los vectores generan todo el espacio $\R^m$ además se dice que la transformación dada por $T \ve{x}=[\ve{v_1},\ve{v_2}, \ldots ,\ve{v_n}]\ve{x}$ es sobre.


El \de{subespacio renglón} de una matriz $A$ corresponde al espacio columna de la transpuesta de $A$, ($Ren(A)=Col(A^T)$)

Por simplicidad a estos subespacios algunas veces se les llama espacios. 

El \de{rango} de $T_A$ o simplemente el \de{rango} de $A$ ???????Es la dimensión de la imagen de $T_A$??????% o la \de{dimensión del espacio generado por las columnas} de $A$ son diferentes nombres para el
es el número de pivotes de $A$ y se denotan $R(T_A)$ o $R(A)$ respectivamente. Recordemos que en el ejemplo de los motores de la sección ????? se dijo que el rango corresponde al menor número de motores necesarios para poder dibujar los mismos puntos.

Para que $T_A$ pueda llegar a cualquier punto del codominio, el rango de $A$ debe ser igual a la dimensión del codominio, ????????es decir al número de renglones de $A$,???????? en este caso se dice que $T_A$ es \de{sobre}.




Se sugiere mirar el ejemplo 20 y 21 de la sección 2.3 de la página 90 y los ejercicios 1 y 2. 



\section{Núcleo  de una transformación de una matriz escalón}
En la sección ????? nos preguntamos, con otras palabras, que dado un covalor  $\ve{b}$ y una transformación matricial $T_A$ ¿cuáles son los valores del dominio $\ve{x}$ que son transformados en $\ve{b}=T_A(\ve{x})$?. En esta sección repetiremos la misma pregunta pero no con cualquier covalor $\ve{b}$ sino que nos preguntaremos ¿cuáles son los valores del dominio $\ve{x}$ que son transformados en $\ve{0}=T_A(\ve{x})$?

En términos de los motores la pregunta queda, ¿cómo se pueden mover los motores para que el punto final permanezca en el origen?.
La respuesta trivial consiste en no mover los motores, pero ¿existirán otras opciones? y ese resultado ¿lo puedo extrapolar a otros puntos?.
%Retomando el ejemplo ???????? nos podemos preguntar si con esos motores podemos dibujar toda la hoja, y si no, entonces que parte de la hoja es la que podemos dibujar.
Para resolver estas inquietudes vamos a introducir las siguientes definiciones.

Si todos los términos constantes son cero el sistema se llama \de{sistema homogeneo}. Un sistema que no es homogéneo $A \ve{x} = \ve{b}$ tiene el \de{sistema homogéneo asociado} $A \ve{x} = \ve{0}$.

\teorema{Un sistema homogéneo siempre es consistente}.\\

El \de{núcleo} o \de{subespacio nulo} de $T_A$ %de $m \x n$ 
contiene todos los vectores-$n$ $\ve{x}$ tales que $A \ve{x} = \ve{0}$. La \de{nulidad} de $T_A$ ??? la dimensión del espacio nulo???? es el número de parámetros de $A$ y se denota $N(T_A)$ o simplemente $N(A)$. Si la nulidad es cero entonces ya vimos que el sistema tiene solamente la solución trivial. En este caso $T_A$ se llama  \de{1 a 1} o \de{inyectiva} y las columnas de $A$ son \de{linealmente independientes} (L. I.), las columnas de $A$ que no son linealmente independientes se llaman \de{linealmente dependientes} (L. D.). Una transformación sobre y 1 a 1 se llama \de{isomorfismo}. En el ejemplo de los motores de la sección ???? se tiene que la única posibilidad de dibujar el origen es no prender los motores. 


En un sistema de ecuaciones $A\ve{x}=\ve{b}$, la solución general corresponde a una solución particular $\ve{p}$ ($A\ve{p}=\ve{b}$) más la solución general del sistema homogéneo asociado $A\ve{x}=\ve{0}$. Esto corresponde a \dn{trasladar} el núcleo de $T_A$ a $\ve{p}$.

\section{Algunos subespacios de $\R^2$ y $\R^3$}

Una \de{recta que pasa por el origen} es el generado por un vector diferente de cero.
En su respectivo espacio ($\R^2$ o $\R^3$) 
\begin{itemize}
\item a la recta $\<\ve{i}\>$ se le conoce como el \de{eje x}, 
\item a la recta $\<\ve{j}\>$ se le conoce como el \de{eje y} 
\item en $R^3$ a la recta $\<\ve{k}\>$ se le conoce como el \de{eje z}. 
\end{itemize}
A estos ejes se les conoce como los ejes coordenados.
 
Un \de{plano que pasa por el origen} es el generado por dos vectores no paralelos y diferentes de cero.  $\R^2$ es un plano. En $\R^3$ un plano nunca es $\R^2$ (pero si tiene la misma forma y por eso se dice que un plano es {isomorfo} a $\R^2$) en cambio 
\begin{itemize}
\item al plano $\<\ve{i},\ve{j}\>$ se le conoce como el \de{plano xy}, 
\item al plano $\<\ve{i},\ve{k}\>$ se le conoce como el \de{plano xz},  
\item al plano $\<\ve{j},\ve{z}\>$ se le conoce como el \de{plano yz}. 
\end{itemize} 
A estos planos se les conoce como los planos coordenados.


Ahora nos podemos preguntar que genera el origen y que genera dos vectores paralelos. En el primer caso, el origen genera el mismo origen y nada más. En el segundo caso, para saber que genera dos vectores paralelos vamos a usar el Teorema \cite[2.?.9]{NJ99} que dice:

\teorema{Si uno de los vectores-$m$ $\ve{v_1},\ve{v_2}, \ldots ,\ve{v_n}$ es una combinación lineal del resto 
 entonces el generador permanece igual si se elimina ese vector.}

%Si dos vectores son paralelos (ver definición en la sección \ref{sec_vectores}) entonces uno es combinación lineal del otro y por lo tanto se puede eliminar uno de ellos y queda el generado por un vector. 
Por lo tanto el generado por dos vectores paralelos también es una recta que pasa por el origen.

¿Qué pasa si aplicamos las transformaciones elementales al eje x en $\R^2$?: la identidad, los dos escalamientos y sumando la segunda componente a la primera. Estas transformaciones elementales no afectan al eje x. Pero el intercambio lo transforma en el eje y. Finalmente, sumando la primera componente a la segunda lo transforma en una recta a 45°. 


------ colocar aquí cap de espacio euclidiano

-----ejercicios


 
\chapter{Operaciones de matrices cuadradas o autotransformaciones}

Borrador de las notas de clase (Bajo licencia GPL), enviar las sugerencias a  gmunoz@udistrital.edu.co.  


Al trabajar con las matrices del mismo tamaño $n \times n$ tenemos la ventaja que la multiplicación matricial está definida entre cualquier par de matrices. Ahora nos podemos preguntar, para cada par de matrices $A,C \in \R^{n \times n}$ ¿existe una matriz $X$ tal que $AX=C$? 

%Corresponde a sistema de $n$ ecuaciones con $n$ incógnitas.
\section{Operación inversa}

Antes de definir la operación inversa vamos a resolver un sistema arbitrario de $2$ ecuaciones con $2$ incógnitas.

\begin{align*}
&\matriz{ax_1&+&bx_2&=&c&\\dx_1&+&ex_2&=&f}&&\bmatriz{a&b\\d&e}\bmatriz{x_1\\x_2}=\bmatriz{c\\f}
\end{align*}
{que corresponde a la siguiente matriz extendida en donde se introduce intencionalmente un desface entre $c$ y $f$ que será útil para calcular la inversa}
\begin{align*}
&\bmatriz{a&b&:&c&\\d&e&:&&f}\\
{R_2-\frac{d}{a}R_1\ra R_2}
&\bmatriz{a&b&:&c&&\\0&e-\frac{d}{a}b&:&-\frac{d}{a}c&+&f}\\
{\frac{a}{ea-db}R_2\ra R_2}
&\bmatriz{a&b&:&c&&\\0&1&:&-\frac{d}{ea-db}c&+&\frac{a}{ea-db}f}\\
{R_1-bR_2\ra R_2}
&\bmatriz{a&0&:&\frac{ea}{ea-db}c&-&\frac{ab}{ea-db}f\\0&1&:&-\frac{d}{ea-db}c&+&\frac{a}{ea-db}f}\\
{\frac{1}{a}R_1\ra R_1}
&\bmatriz{1&0&:&\frac{e}{ea-db}c&-&\frac{b}{ea-db}f\\0&1&:&-\frac{d}{ea-db}c&+&\frac{a}{ea-db}f}\\
\end{align*}
Observe que la solución del sistema de ecuaciones se pueden escribir de la siguiente forma
\begin{align*}
\bmatriz{x_1\\x_2}&=\bmatriz{\frac{e}{ea-db}c&-&\frac{b}{ea-db}f\\-\frac{d}{ea-db}c&+&\frac{a}{ea-db}f}\\
&=\bmatriz{\frac{e}{ea-db}&\frac{-b}{ea-db}\\ \frac{-d}{ea-db}&\frac{a}{ea-db}}\bmatriz{c\\f}\\
&=\frac{1}{ea-db}\bmatriz{{e}&{-b}\\ {-d}&{a}}\bmatriz{c\\f}\\
&=B\bmatriz{c\\f} \text{, con } B=\frac{1}{ea-db}\bmatriz{{e}&{-b}\\ {-d}&{a}}
\end{align*}

Si $ea-db \neq 0$ entonces hemos encontrado una matriz $B$ que permite despejar la ecuación $A\ve{x}=\ve{b}$, quedando la ecuación  $\ve{x}=B\ve{b}$. %LA matriz $B$se denota $A^{-1}$

Para el siguiente sistema de ecuaciones de $2 \times 2$, encuentre la matriz de coeficientes $A$, la matriz inversa $A^{-1}$,  despeje $\ve{x}$ y compruebe el resultado.
\begin{align*}
3x_1+2x_2&=4\\
1x_1+3x_2&=2
\end{align*}

Encontramos que la inversa de $A=\bsmatriz{a & b\\d & e}$ es $B=\frac{1}{ea-db}\bsmatriz{{e}&{-b}\\ {-d}&{a}}$.
 Observe que pasa al multiplicar las dos matrices (por supuesto asumiendo que $ea-db \neq 0$).
\begin{align*}
AB=I &&  BA=I
\end{align*}
%remplazar una ecuación en la otra  
%
%La siguientes no son ciertas????? Falta probar que si Ax=x entonces A=I????????????????????
%
%\[A\ve{x}=\ve{b} \ra A(B\ve{b})=\ve{b} \ra (AB)\ve{b}=\ve{b} \ra AB=I\]
%\[\ve{x}=B\ve{b} \ra \ve{x}=B(A\ve{x}) \ra \ve{x}=(BA)\ve{x} \ra BA=I\]
En términos generales decimos que si: 
\begin{itemize}
\item $A$ es una matriz de tamaño $n \times n$ y
\item se puede encontrar una matriz $B$ del mismo tamaño tal que $AB=I=BA$,
\end{itemize} 
entonces se dice que $A$ es \de{invertible} y $B$ se denomina la \de{inversa} de $A$ que se denota $A^{-1}$. 

\teorema{La matriz $A$ de tamaño $n \times n$ se puede invertir si y sólo si el sistema $A\ve{x}=\ve{b}$ tiene solución única para cada vector-$n$ $\ve{b}$. Esta solución única esta dada por \[\ve{x}=A^{-1}\ve{b}\] }

%---cambiar formato para teoremas

%En el caso de las matrices de $2 \times 2$ $\bsmatriz{a & b\\d & e}$ el número $ea-db$ determina si la matriz tiene o no inversa. En la siguiente sección encontraremos este número \dn{determinante} para cualquier matriz cuadrada que indica si la matriz tiene inversa. %A continuación se resume la operación inversa.

 

Al comienzo de la sección encontramos la inversa de $\bsmatriz{a & b\\d & e}$ resolviendo el sistema $\bsmatriz{a & b &: &c&\\d & e &: &&f}$. Pero debido a que $\bsmatriz{c\\f}=\bsmatriz{1 & 0\\0 & 1}\bsmatriz{c\\f}$, el procedimiento para calcular la inversa de la matriz $A$  usualmente se realiza extendiendo la matriz con la matriz identidad, de la siguiente forma.

\begin{align*}
&\bmatriz{a&b&:&1&0\\d&e&:&0&1}\\
{R_2-\frac{d}{a}R_1\ra R_2}
&\bmatriz{a&b&:&1&0&\\0&e-\frac{d}{a}b&:&-\frac{d}{a}&1}\\
{\frac{a}{ea-db}R_2\ra R_2}
&\bmatriz{a&b&:&1&0&\\0&1&:&-\frac{d}{ea-db}&\frac{a}{ea-db}}\\
{R_1-bR_2\ra R_1}
&\bmatriz{a&0&:&\frac{ea}{ea-db}&\frac{-ab}{ea-db}\\0&1&:&\frac{-d}{ea-db}&\frac{a}{ea-db}}\\
{\frac{1}{a}R_1\ra R_1}
&\bmatriz{1&0&:&\frac{e}{ea-db}&\frac{-b}{ea-db}\\0&1&:&\frac{-d}{ea-db}&\frac{a}{ea-db}}\\
\end{align*}


Este procedimiento se puede generalizar para matrices cuadradas de cualquier tamaño. %Si la matriz no es invertible entonces el lado izquierdo no resulta equivalente a la matriz identidad. %, lo cual veremos más adelante.

\teorema{La matriz $A$ de tamaño $n \times n$ se puede invertir si y sólo si $A$ tiene $n$ pivotes.}



Resumiendo.
\defOpMat{A^{-1}}{Inversa de una matriz $A$ de orden $n$}
{si es cuadrada y tiene $n$ pivotes.}
{size(A)}
{b_{ij}$ que se calcularán usando Gauss-Jordan o la adjunta$ }  
{}\verb!inv{A}!





%En Scilab la inversa se escribe \verb!inv(A)!. 
Para las matrices $A$ y $B$, es un error de notación escribir $A/B$, porque como la multiplicación matricial no es conmutativa, entonces $A/B$ se puede interpretar de dos formas o $B^{-1}A$ o $AB^{-1}$. En Scilab \verb!A/B! equivale a \verb!A*inv(B)! y \verb!A\B! equivale a  \verb!inv(A)*B!. ¡Esta notación es únicamente para Scilab NO PARA LA CLASE!. De manera análoga a la multiplicación \verb!A.*B!, las divisiones  \verb!A./B! y \verb!B.(A)! dividen elemento a elemento dos matrices del mismo tamaño.



Usando la eliminación de Gauss encuentre la inversa de las siguientes matrices, si es posible.
\begin{align*}
\bmatriz{1 & 2 & 3\\3 & 1 & 2\\2 & 3 & 1} & &
\bmatriz{1 & 2 & 3\\2 & 4 & 6\\3 & 6 & 9} & &
\bmatriz{1 & 0 & 0\\0 & 2 & 0\\0 & 0 & 3} 
\end{align*}



%\subsubsection{Inversa de una matriz}
Si $A$ y $B$ tienen inversa y $c$ es un escalar distinto de cero entonces las siguientes construcciones son invertibles y cada una cumple la correspondiente igualdad.

\teorema{
  \dctb{InvUnica}{La inversa es única}{Si $AB=I=BA$  entonces $B=A^{-1}$}
  \dctb{InvProd}{La inversa del producto}{$(AB)^{-1}=B^{-1}A^{-1}$}
  \dctb{InvInv}{La inversa de la inversa}{$(A^{-1})^{-1}=A$}
  \dctb{InvEscMat}{Escalar sale invertido}{$(cA)^{-1}=\frac{1}{c}(A)^{-1}$}
    \dctb{InvConmTra}{Inversa conmuta con transpuesta}{$A^T$ es invertible y $(A^T)^{-1}=(A^{-1})^T$}}
\subsection{Potencia de una matriz} \label{PotZ}

En la sección \ref{PotN} se vieron algunas propiedades para exponentes de números naturales, ahora se extenderá esta definición para los exponentes de números enteros, pero se restringirá a las matrices invertibles.
 
%Para una matriz cuadrada $A$ se definen las potencias: $A^0=I$, para $n \in \N_1$, $A^n=AA \ldots A$ $n$ veces y si A es invertible $A^{-n}=(A^{-1})^n$.
%Las siguientes son algunas propiedades de las potencias. 

Asumiendo que $A$ es invertible, que $c$ es un escalar distinto se cero y que $r,s \in \Z$, se tiene que $A^r$ es invertible y que las siguientes construcciones también los son. Además, cada una cumple la respectiva igualdad.

\teorema{
  \dctb{SumaExpZ}{Suma de Exponentes}{$A^r A^s=A^{r+s}$}
  \dctb{InvConmExpZ}{Inversa, exponente conmutan}{ $(A^n)^{-1}=(A^{-1})^n=A^{-n}$}
  \dctb{ExpExpZ}{Producto de exponentes}{$((A)^{r})^s=A^{rs}$}
  \dctb{ExpEscMatZ}{El escalar sale con exponente}{$(cA)^{r}=c^{r}(A)^{r}$}
}
%------------
%%\newcommand{\copiaA}{
%Inversa de una matriz de $2 \times 2$.
%
%Sea
%\[A=\begin{bmatrix}
%a_{11} & a_{12}  \\
%a_{21} & a_{22}   \\
%\end{bmatrix}\]
%La inversa de $A$ es:
%\[A^{-1}=\frac{1}{a_{11}a_{22}-a_{12}a_{21}}\begin{bmatrix}
%a_{22} & -a_{12}  \\
%-a_{21} & a_{11}   \\
%\end{bmatrix}\]
%%\begin{prop}\label{prop_invMdxd}
%\teorema{Una matriz $A=\begin{bmatrix}
%a_{11} & a_{12}  \\
%a_{21} & a_{22}   \\
%\end{bmatrix}$ de $ 2\x 2$ tiene inversa si y sólo si \[a_{11}a_{22}-a_{12}a_{21} \neq 0\]}



%\end{prop}
%\addtocounter{prop}{-1}}
%\copiaA
%%Explicación: 
%\proof
%Las proposiciones con ``si sólo si'' requieren probar dos implicaciones. En este caso serían:
%\begin{itemize}
%\item[(i)] Si $A$ tiene inversa entonces $a_{11}a_{22}-a_{12}a_{21} \neq 0$.
%\item[(ii)] Si $a_{11}a_{22}-a_{12}a_{21} \neq 0$ entonces $A$ tiene inversa.
%\end{itemize}
%Estas mismas implicaciones se puede escribir en su forma contra-reciproca
%\begin{itemize}
%\item[(i)] Si $a_{11}a_{22}-a_{12}a_{21} = 0$ entonces $A$ no tiene inversa.
%\item[(ii)] Si $A$ no tiene inversa entonces $a_{11}a_{22}-a_{12}a_{21} = 0$.
%\end{itemize}
%%\end{proof}
%Debido a lo sencillo de esta prueba se pueden tratar ambas implicaciones al mismo tiempo.
%
%
%
%\newslide %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\copiaA
%Explicación: 
%\begin{proof}
%$a_{11}a_{22}-a_{12}a_{21} \neq 0$ si y sólo si $\frac{1}{a_{11}a_{22}-a_{12}a_{21}} \in \R$.  Debido a la propiedad Inver, que afirma que el único elemento de $\R$ que no tiene inverso es el $0$. 
%
%$\frac{1}{a_{11}a_{22}-a_{12}a_{21}} \in \R$ si y sólo si existe el producto de escalar por matriz \[\frac{1}{a_{11}a_{22}-a_{12}a_{21}}\begin{bmatrix}
%a_{22} & -a_{12}  \\
%-a_{21} & a_{11}   \\
%\end{bmatrix}\]
%debido precisamente a la definición del producto de escalar por matriz.
%
%$A$ es invertible   si y solo si  existe el producto de escalar por matriz \[\frac{1}{a_{11}a_{22}-a_{12}a_{21}}\begin{bmatrix}
%a_{22} & -a_{12}  \\
%-a_{21} & a_{1}   \\
%\end{bmatrix}\]
%debido a la definición de la matriz inversa.
%
%%La explicación de (i) me parece más sencilla a partir de la forma contra-reciproca ya que una división no está definida cuando el denominador es cero y por lo tanto la inversa de $A$ no se puede calcular.
%
%%Para la explicación de (ii) me parece mejor  la afirmación original ya que cada uno de los elementos de la matriz así como el escalar se pueden calcular. 
%\end{proof}
%
%Aunque una explicación tan detallada usualmente no es necesaria, aquí se hizo el esfuerzo de escribirla con el fin de resaltar cada una de las definiciones y propiedades que se utilizaron para esta prueba: la  propiedad Inver del campo $\R$, la definición de producto de escalar por matriz y la definición de inversa de una matriz. Cada una puede pasar inadvertida debido a la familiaridad de la notación.

Se recomienda hacer los ejerciicos de la sección 3.2 de \cite{NJ99}.


\newslide
\section{Operación determinante}


% del cambio de tamaño por un cambio de base en $\R^n$}
%Si no se dice lo contrario, en esta sección usaremos las matrices cuadradas.
%\subsection{El determinante y la inversa}\cite[pg170]{NJ99}
Recordemos que si $A=\bsmatriz{a&b\\c&d}$, entonces %de $2 \times 2$ 
 \[A^{-1}=\frac{1}{ad-cb}\bmatriz{d&-b\\-c&a}.\]
 En donde al numero 
 $ad-cb$ 
 %es el \de{determinante de una matriz de $2 \times 2$} 
% \\[0.3cm]
% \teorema{ Una matriz (de $2 \times 2$) es invertible si y sólo si su determinante es diferente de cero.}
%. \\[0.3cm]
%Algunas formas de denotar el determinante son
%
%\[det[A]=|A|=\vmatriz{a&b\\c&d}=ad-cb\]
es determinante para saber si la matriz $A$ de $2 \times 2$ tiene inversa y se le conoce como el \de{determinante} y se denota con líneas verticales.
\[\vmatriz{a&b\\c&d}=ad-cb\]



Gráficamente en $\R^2$ si las columnas de una matriz $A$ representan los lados de un paralelogramo, el determinante de $A$ es el área del paralelogramo (\cite[pg388]{NJ99}). En $\R^3$ sucede algo similar con el volumen paralelepípedo (\cite[pg118]{NJ99}).


Si el área del paralelogramo o el volumen del paralelepípedo es cero entonces uno de los vectores de $A$ depende de los otros y el determinante es cero. Pero si el determinante es diferente de cero entonces los vectores son independientes.


 Observe que el determinante está formado por los productos de elementos que no  repiten filas o columnas.  %Observe que el determinante de esta matriz está  
%
%Precisamente $ad-cb$ se le conoce como el \de{determinante de una matriz de } 
%
% En general, el numero determinante para saber si una matriz $A$ de $n \times n$ tiene inversa 
%\subsection{Generalización de los determinantes}\cite[6.4]{NJ99}
%\begin{itemize}
%\item 
%formado por la suma de todos los productos (signados) de $2$ elementos de $A$ que provienen de \underline{distintos} renglones y columnas.
%\item 
%Se asigna un signo a cada producto. Se suman todos los productos con signo.
%\end{itemize}
En general para una matriz $A$ de $n \times n$, el determinante está formado por la suma de todos los productos (signados) de $n$ elementos de $A$ que no repiten ni fila ni columna. En total hay $n!$ productos,
%de una matriz de $n \times n$ está formado por $n!$ productos,
%El número de productos es $n!$, 
y recordemos que
\begin{align*}
1!&=1 & 2!&=2 & 3!&=6 &4!&=24& 5!&=120& &\ldots& 10!&=3\ 628\ 800
\end{align*}
Sin embargo esta es una forma complicada de calcularlo, especialmente por el signo. A continuación se presenta una forma para encontrar el determinante.

%\newslide 


\subsection{Determinante y desarrollo por cofactores}\cite[6.1]{NJ99}

El determinante de %una matriz de $3 \times 3$ se puede escribir en términos de determinantes de $2 \times 2$. En general, 
una matriz de orden $n$ se puede escribir en termino de matrices de orden $n-1$ de la siguiente manera.

\begin{itemize}
\item El \de{menor}(i,j) de una matriz $A$ se representa por $M_{ij}$ y es el determinante que se obtiene  eliminando el $i$-ésimo renglón y la $j$-ésima columna de $A$.

\item A cada elemento de $A$ se le asigna un signo como un tablero de ajedrez $(-1)^{i+j}$. El \de{cofactor}(i,j), $C_{ij}$, es el menor (i,j) multiplicado por el signo.  $C_{ij}=(-1)^{i+j}M_{ij}$.

\item Se elije un renglón o una columna y se suman los cofactores --- multiplicados por los respectivos elementos.
\end{itemize}

El \de{determinante} de $A$ (denotado por $|A|$ o $det(A)$) está dado por 
\[|A|=a_{11}C_{11} + a_{12}C_{12} + \cdots + a_{1n}C_{1n} \]
Resumiendo


\defOpMatA{|A|}{Determinante de una matriz $A$}
{si es cuadrada.}
{un escalar}
{por cofactores y se puede simplificar el cálculo mediante la eliminación gaussiana}  
{}\verb!det{A}!


\teorema{\cite[Teorema 2.3.1]{Gro05}
La expansión por cofactores puede ser desarrollada por cualquier renglón y no necesariamente por el primero.
\begin{align*}
|A|=&a_{11}C_{11} + a_{12}C_{12} + \cdots + a_{1n}C_{1n} 
\\=&a_{i1}C_{i1} + a_{i2}C_{i2} + \cdots + a_{in}C_{in} 
%=&a_{11}C_{11} + a_{12}C_{12} + \cdots + a_{1n}C_{1n} 
\end{align*}}

%La demostración de este teorema también se realiza por inducción sobre el tamaño de la matriz.

Demuestre que los determinante son 
\[\begin{vmatrix}
1 & 3 & 5 & 2 \\
0 & 0 & 0 & 4 \\
2 & 1 & 9 & 6 \\
3 & 2 & 4 & 8 \\
\end{vmatrix}=192\]
\[\begin{vmatrix}
-2& 3 & 0 & 1 \\
0 & 3 & 2 & 4 \\
0 & 0 & 1 & 3 \\
0 & 0 & 0 &-2 \\
\end{vmatrix}=12\].


%\newslide

%\newslide 



%\subsection{El determinante y la razón de áreas}\cite[pg338]{NJ99}
%\begin{itemize}
%\item $B=\bmatriz{0&0&1&1\\0&1&1&0}$ representa los puntos de una figura cerrada que encierra un área $s_B$.
%\item $A=\bmatriz{a_{11}&a_{12}\\a_{21}&a_{22}}$ representa una transformación matricial de $\R^2$ en $\R^2$.
%\item $C=AB$ representa los puntos de la figura transformada por $A$, que encierra un área $s_C$.
%\end{itemize}
%Es conocido que la razón de las áreas es el determinante de $A$.
%\\[0.3cm]
%\teorema{\[\frac{s_C}{s_B}=a_{11}a_{22}-a_{21}a_{22}\]}
%%El área de los puntos de $C$ dividido el área de los puntos de $B$ está dado por el determinante de $A$. 
%. \\[0.3cm]
%¿Qué pasa si el determinante es cero?
%
%%Supongamos ahora que la matriz $B$ representa los puntos de una figura cerrada en $\R^2$. Por ejemplo un cuadrado. Supongamos también que $A$ es una transformación de de $\R^2$ en $\R^2$, entonces $A$ transforma el rectángulo en un paralelogramo, en donde los punto s del paralelogramo esta dados por la multiplicación matricial $AB$. 
%
%
%%Cuando la matriz $A=\bsmatriz{a&b\\c&d}$ representa una transformación de $\R^2$ en $\R^2$ el determinante indica la relación del área 
%%\newslide
%\subsection{La razón de volúmenes}\cite[pg118]{NJ99}
%\begin{itemize}
%\item Si $B=\bmatriz{0&0&1&1&0&0&1&1\\0&1&1&0&0&1&1&0\\0&0&0&0&1&1&1&1}$ representa los puntos de una figura cerrada que encierra un volumen $v_B$.
%\item $A=\bmatriz{a_{11}&a_{12}&a_{13}\\a_{21}&a_{22}&a_{23}\\a_{31}&a_{32}&a_{33}}$ representa una transformación matricial de $\R^3$ en $\R^3$.
%\item $C=AB$ representa los puntos de la figura transformada por $A$, que encierra un volumen $v_C$.
%\end{itemize}
%Es conocido que la razón de los volúmenes es
%\teorema{\begin{align*}\frac{v_C}{v_B}=&a_{11}a_{22}a_{33}+a_{21}a_{32}a_{13}+a_{31}a_{12}a_{23}\\&-a_{11}a_{32}a_{23}-a_{21}a_{12}a_{33}-a_{31}a_{22}a_{13}\end{align*}}
%

%\newslide

%\newslide

\subsection{Determinate de una matriz de $3 \times 3$}
%eterminante de una matriz de $3 \x 3$

\begin{align*} %\text{ Sea }
|A|=&\begin{vmatrix}
a_{11} & a_{12} & a_{13} \\
a_{21} & a_{22} & a_{23} \\
a_{31} & a_{32} & a_{33} \\	
\end{vmatrix}\\
=&+a_{11}\begin{vmatrix}
 a_{22} & a_{23} \\
 a_{32} & a_{33} \\	
\end{vmatrix}\\
&-a_{12}\begin{vmatrix}
 a_{21} & a_{23} \\
 a_{31} & a_{33} \\	
\end{vmatrix}\\
&+a_{13}\begin{vmatrix}
 a_{21} & a_{22} \\
 a_{31} & a_{32} \\	
\end{vmatrix}\\
\intertext{Cada una de estas matrices de $2 \x 2$ se llaman el \de{menor} y como cada menor tiene dos productos elementales entonces una matriz de $3 \x 3$ tiene $6$ productos elementales}
|A|=&+ a_{11}a_{22}a_{33} - a_{11}a_{23}a_{32}\\
 &+ a_{12}a_{23}a_{31} - a_{12}a_{21}a_{33}\\
 &+ a_{13}a_{21}a_{32} - a_{13}a_{22}a_{31}
 \end{align*}
Estos seis productos corresponden a las diagonales de la matriz de $3 \x 5$ que corresponde al adjuntar a $A$ las primeras dos columnas como se muestra en \cite[Sec. 2.1 pg 174]{Gro05}.


%El determinante de una matriz de $3 \x 3$ se puede ver como la suma de los productos de las diagonales cuyos indices suman 

Demuestre que el determinante es 
\[\begin{vmatrix}
1 & 2 & 4 \\
1 & 3 & 6 \\
2 & 4 & 3 \\
\end{vmatrix}=-5\]

\section{Determinante de matrices triangulares}


Una matriz cuadrada se llama \de{triangular superior} si todas sus componentes abajo de la diagonal son cero. Es una matriz \de{triangular inferior} si todos sus componentes arriba de la diagonal son cero. Una matriz se llama \de{diagonal} si todos los elementos que no están sobre la diagonal son cero. 

\teorema{Sea $A$ una matriz de $n \times n$ triangular superior o inferior. Entonces $|A|=a_{11} a_{22} \ldots a_{nn}$.}
%\cite[Teorema 2.1.1]{Gro05} Esta demostración se realiza usando inducción, la cual requiere dos pasos: el primero consiste en mostrar que una matriz de $2 \times 2$ lo cumple. El segundo paso es mostrar que si toda matriz de $B$ de $(n-1) \times (n-1)$ cumple que $|B|=(B)_{11} (B)_{22} \ldots (B)_{n-1,n-1}$ entonces también lo cumple cualquier matriz $A$ de $n \times n$.


%\newslide


%\newslide



%
%
%
%Determinante de una matriz de $2 \times 2$
%
%Sea
%\[A=\begin{bmatrix}
%a_{11} & a_{12}  \\
%a_{21} & a_{22}   \\
%\end{bmatrix}\]
%El determinante de $A$ se define así:
%\begin{align*}
%det(A)&=|A|=\begin{vmatrix}
%a_{11} & a_{12}  \\
%a_{21} & a_{22}   \\
%\end{vmatrix}\\
%&=a_{11}a_{22}-a_{12}a_{21}
%\end{align*}
%Las tres notaciones significan lo mismo. El determinante de una matriz de $2 \x 2$ se puede ver como la el producto de la diagonal principal menos la el producto de la  diagonal de los elementos que $i+j=3$.
%En Scilab el determinante de $A$ es \verb!det(A)!
%
%Con esta definición la Proposición \ref{prop_invMdxd} queda así:
%
%\begin{corol}
%Una matriz $A$ de $2 \x 2$ tiene inversa si y sólo si \[|A| \neq 0\]
%\end{corol}
%
%En Scilab el determinante de $A$ es \verb!det(A)!
%
%\newslide %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Determinantes para matrices de $n \x n$
%
%%$a_{11}a_{22}$ y $a_{12}a_{21}$
%
%En \cite[pg 115]{Ant06} definen  determinante  para una matriz $A$ cuadrada de orden $n$
%%Sea $A$ una matriz cuadrada. La función \de{determinante} se denota  por $\ds{det}$, y $det(A)$ se define 
%como la suma de los productos elementales con signo de A.
%
%En donde un \de{producto elemental} de una matriz de $n \times n$ se entiende cualquier producto de $n$ elementos de $A$, de los cuales ningún par de elementos proviene del mismo renglón o de la misma columna. En caso de la matriz 
%\begin{align*}
%A=\begin{bmatrix}
%a_{11} & a_{12}  \\
%a_{21} & a_{22}   \\
%\end{bmatrix}
%\intertext{sus dos productos elementales son}
%a_{11}a_{22}\ \ \ \ \ \ \ \ a_{12}a_{21}
%\end{align*}
%
%Las definiciones de \de{producto elemental con signo positivo o negativo} corresponden respectivamente a las permutaciones pares e impares de los productos, pero estas definiciones quedan fuera del alcance de esta presentación, por ese motivo los determinantes serán definidos de manera inductiva, es decir  definiendo  el determinante de una matriz de orden $n$ en función del determinante de matrices de orden $n-1$.
%
%%\newslide %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%Determinante de una matriz de $3 \times 3$ \cite{Gro05}
%%
%%\[ \text{ Sea }A=\begin{bmatrix}
%%a_{11} & a_{12} & a_{13} \\
%%a_{21} & a_{22} & a_{23} \\
%%a_{31} & a_{32} & a_{33} \\	
%%\end{bmatrix}\]
%%
%%El determinante de esta matriz es
%%
%%\[|A|=a_{11}\begin{bmatrix}
%% a_{22} & a_{23} \\
%% a_{32} & a_{33} \\	
%%\end{bmatrix}
%%-a_{12}\begin{bmatrix}
%% a_{21} & a_{23} \\
%% a_{31} & a_{33} \\	
%%\end{bmatrix}
%%+a_{13}\begin{bmatrix}
%% a_{21} & a_{22} \\
%% a_{31} & a_{32} \\	
%%\end{bmatrix}\]
%%
%\newslide %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%Determinante de una matriz de $2 \x 2$
%%
%%\begin{align*}
%%|A|=&\begin{vmatrix}
%%a_{11} & a_{12}  \\
%%a_{21} & a_{22}   \\
%%\end{vmatrix}\\
%%=&a_{11}a_{22}-a_{12}a_{21}
%%\end{align*}
%D
%
%\newslide %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Determinante de $n \times n$ y definiciones de menor y de cofactor. \cite{Gro05}
%
%Sea $A$ una matriz de $n \times n$ y sea $M_{ij}$ la matriz de $(n-1) \times (n-1)$ obtenida de $A$ eliminando el renglon $i$ y la columna $j$. $Mij$ se llama el \de{menor $ij$} de $A$. 
%
%El \de{cofactor $ij$} de $A$, denotado por $C_{ij}$, está dado por 
%\[C_{ij}:=(-1)^{i+j}|M_{ij}|\] 
%Es decir que el cofactor es el determinante del menor con el signo.
%
%Es de resaltar que la notación $a_{ij}$ (con paréntesis) representa el elemento $ij$ de $A$, mientras que $C_{ij}$ (sin paréntesis) representa el cofactor $ij$ de $A$.
%
%El \de{determinante} de $A$ (denotado por $|A|$ o $det(A)$) está dado por 
%\[|A|=a_{11}C_{11} + a_{12}C_{12} + \cdots + a_{1n}C_{1n} \]
%
%\newslide %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%


%\section{determinante e inversa de las matrices elementales}

   
%
%
%
% 
%
%\newslide %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Matrices elementales y sus determinantes
%
%Una matriz elemental $E$ es la que se obtiene al realizar una operación elemental a la matiz identidad $I$. \cite[Teorema 1.10.1]{Gro05} Para realizar una operación elemental en una matriz $A$, se multiplica $A$  por la izquierda por la matriz elemental adecuada.
%
%Ejercicio. Encontrar las respectivas matrices elementales %(E1, E2 y E3)
%para cada uno de los tipos de operaciones elementales: 
%\begin{description}
%\item[(Tipo multiplicación)] multiplica el renglón 3 por 2 ($2R3 \ra R3$)
%\item[(Tipo intercambio)] intercambia el renglón 1 con el renglón 2 ($R1 \lra R2$) 
%\item[(Tipo suma)] suma 4 veces el renglón 3 al renglón 2 ($R2 + 4 R3 \ra R2$)
%\end{description}
%a la matriz $\begin{vmatrix}
%1 & 2 & 4 \\
%1 & 3 & 6 \\
%2 & 4 & 3 \\
%\end{vmatrix}$ compruébelos y halle el determinante y la inversa de cada uno.
%
%
%  
%
%
%
%
%
%\newslide %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{verbatim}
%-->I=eye(3,3)
% I  =
% 
%    1.    0.    0.  
%    0.    1.    0.  
%    0.    0.    1.  
% 
%-->// multiplica el renglón 3 por 2
%
%-->E1=I;E1(3,:)=2*E1(3,:)
% E1  =
% 
%    1.    0.    0.  
%    0.    1.    0.  
%    0.    0.    2.  
% 
%-->A=[ 1 2 4; 1 3 6; 2 4 3]
% A  =
% 
%    1.    2.    4.  
%    1.    3.    6.  
%    2.    4.    3.  
% 
%-->E1*A
% ans  =
% 
%    1.    2.    4.  
%    1.    3.    6.  
%    4.    8.    6.  
%
%-->det(E1)
% ans  =
% 
%    2.  
% 
%-->inv(E1)
% ans  =
% 
%    1.    0.    0.   
%    0.    1.    0.   
%    0.    0.    0.5  
% 
%\end{verbatim}
%\newslide %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{verbatim} 
%
%-->// intercambia el renglón 1 con el renglón 2 
%
%-->E2=I;E2(3,:)=I(2,:);E2(2,:)=I(3,:)
% E2  =
% 
%    1.    0.    0.  
%    0.    0.    1.  
%    0.    1.    0.  
% 
%-->A=[ 1 2 4; 1 3 6; 2 4 3]
% A  =
% 
%    1.    2.    4.  
%    1.    3.    6.  
%    2.    4.    3.  
% 
%-->E2*A
% ans  =
% 
%    1.    2.    4.  
%    2.    4.    3.  
%    1.    3.    6.  
%    
%
%-->det(E2)
% ans  =
% 
%  - 1.  
% 
%-->inv(E2)
% ans  =
% 
%    1.    0.    0.  
%    0.    0.    1.  
%    0.    1.    0.  
% 
%\end{verbatim}
%\newslide %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{verbatim} 
%
%-->// suma 4 veces el renglón 3 al renglón 2
% 
%-->E3=I;E3(2,:)=E3(2,:)+4*E3(3,:)
% E3  =
% 
%    1.    0.    0.  
%    0.    1.    4.  
%    0.    0.    1.  
% 
%-->A=[ 1 2 4; 1 3 6; 2 4 3]
% A  =
% 
%    1.    2.    4.  
%    1.    3.    6.  
%    2.    4.    3.  
% 
%-->E3*A
% ans  =
% 
%    1.    2.     4.   
%    9.    19.    18.  
%    2.    4.     3.   
%
%-->det(E3)
% ans  =
% 
%    1.  
% 
%-->inv(E3)
% ans  =
% 
%    1.    0.    0.  
%    0.    1.  - 4.  
%    0.    0.    1.  
%
%\end{verbatim}
%\newslide %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

 
\section{Determinantes y operaciones elementales}
A continuación se presenta como se altera el determinante el realizar operaciones elementales o lo que es lo mismo al ser multiplicada por una matriz elemental. (Recordemos que una matriz elemental es a la que se le ha hecho una operación elemental) \cite[6.2]{NJ99} También se indica cual es la operación inversa para restaurar la matiz original o lo que es lo mismo la matriz elemental inversa.

\teorema{
\dctb{OpEl}{Operacion Elemental}{
\begin{enumerate}
\item Si la matriz $B$ se obtiene de multiplicar \underline{un} renglón (o una columna) por una constante $k$ distinta de cero, entonces  $|B|=k|A|$. La operación inversa es multiplicar el mismo renglón (o columna) por $\frac{1}{k}$.
\item Si se intercambian dos renglones (o dos columnas) entonce el determinante cambia de signo. La operación inversa es volver a intercambiar los mismos dos renglones (o dos columnas).
\item El determinante no cambia si se suma un múltiplo de un renglón (columna) a otro renglón (columna). La operación inversa es restar el mismo múltiplo.
\end{enumerate}}}


%Determinante e inversa de la matriz elemental tipo multiplicación 

Resumiendo el determinante y la inversa de las matrices elementales se obtiene (\cite[Tabla 1.4 y Lema 2.3.1]{Gro05})\\
%\renewcommand{arraystretch}{1.3}
\begin{tabular}{||c|c|c||c|c|c||}\hline \hline
operación&ejem.&det.&inversa&ejem.&det. de inv.\\\hline \hline
\vphantom{\Large hola}
%multiplicación&
$cRi \ra Ri$&$\bsmatriz{c&0\\0&1}$&$c$&$\frac{1}{c}Ri \ra Ri$&$\bsmatriz{\frac{1}{c}&0\\0&1}$&$\frac{1}{c}$\\[0.3mm] \hline
%intercambio&
$Ri \lra Rj$&$\bsmatriz{0&1\\1&0}$&$-1$&$Ri \lra Rj$&$\bsmatriz{0&1\\1&0}$&$-1$\\ \hline
%suma&
$Ri + c Rj \ra Ri$&$\bsmatriz{1&-c\\0&1}$&$1$&$Ri - c Rj \ra Ri$&$\bsmatriz{1&c\\0&1}$&$1$\\ \hline \hline
\end{tabular}


De esta tabla se concluyen el siguientes resultado para una matriz $A$ de orden $n$. 

%\cite[Lema 2.3.2]{Gro05}
%Si $E$ una matriz elemental entonces \[det(EA)=det(E)det(A)\]
%
%
%\newslide %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Con el anterior resultado se puede calcular el determinante de una matriz $A$ aplicando primero la eliminación gaussiana.
Demuestre que el determinante es 
\[\begin{vmatrix}
1 & 2 & 4 \\
1 & 3 & 6 \\
2 & 4 & 3 \\
\end{vmatrix}=-5\]

\begin{verbatim}
-->A=[ 1 2 4; 1 3 6; 2 4 3]
 A  =
 
    1.    2.    4.  
    1.    3.    6.  
    2.    4.    3.  
 
-->A(1,:)=(1/A(1,1))*A(1,:) // Para que no cambie el det.
                            // se debe multiplicar por A(1,1)=1
 
    1.    2.    4.  
    1.    3.    6.  
    2.    4.    3.  
 
-->A(2,:)=A(2,:)-A(2,1)*A(1,:) // Esta operación no cambia el det.
 
    1.    2.    4.  
    0.    1.    2.  
    2.    4.    3.  
 
-->A(3,:)=A(3,:)-A(3,1)*A(1,:) // Esta operación no cambia el det.
 
    1.    2.    4.  
    0.    1.    2.  
    0.    0.  - 5.  
    
--> // El proceso podría detenerse aquí ya que al tener una matriz
//triangular el determinante es el producto de la diagonal (-5) por 1.
 
-->A(2,:)=(1/A(2,2))*A(2,:) // Para que no cambie el det.
                            // se debe multiplicar por A(2,2)=1

 
    1.    2.    4.  
    0.    1.    2.  
    0.    0.  - 5.  
 
-->A(3,:)=A(3,:)-A(3,2)*A(2,:) // Esta operación no cambia el det.
 
    1.    2.    4.  
    0.    1.    2.  
    0.    0.  - 5.  
 
-->A(3,:)=(1/A(3,3))*A(3,:) // Para que no cambie el det.
                            // se debe multiplicar por A(3,3)=-5

 
    1.    2.    4.  
    0.    1.    2.  
    0.    0.    1.  
-->// En este caso el producto de la diagonal es 1 el cual hay
// que multiplicar por -5, 1 y 1.



\end{verbatim}
%
%
%\newslide %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\subsection{Otras propiedades del determinante}\cite[6.2]{NJ99}
Para las siguientes afirmaciones asuma que $A$, $B$ son matrices de $n \times n$ y que $c$ es un escalar.

\teorema{
  \dctb{DetTra}{Determinante de transpuesta}{$|A^T|=|A|$}
  \dctb{DetEscMat}{Determinante de escalar $e$ por matriz $A$}{$|cA|=c^n|A|$} 
  \dctb{DetInv}{Determinante de la inversa}{$|A^{-1}|=|A|^{-1}$si $|A|\neq 0$}
  \dctb{DetMul}{Determinante de multiplicación de matrices}{$|AB|=|A|\ |B|$}
    }

%\begin{enumerate}
%\item Una matriz y su transpuesta tienen el mismo determinante.
%\item La suma de dos determinantes de matrices que difieren en sólo un renglón es una de las matrices con la suma de los renglones que difieren.
%\item En una matriz de orden $n$ el determinante de una constante por la matriz es la constante  a la $n$ por el determinante de la matriz.
%\item En una matriz invertible, el determinante de la inversa e el inverso del determinante. 
%\end{enumerate}
%Determinante de multiplicación de matrices
%
%\cite[Lema 2.3.2]{Gro05}
%Si $A$ y $B$ son matrices de orden $n$ entonces \[det(AB)=det (A) det(B)\]
%Comprueba para $A=\bmatriz{1 & 2\\ 3 & 4}$ y $B=\bmatriz{5 & 6 \\ 7 & 8}$
%
%\begin{verbatim}
%-->A=[1 2 ; 3 4]
% A  =
%
%    1.    2.  
%    3.    4.  
% 
%-->B=[5  6 ; 7  3]
% B  =
% 
%    5.    6.  
%    7.    3.  
% 
%
%
%
%
%
%-->det(A) =  - 2.  
%
%-->det(B) =  - 27.  
%
%-->det(A)*det(B) =    54.  
% 
%-->A*B  =
%    19.    12.  
%    43.    30.  
%
%-->det(A*B) = 54.  
% 
%
%\end{verbatim}
%
%Lo anterior permite concluir que 
%\[|A^{-1}|=\frac{1}{|A|}\]
%debido a que el determinante de $A$ por el de su inversa da el determinante de la identidad.
%%\cite[Teorema 2.4.10]{Gro05}
%%$A$ es invertible si y sólo $|A| \neq 0$.
%
% 
%\newslide %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



La suma de determinantes NO es el determinante de la suma de matrices, pero es algo parecido. La suma de los determinantes de dos matrices que difieren sólo en un renglón da a una matriz que suma esos dos renglones. Por ejemplo.

\[\begin{vmatrix}
a_{11} & a_{12} & a_{13} \\
a_{21} & a_{22} & a_{23} \\
a_{31} & a_{32} & a_{33} \\	
\end{vmatrix}
+\begin{vmatrix}
a_{11} & a_{12} & a_{13} \\
b_{21} & b_{22} & b_{23} \\
a_{31} & a_{32} & a_{33} \\	
\end{vmatrix}
=\begin{vmatrix}
a_{11} & a_{12} & a_{13} \\
a_{21}+b_{21} & a_{22}+b_{22} & a_{23}+b_{23} \\
a_{31} & a_{32} & a_{33} \\	
\end{vmatrix}\]
Es de resaltar que se están sumando determinantes y no matrices debido a que la matriz no tiene corchetes sino líneas.


%---------------Renglón por escalar


\subsection{Cuando el determinante es cero}

Cuando el determinante es cero la matriz no es invertible
\cite[Teorema 6.2.5]{NJ99} Algunos casos cuando el determinante de una matriz $A$ de orden $n$ es cero. %Imagine la matriz como la matriz de coeficientes de un sistema de $n$ ecuaciones homogéneas con $n$ incógnitas o como $3$ `planos' que pasan por el origen en un espacio de $3$ dimensiones  

\begin{itemize}
\item Si tiene un renglón de ceros. Ej. $\vmatriz{1 & 2 & 3\\ 0 & 0 & 0 \\ 7 & 1 & 5}$. Lo cual se deduce al desarrollar el determinante por el renglón de ceros.
\item Si tiene dos renglones que son iguales. Ej. $\vmatriz{1 & 2 & 3\\ 1 & 2 & 3 \\ 7 & 1 & 5}$. Lo cual se debe al hacer la operación elemental de restar los dos renglones ya que obtenemos el resultado anterior.
\item Si tiene un renglón múltiplo de otro.  Ej. $\vmatriz{1 & 2 & 3\\ 2 & 4 & 6 \\ 7 & 1 & 5}$. Esto se debe al hacer la operación elemental de restar multiplicar el renglón por una constante ya que obtenemos el resultado anterior.
\item Si un renglón la suma de múltiplos de otros dos renglones. Ej. el renglón 2 es la suma de los otros dos. $\vmatriz{1 & 2 & 3\\ 8 & 3 & 8 \\ 7 & 1 & 5}$. En este caso se realizan ambas operaciones elementales de sumar y multiplicar.
\end{itemize}
\

%Si el determinante es cero la matriz no es invertible y por lo tanto el sistema homogéneo no tiene solución única. En particular el determinante es cero cuando la matriz tiene: 
%\begin{itemize}
%\item un renglón (o columna) de ceros.
%\item dos renglones (o columnas) iguales.
%\item un renglón (o columna) múltiplo de otro.
%\item un renglón (o columna) es la suma de múltiplos de otros dos.
%\end{itemize}





%\newslide
\subsection{Cuando el determinante no es cero}


\teorema{\cite[Teorema 1.10.3 y Teorema 2.3.2]{Gro05} y \cite[Teorema 2.3.3]{Ant06}
Si $A$ es una matriz cuadrada de orden $n$, las siguientes afirmaciones son equivalentes: 
\begin{enumerate}[ii]
\item[(i)] $A$ es invertible.
\item[(ii)] $A$ es el producto de matrices elementales.
\item[(iii)] $|A| \neq 0$.
\item[(iv)] $A$ no tiene parámetros.
\item[(v)] Cada columna de $A$ tiene un pivote.
\item[(vi)] $A$ tiene $n$ pivotes.
\item[(vii)] Cada renglón de $A$ tiene un pivote.
\item[(viii)] El sistema $A\ve{x}=\ve{b}$ tiene solución única para cualquier vector $\ve{b} \in \R^n$.
\item[(ix)] La única solución del sistema $A\ve{x}=\ve{0}$ es la trivial $\ve{x}=\ve{0}$.
\item[(x)] $A$ es equivalente a la identidad.
\item[(xi)] Las columnas de $A$ son linealmente independientes. 
\item[(xi)] Los renglones de $A$ son linealmente independientes. 
\item[(xii)] Las columnas de $A$ generan a $R^n$
\item[(xiii)] Los renglones de $A$ generan a $R^n$
\item[(xiv)]  $A$ es una base de $R^n$
\end{enumerate}
}

Una forma para demostrar esta triple equivalencia consiste en mostrar primero que (i) implica (ii), luego se muestra que (ii) implica (iii) y finalmente se muestra que (iii) implica (i).

(i) implica (ii) debido a que la eliminación de Gauss-Jordan aplicada a una matriz invertible da como resultado la identidad entonces es posible multiplicar $A$ por las respectivas matrices elementales para obtener la identidad. 
\[E_k \ldots E_2 E_1 A = I\]
Entonces la multiplicación de las inversas de las matrices elementales permiten reconstruir $A$.
\[A=E_1^{-1} E_2^{-1} \ldots E_k^{-1} \]

(ii) implica (iii) debido a que el determinante de cada una de las matrices elementales es diferente de cero y como el determinante del producto es el producto de los determinantes (para matrices elementales hasta ahora) entonces el determinante de $A$ tampoco es cero.

(iii) implica (i) debido a que la eliminación de Gauss-Jordan  da como resultado una matriz escalonada $R$, 
\[E_k \ldots E_2 E_1 A = R\]
por lo tanto si $det (A) \neq 0$ entonces $det(R) \neq 0$
\[|E_k| \ldots |E_2| |E_1| |A| = |R|\]
Lo cual implica que $R=I$ y por lo tanto $A$ es invertible.
%\cite[]{Gro05}
%$A$ es invertible 

\section{Adjunta}

Sea $A$  una matriz de orden $n$. La matriz cuyo $(i,j)$-ésimo elemento es el cofactor $C_{ij}$ de $A$ es la \de{matriz de cofactores de $A$}. Su transpuesta es la \de{adjunta de $A$}, y se representa por $Adj(A)$.
\[Adj(A)=\bmatriz{C_{11} & C_{21} & \cdots & C_{n1} \\
C_{12} & C_{22} & \cdots & C_{n2} \\
\vdots & \vdots & \ddots & \vdots \\
C_{1n} & C_{1n} & \cdots & C_{nn} \\}\]

\teorema{Sea $A$ una matriz de orden $n$. Entonces 
\[A \ Adj(A) = det(A) I_n = Adj(A) \ A\]
y por lo tanto si $A$ es invertible
\[A^{-1}=\frac{1}{det(A)}Adj(A)\]}

\section{Regla de Cramer}
\teorema{Si $det(A) \neq 0$, la solución del sistema $Ax=b$ es  \[x_1=\frac{|A_1|}{|A|}, \ x_2=\frac{|A_2|}{|A|}, \ldots, \ x_n=\frac{|A_n|}{|A|}, \] 
En donde $A_i$ corresponde a la matriz $A$ remplazando la columna $i$ por $b$.}

%
%\newslide %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\newslide %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Determinante de operaciones matriciales
%
%YA se vio que el determinate de la multiplicación matricial es la multiplicación de los determinantes
%\newslide %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%newslide %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%Dada una matriz $A$ de orden $n$. El determinante de un escalar por matriz es 
% \[|cA|=c^{n}|A|\]
% debido a que cada uno de los $n$ renglones se multiplica por $c$.
%
%
%%\newslide %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\newslide %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%La transpuesta no afecta el determinantes
%%\begin{align*}
%\[det(A^T)=det{A^T}\]
%%\end{align*}
%debido a que los productos elementales con signo no se afectan. Este es un resultado muy importante debido a que permite extrapolar todo lo dicho para los renglones a las columnas. 
%
%%\newslide %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%%Taller de repaso.
%%
%%El determinante de una matriz de $2 \x 2$ se define
%%\begin{align*}
%%&\begin{vmatrix}
%%a_{11} & a_{12}  \\
%%a_{21} & a_{22}   \\
%%\end{vmatrix}=a_{11}a_{22}-a_{12}a_{21}
%%\intertext{Calcule}
%%&\begin{vmatrix}
%%3 & 1  \\
%%2 & 9   \\
%%\end{vmatrix}
%%\end{align*}
%%
%%El determinante de una matriz de $3 \x 3$ se define
%%
%%\begin{align*} %\text{ Sea }
%%\begin{vmatrix}
%%a_{11} & a_{12} & a_{13} \\
%%a_{21} & a_{22} & a_{23} \\
%%a_{31} & a_{32} & a_{33} \\	
%%\end{vmatrix}
%%=&+a_{11}\begin{vmatrix}
%% a_{22} & a_{23} \\
%% a_{32} & a_{33} \\	
%%\end{vmatrix}\\
%%&-a_{12}\begin{vmatrix}
%% a_{21} & a_{23} \\
%% a_{31} & a_{33} \\	
%%\end{vmatrix}\\
%%&+a_{13}\begin{vmatrix}
%% a_{21} & a_{22} \\
%% a_{31} & a_{32} \\	
%%\end{vmatrix}\\
%% \end{align*}
%%Usando esta definición muestre que 
%%\[\begin{vmatrix}
%%1 & 2 & 8 \\
%%1 & 3 & 6 \\
%%2 & 0 & 3 \\
%%\end{vmatrix}=-21\]
%%En la definición anterior se desarrolló el determinante por la primera columna pero da igual si se desarrolla por cualquier columna o cualquier renglón. Calcule ahora el determinante desarrollándolo por la segunda columna, compare cual método es más sencillo y explique porque.
%%
%%En el caso de las matrices de $3 \x 3$ los productos elementales del determinante se puede con calcular expandiendo la matriz como se muestra en \cite[Sec. 2.1 pg 174]{Gro05}.
%%\begin{align*} %\text{ Sea }
%%&\begin{vmatrix}
%%a_{11} & a_{12} & a_{13} \\
%%a_{21} & a_{22} & a_{23} \\
%%a_{31} & a_{32} & a_{33} \\	
%%\end{vmatrix}
%%\begin{matrix}
%%a_{11} & a_{12}  \\
%%a_{21} & a_{22}  \\
%%a_{31} & a_{32}  \\	
%%\end{matrix}\\
%%%
%%%los productos 
%%%Calcule de nuevo este determinante   
%%%\begin{align*} %\text{ Sea }
%%%
%%%\intertext{Cada una de estas matrices de $2 \x 2$ se llaman el \de{menor} y como cada menor tiene dos productos elementales entonces una matriz de $3 \x 3$ tiene $6$ productos elementales}
%%%|A|=
%%&+ a_{11}a_{22}a_{33} \  + a_{12}a_{23}a_{31} \  + a_{13}a_{21}a_{32} \\
%%&- a_{11}a_{23}a_{32} \  - a_{12}a_{21}a_{33} \  - a_{13}a_{22}a_{31}
%% \end{align*}
%%Calcule nuevamente el determinante anterior usando este método y compárelo con los métodos anteriores.
%%
%%En una matriz de $2 \x 2$ el número de productos elementales son $2$, en una matriz de $3 \x 3$ son $3 \cdot 2$ y en una matriz de $4 \x 4$ son $4 \cdot 3 \cdot 2 = 24$ productos elementales, los cuales son demasiados para se representados como se mostró en el último metodo de las matrices de $3 \times 3$ por tal motivo para las matrices de $4 \x 4$ y superiores el determinante se desarrolla por el renglón o la columna con más ceros.
%%
%%
%%
%%%Determinante de una matriz de $4 \times 4$
%%
%%%\[ \text{ Sea }A=\begin{bmatrix}
%%%a_{11} & a_{12} & a_{13} & a_{14} \\
%%%a_{21} & a_{22} & a_{23} & a_{24} \\
%%%a_{31} & a_{32} & a_{33} & a_{34} \\	
%%%a_{41} & a_{42} & a_{43} & a_{44} \\	
%%%\end{bmatrix}\]
%%
%%
%%Demuestre que los determinante son 
%%\[\begin{vmatrix}
%%-2& 3 & 0 & 1 \\
%%0 & 3 & 2 & 4 \\
%%0 & 0 & 1 & 3 \\
%%0 & 0 & 0 &-2 \\
%%\end{vmatrix}=12\]
%%\[\begin{vmatrix}
%%1 & 3 & 5 & 2 \\
%%0 & 0 & 0 & 4 \\
%%2 & 1 & 9 & 6 \\
%%3 & 2 & 4 & 8 \\
%%\end{vmatrix}=192\]
%
%
%
%
%
%
%\teorema{El determinante de un producto es el producto de los determinantes}
%\teorema{Un determinante es diferente de cero si y sólo si\\esa matriz es invertible si y sólo si \\el sistema homogéneo asociado tiene solución única.}
%%\teorema{El sistema homogéneo tiene solución única si y sólo si la matriz de coeficientes tiene determinante diferente de cero }
%
%
%
%


----------------incluir el producto cruz y vectores propios


%

Para estudiar este tema hay que realizar los ejercicios de las secciones 6.1 y 6.2 de \cite{NJ99}.

En particular se recomiendan los ejercicios 6.1.\{1.a, 3.a, 7, 9, 17, 30.a\} 6.2.\{1-19, 24, 28, 32, 35, 36, 37, 38, 39\} 

Dado un sistema de ecuaciones responda
\begin{enumerate}
\item escriba la matriz de coeficientes $A$
\item escriba el vector de las constatntes $\ve{b}$
\item escriba el vector de las variables $\ve{x}$
\item Escriba la matriz extendida $[A:b]$
\item escriba el sistema de ecuaciones como un producto de matriz por vector $A\ve{x}=\ve{b}$

para la matriz de coeficientes $A$ responda
\item ¿cuál es el tamaño de $A$?
\item ¿cuál es el número de columnas (n) de $A$? 
\item ¿cuál es el número de renglones (m) de $A$? 
\item ¿es identidad?
\item ¿cual es la diagonal?
\item ¿es diagonal?
\item ¿es triangular superior?
\item ¿es triangular inferior?
\item ¿es simétrica?
\item ¿es el resultado de una operación elemental?

para la matriz extendida $[A:b]$ responda
\item ¿tiene un renglón ceros?
\item E1 ¿renglones cero en la parte inferior?
\item Escalón E1 y E2 ¿el elemento delantero del siguiente renglón está a la derecha los si?
\item Escalonada E1, E2 y E3 ¿es 1 cada pivote?
\item Escalón reducido E1, E2,E3 y E4 ¿hay ceros arriba y abajo de cada pivote?

Utilizando operaciones elementales 
\item Colocar la matriz en forma escalon pero no escalonada
\item Colocar la matriz en forma escalonada pero no escalón reducida

\item ¿cuales son las posiciones de los pivotes de $[A:b]$?
\item ¿cuales variables son delanteras o pivotes?
\item ¿cuales variables son libres o parámetros?
\item ¿es inconsistente?
\item ¿tiene solución única?
\item ¿tiene infinitas soluciones?

\item encuentre una solución particular
\item escriba la solución general
\item escriba el conjunto solución

\item ¿$A$ genera a $\R^n$?
\item ¿$A$ genera a $\R^m$?
\item ¿Las columnas de da $A$ son L.I.?
\item ¿Los renglones de $A$ son L.I.?
\item ¿$A$ es una base?
\item la matriz $A$ ¿es equivalente a la identidad?

\item Colocar $[A:b]$ en forma escalón reducido

\item Encuentre la inversa de $A$ utilizando operaciones elementales
\item Despeje $\ve{x}$ utilizando $A^{-1}$

\item Encuentre $|A|$ si es posible.
\item Encuentre la adjunta de $A$ si $A$ es cuadrada.
\item Encuentre $A^{-1}$ usando la adjunta si es posible.
\item Despeje $\ve{x}$ utilizando la regla de Cramer.


\end{enumerate}


\section{Bases e Isomorfismos}
Sea $V$ el espacio generado por las columnas de una matriz $A \in \R^{m \times n}$. Se dice que la matriz 	$B$ es una \de{base} del espacio $V$ si las columnas de $B$ cumplen dos cosas
\begin{itemize}
\item generan a $V$.
\item son linealmente independientes.
\end{itemize}
El número de columnas de $B$ se llama la \de{dimensión} del espacio $V$ y se denota $dim(V)$.

\teorema{Con la notación anterior se tiene que 
\begin{itemize}
\item $dim(V)$ corresponde al número de columnas pivote de $A$.
\item $T_B$ es un isomorfismo entre $\R^{dim(V)}$ y $V$.
\end{itemize}
}

\teorema{Para cualquier matriz $B$ se tiene que $T_B$ es un isomorfismo entre $\R^{R(B)}$ y $gen(B)$ si y sólo si $B$ es una base de $gen(B)$}

Un isomorfismo $T_B$ entre $\R^{R(B)}$ y $gen(B)$ permite relacionar cada vector de $\ve{b} \in gen(B)$ con uno y sólo un vector de $\ve{c} \in \R^{R(B)}$ llamado \de{vector de coordenadas} de $\ve{b}$ con respecto a la base $B$ y se denota $\ve{c}=[\ve{b}]_B$. %Esto permite decir que el vector $\ve{c} \in \R^{R(B)}$ corresponde a las \de{coordenadas} de del vector $\ve{b} \in gen(B)$. 

%Si la base $B$ es una matriz cuadrada de orden $n$ entonces $R(B)$ genera todo el espacio $R^n$ y en este caso se cumple






\teorema{\cite[Teorema 1.10.3 y Teorema 2.3.2]{Gro05} y \cite[Teorema 2.3.3]{Ant06}
%Si la base $B$ es una matriz cuadrada de orden $n$ entonces $R(B)$ genera todo el espacio $R^n$ y en este caso se cumple
Si $A$ es una matriz \underline{cuadrada} de orden $n$, las siguientes afirmaciones son equivalentes: 
\begin{enumerate}[ii]
\item[(i)] $A$ es invertible.
\item[(ii)] $A$ es el producto de matrices elementales.
\item[(iii)] $|A| \neq 0$.
\item[(iv)] $A$ no tiene parámetros.
\item[(v)] Cada columna de $A$ tiene un pivote.
\item[(vi)] $A$ tiene $n$ pivotes.
\item[(vii)] Cada renglón de $A$ tiene un pivote.
\item[(viii)] El sistema $A\ve{x}=\ve{b}$ tiene solución única para cualquier vector $\ve{b} \in \R^n$.
\item[(ix)] La única solución del sistema $A\ve{x}=\ve{0}$ es la trivial $\ve{x}=\ve{0}$.
\item[(x)] $T_A$ y $A$ tienen rango $n$.
\item[(xi)] La dimensión de las columnas de $A$ es $n$
\item[(xii)] La nulidad de $T_A$ y de $A$ es cero.
\item[(xiii)] $A$ es una base de $\R^n$.
\item[(xiv)] $T_A$ es un isomorfismo en $\R^n$
\item[(xv)] El espacio generado por los renglones de $A$ es $\R^n$.
%\item[(xvi)] .
\end{enumerate}
}
%Sis ecua
%MAt vect
%mat ext
%trans lin
%cordenadas
%combiancion lineal


????????????????????Algoritmo para econtrar base de la imagen y del núcleo

\section{Inverso de un Isomorfismo $T_B$ entre $\R^{R(B)}$ y $gen(B)$}
Si $B$ es una matriz cuadrada el problema queda resuelto porque el inverso de $T_B$ es $(T_B)^{-1}:=(T_{B^{-1}})$. Pero si la matriz no es cuadrada no se puede obtener la matriz inversa y por lo tanto hay problemas.

En este caso procedemos de la misma manera en que encontramos la inversa de una matriz. Resolviendo un sistema de ecuaciones. Lo cual vamos a ilustrar mediante un ejemplo.

Suponga el espacio $V=gen \parr{ \bsmatriz{1&-1&0&1\\1&1&1&0\\1&1&1&0}}$. Debido a que
 \[\bsmatriz{1&-1&0&1\\1&1&1&0\\1&1&1&0}%$ es equivalente a $
 \sim \bsmatriz{1&-1&0&1\\0&2&1&-1\\0&0&0&0}\] tenemos que las dos primeras columnas son pivotes y por lo tanto una base de $V$ es 
%\bsmatriz{1\\1\\1} \bsmatriz{-1\\1\\1}  \bsmatriz{0\\1\\1}  \bsmatriz{1\\0\\0}} $. Debido a que $\bsmatriz{1&-1&0&1\\1&1&1&0\\1&1&1&0}$ es equivalente a $\bsmatriz{1&-1&0&1\\0&2&1&-1\\0&0&0&0}$ tenemos que las dos primeras columnas son pivotes y por lo tanto la base es 
%$V=gen \parr{ \bsmatriz{1\\1\\1} \bsmatriz{-1\\1\\1}} $
$B=\bsmatriz{1&-1\\1&1\\1&1}$. Por lo tanto $T_B$ es un isomorfismo entre  $\R^2$ y $V$. Al ser isomorfismo entonces nos podemos preguntar por la transformación inversa ${T_B}^{-1}$. Para encontrar esta transformación vamos a resolver el siguiente sistema de ecuaciones.
\begin{align*}
&\ \matriz{x_1&x_2}&&&&\ \matriz{x_1&x_2}\\
&\bmatriz{1&-1&:&y_1&&\\1&1&:&&y_2&\\1&1&:&&&y_3}&& \sim && \bmatriz{1&0&:&\frac{1}{2}y_1&+&\frac{1}{2}y_2&+&0\\0&1&:&-\frac{1}{2}y_1&+&\frac{1}{2}y_2&+&0\\0&0&:&0&-&y_2&+&y_3}
\end{align*}

Hemos encontrado una matriz que permite despejar la ecuación $A\ve{x}=\ve{b}$ en la ecuación  $\ve{x}=B\ve{b}$. 

La tercera ecuación restringe los valores de $y_1$, $y_2$ y $y_3$ para que permanezcan dentro de $V$, es de resaltar que $0y_1-y_2+y_3=0$ es la ecuación del plano $V$ generado por $B$. Las primeras dos ecuaciones nos permites despejar $x_1$ y $x_2$. 

\[\bmatriz{x_1\\x_2}=\bmatriz{\frac{1}{2}&\frac{1}{2}&0\\ -\frac{1}{2}&\frac{1}{2}&0} \bmatriz{y_1\\y_2\\y_3}\]

----- falta mostrar que es semi-inversa
\section{Subespacio propio}
Un \de{subespacio propio} de $\R^n$ es cualquier subespacio diferente del mismo $\R^n$. Por ejemplo los subespacios propios de  $\R^3$ son: el origen, las recta que pasan por el origen y  los planos que pasan por el origen pero no todo el espacio.

Debido a que la dimensión del subespacio propio es menor a la del espacio entonces la matriz formada por una base del subespacio \underline{nunca} es cuadrada.

Ejemplo: Encontrar una base para el plano que pasa por el origen $x+2y+3z=0$.

\section{Coordenadas de una base}
Sabemos que dada una base $B=[\ve{v_1},\ve{v_2},\ldots,\ve{v_n}]$ de un subespacio $V$ podemos definir cualquier vector $\ve{u} \in V$ de manera única, dando los valores de los coeficientes $\bsmatriz{c_1\\ \vdots \\ c_n}$, ya que
\[ \ve{u} = c_1 \ve{v_1} + c_2 \ve{v_2} + \cdots + c_n \ve{v_n} %=\bmatriz{\ve{v_1} & \ve{v_2} & \ldots & \ve{v_n} } \bsmatriz{c_1\\ \vdots \\ c_n}
\]

el vector de coeficientes $\bsmatriz{c_1\\ \vdots \\ c_n}$ se llama el \de{vector coordenado} o \de{vector de coordenadas} de $\ve{u}$ con  respecto a $V$ y se escribe
\[ [\ve{u}]_B = \bsmatriz{c_1\\ \vdots \\ c_n} \]  

Recuerde: Para encontrar las coordenadas del vector $\ve{u}$ en la base $B$ hay que resolver el sistema $[B:\ve{b}]$.

\section{Transformaciones lineales de $\R^n$ a $\R^m$ \%(cambiar de lugar esta sección y la siguiente)}

Una transformación lineal de $\R^n$ a $\R^m$ $T_A:\R^n \ra \R^m $ es una función  de $\R^n$ a $\R^m$  que cumple para todo $\ve{u}, \ve{v} \in \R^n$
\begin{itemize}
\item $T(\ve{u}+\ve{v})=T(\ve{u})+T(\ve{v})$
\item $T(c\ve{u})=cT(\ve{u})$ 
\end{itemize}

Una transformación matricial $T_A:\R^n \ra \R^m $ se llama \de{uno a uno} si a para cualquier par de vectores distintos de  $\R^n$ nunca les corresponde el mismo vector en $\R^m$.

$T_A$ es uno a uno\\ si y sólo si 
las columnas de $A$ son L. I. \\ si y sólo si la nulidad es cero. 


Las transformaciones matriciales debida a una base $B$ siempre son uno a uno debido a que como cada columna de $B$ tiene pivote entonces el sistema de ecuaciones correspondiente tiene solución única. 

Una transformación matricial $T_A:V \ra W $ se llama \de{isomorfismo} si $T_A$ es uno a uno y sobre.

Demuestre que la transformación $T_A:\{\bsmatriz{x\\0\\z} \mid x, z \in \R\} \ra \R^2$  es un isomorfismo.

\section{Transformaciones matriciales entre espacios}

La matriz formada por la base de un espacio $\R^n$ siempre es una matriz cuadrada e invertible. A continuación sólo hablaremos de las bases para $\R^n$.

Cuales de las siguientes matrices son base de $\R^4$...


Las columnas de la matriz identidad corresponden a la base canónica. Pero hay muchas otras bases.

Calcule las coordenadas ... en cada una de las bases


Una Base se puede ver como una transformación

Si tengo las coordenadas en una base como encuentro el vector (en la base canónica)



\section{Cambio de base}

















---llegar a la formula usual

%determinante

\newslide
\section{Vectores y bases ortogonales}

\cite[8.1]{NJ99}


Dos vectores son \de{ortogonales} si su producto punto es cero.\\[1cm]

En un \de{conjunto de vectores-$m$  ortogonal} cada par de vectores son ortogonales. Una \de{base ortogonal} es una base de un \underline{subespacio} de $\R^n$, cuyo conjunto de vectores es ortogonal. 

\teorema{Si $\ve{u}$ es la combinación lineal de un conjunto de vectores ortogonales \[\ve{u}=c_1 \ve{v_1} + c_2 \ve{v_2} + \cdots + c_i \ve{v_i} + \cdots  +  c_n \ve{v_n} \] entonces  \[c_i = \frac{\ve{u} \cdot \ve{v_i} }{ \ve{v_i} \cdot \ve{v_i} }\]}%\\[1cm]
\\\teorema{En cualquier conjunto ortogonal de vectores distintos de cero los vectores son linealmente independientes}
\\\teorema{Si las columnas de $A=[ \ve{v_1} \ \  \ve{v_2} \cdots  \ve{v_n} ]$ forman un conjunto ortogonal entonces \[A^T A=\bmatriz{
|| \ve{v_1} || & 0 & \cdots & 0\\
0 & || \ve{v_2} || & \cdots & 0\\
\vdots & \vdots & \ddots \vdots\\
0& 0& \cdots  &  || \ve{v_n} ||
}\]}

\newslide
\section{Bases ortonormales}
Una \de{base ortonormal} es una base ortogonal en la cual cada vector es unitario.
\\\teorema{Si los vectores-$m$ $ \ve{v_1} , \ve{v_2} , \cdots , \ve{v_n}$ forman una base ortonormal de un \underline{subespacio} $V$ de $\R^m$, entonces cada vector-$m$ $ \ve{u} $ de $V$ puede escribirse en forma única \[ \ve{u} =  ( \ve{u} \cdot \ve{v_1} ) \ve{v_1} + ( \ve{u} \cdot \ve{v_2} ) \ve{v_2} + \cdots + ( \ve{u} \cdot \ve{v_n} ) \ve{v_n}. \] 
Ademas, si $A=[\ve{v_1} \ \  \ve{v_2} \cdots  \ve{v_n}]$ es una matriz de $m \times n$ entonces \[A^T A =I_n\]}

\newslide
\section{Matrices ortogonales}
Una \de{matriz ortogonal} es una matriz 
\begin{itemize}
\item cuadrada 
\item con todas sus columnas son unitarias 
\item cualquier par de columnas son ortogonales
\end{itemize} Para esta matriz un mejor nombre sería matriz ortonormal, pero este nombre no se usa, una matriz que no es cuadrada o que sus columnas no sean unitarias \underline{no} se llama ortogonal. Las columnas de una matriz ortogonal de $n \times n$ forman una base ortonormal del \underline{espacio} $\R^n$
\\\teorema{Para una matriz cuadrada invertible $A$ se tienen las siguientes equivalencias. 
\begin{itemize}
\item $A$ es ortogonal (i. e. columnas unitarias y ortgonales).
\item $A^{-1}=A^T.$
\item $A \ve{u} \cdot A \ve{v} = \ve{u} \cdot \ve{v} $ para todos los vectores-$m$ $ \ve{u} $ y $ \ve{v}. $
\item $||A \ve{v} || = || \ve{v} ||$  para todos los vector-$m$ $ \ve{v}. $
\end{itemize}}


\newslide

\section{vectores propios}


Un \de{vector propio} o \de{vector característico} $v$ de una matriz $A$ de $n \times n$, es un vector diferente de cero $v \in \R^n$ que cumple 
\[Av=\lambda v\]   
Donde el escalar $\lambda$ se conoce como un \de{valor propio} o \de{valor característico} de $A$ asociado al vector propio $v$. Los valores propios pueden ser cero.\\ 
\teorema{--Con la notación anterior, 
%\begin{itemize}
%\item 
$\lambda$ es un valor característico de $A$ si y sólo si \[det(A- \lambda I)=0\]
esta ecuación se llama \de{ecuación característica}. \\ $det(A- \lambda I)$ es él \de{polinomio característico},\\  el cual se puede factorizar 
\[( \lambda _1- \lambda )^{r_1} ( \lambda _2- \lambda )^{r_2} \cdots ( \lambda _k- \lambda )^{r_k} \] 
los números $r_i$ se llaman la \de{multiplicidad algebraica}. }\\
\teorema{Una matriz cuadrada $A$ es invertible si y sólo si 0 NO es un valor característico de $A$.}
\\\teorema{Los valores propios de una matriz diagonal son sus elementos diagonales}
%\item 
%\newslide
\\\teorema{$v$ es un vector propio de $A$ asociado a $\lambda$ si y sólo si  $v$ es una solución no trivial del sistema \[(A- \lambda I)v=0\]
$(A - \lambda I$) se llama la \de{matriz característica}
%\end{itemize}
}

El núcleo de $(A- \lambda I)v=0$ se llama el \de{espacio característico} y en [NJ99] se denota con $E_\lambda$. La \de{multiplicidad geométrica} es la dimensión del espacio característico.

 

%\newslide

Algoritmo para el calculo de valores característicos, vectores característicos y bases de espacio característico.
\begin{enumerate}
\item Se encuentran las raíces $\lambda_i$ del polinomio característico $det(A- \lambda I)=0$.
\item Para cada valor propio $\lambda_i$ encontrar una base del núcleo $(A- \lambda I)v=0$. 
\end{enumerate}

\newslide
Las matrices diagonales tienen las siguientes ventajas
\begin{itemize}
\item Cuando multiplican no mezclan renglones
\item Es fácil calcular potencias 
\end{itemize}



Dos matrices cuadradas $A$ y $B$ son \de{semejantes} si existe una matriz invertible $C$ tal que \[B=C^{-1}AC.\]

Si una matriz cuadrada $A$ es semejante a una matriz diagonal $D$ entonces se dice que $A$ es \de{diagonalizable}.


\teorema{Si $A$ y $B$ son matrices semejantes entones de $n \times n$ entonces $A$ y $B$ tienen el mismo polinomio característico y por lo tanto los mismos valores característicos}

Una matriz $A$ es \de{diagonalizable} si existe una matriz diagonal $D$ que es semejante a $D$.

%\teorema{Si $A$ es diagonalizable entonces es semejante a una matriz diagonal cuyas componentes son los valores característicos de $A$}  

\teorema{Una matriz $A$ es diagonalizable si y sólo si tiene $n$ vectores característicos linealmente independientes. En este caso la matriz diagonal $D$ que es semejante a $A$ está formada por los valores característicos de $A$.
Si $C$ es una matriz cuyas columnas son vectores característicos linealmente independientes de $A$, entonces \[D=C^{-1}AC\]}

Algoritmo para diagonalizar una matriz, cuando es posible.
\begin{enumerate}
\item Determinar las bases $B_1, \ldots, B_k$ para cada valor propio y formar la unión $B=[B_1  \cdots  B_k]$.
\item Si $B$ no es cuadrada detenerse. A no es diagonalizable.
\item Si $B$ es cuadrada diagonaliza a $A$ ya que $D=B^{-1}AB$, donde $D$ es la matriz diagonal con los valores propios en el respectivo orden de $B$.
\end{enumerate}

\teorema{Si $A$ es diagonalizable por $P$ y $D$ entonces para $k=1,2,\ldots$\[A^k=PD^kP^{-1}\]
Si $A$ es invertible entonces es posible que $k=-1,-2,\ldots$}

\newslide
Recordemos que una matriz $A$ es \de{simétrica} si $A'=A$, es decir si sus columnas son iguales a sus renglones.
\[\bmatriz{a&b\\b&c},\bmatriz{a&b&c\\b&d&e\\c&e&f},\dots\]
El nombre hace referencia a la simetría que hay respecto a la diagonal.

\teorema{Sea $A$ una matriz real simétrica de $n\times n$, entonces $A$ tiene $n$ vectores característicos reales ortonormales}
%\begin{itemize}
%\item Los valores característicos de $A$ son reales.
%\item Si $\lambda_1$ y \lambda_2





\newslide



\chapter{Espacio Euclidiano}


\section{Operación producto punto}

Al multiplicar las matrices $C=AB$, con $A \in \R^{m \times k}$ y $B \in \R^{k \times n}$, cada elemento de la matriz $C$ corresponde \[c_{ij}=a_{i1}b_{1j}+a_{i2}b_{2j}+ \cdots +a_{ik}b_{kj}\]
donde $a_{i1},a_{i2},\ldots,a_{ik}$ es el renglón $i$-esimo de la matriz $A$ y $b_{1j},b_{2j},\ldots,b_{kj}$ es la columna $j$-esima de la matriz $B$. Esto define el \de{producto punto} o \de{producto escalar} entre dos vectores-$m$ (independientemente si son renglón o columna) $\ve{u}=[a_{1},a_{2},\ldots,a_{m}]$ y $\ve{v}=[b_{1},b_{2},\ldots,b_{m}]$ que da como resultado un escalar.
%\[[a_{1} \ a_{2} \ \ldots \ a_{m}] \cdot [b_{1} \ b_{2} \ \ldots \ b_{m}] =a_{1}b_{1}+a_{2}b_{2}+ \cdots +a_{m}b_{m}\] 


%El \de{producto punto} o \de{producto escalar} o %\de{producto interno} 
%entre dos n-vectores $\overline{a}$ y $\overline{b}$ se define
\begin{align*}
\ve{u} \cdot \ve{v}
&=[ a_1 \  a_2 \  \cdots \  a_n ] \cdot [ b_1 \ b_2 \ \cdots \ b_n ] \\
&=\begin{bmatrix} a_1 \\ a_2 \\ \vdots \\ a_n \end{bmatrix} \cdot [ b_1 \  b_2 \  \cdots \  b_n ] \\
&=[ a_1 \  a_2 \  \cdots \  a_n ] \cdot \begin{bmatrix} b_1 \\ b_2 \\ \vdots \\ b_n \end{bmatrix} \\
&=\begin{bmatrix} a_1 \\ a_2 \\ \vdots \\ a_n \end{bmatrix} \cdot \begin{bmatrix} b_1 \\ b_2 \\ \vdots \\ b_n \end{bmatrix}
:= a_1 b_1 + a_2 b_2 + \cdots + a_n b_n 
\end{align*}

Si consideramos los vectores como matrices columna entonces el producto punto se puede considerar como un producto matricial. 
\[\ve{u} \cdot \ve{v}=\ve{u}^T  \ve{v}\]

Resumiendo
%\defOpMat{AB}{Producto punto entre el vector-$m_1$ $\ve{v_1}$ y el vector-$m_2$ $\ve{v_2}$}
%{ si y solo si $m_1=m__2$}%el número de columnas de A coincide con el número de  renglones de $B$}
%{m_1=m_2}%{[size(A,"r"),size(A,"c")}
%{(v_1)_{1}(v_2)_{1}+(v_1)_{2}(v_2)_{2}+ \cdots +(v_{m_1})_{m_1}(v_2)_{m_{_2}}}
%{}\verb!A*B!

\defOpMatA{\ve{v} \cdot \ve{u}}{Producto punto entre el vector-$m$ $\ve{v}$ y el vector-$n$ $\ve{u}$}
{si $m=n$}
{un escalar}%, \text{donde $m$ es el número de renglones de} \text{$A$ y $n$ es el número de columnas de $B$}}
{$(\ve{v})_{1}(\ve{u})_{1}+(\ve{v})_{2}(\ve{v})_{2}+\cdots+(\ve{v})_{m}(\ve{u})_{n}$ }
{}%\verb!A*B!
%Para el producto punto en Scilab hay que 
usando la multiplicación matricial y trasponiendo el primer vector, debido a que Scilab no tiene vectores como tal. A continuación se muestra un ejemplo.
\begin{verbatim}
-->a=[1;2;3];
-->b=[1;1;1];
-->a'*b
 ans  =
    6.  
\end{verbatim}

Dos vectores-$m$ son \de{ortogonales} o \de{perpendiculares} si el producto punto entre ellos es cero. Gráficamente dos vectores ortogonales tienen un \dn{ángulo} de 90°. 


%\newslide
%El producto punto permite describir cada uno de los elementos de la multiplicación de matrices %$A=(a_{ij})_{m \times r}$ y $B=(b_{ij})_{r \times n}$.
Algunas propiedades del producto punto son

\teorema{
\dctb{ConPun}{Conmutativa del producto punto}{$\ve{u}\cdot\ve{v}=\ve{v}\cdot\ve{u}$}
\dctb{DitrPun}{Distributtiva del producto punto}{$\ve{u}\cdot(\ve{v}+\ve{w})=\ve{v}\cdot\ve{u}+\ve{v}\cdot\ve{w}$}
\dctb{AsoPun}{Asociativa del producto punto}{$c( \ve{u} \cdot \ve{v})=(c \ve{v}) \cdot \ve{u}= \ve{v} \cdot (c \ve{u})$}
\dctb{PosPun}{Definición positiva del producto punto}{ $ \ve{u} \cdot \ve{u} \geq 0$. Además, $ \ve{u} \cdot \ve{u} = 0$ si y sólo si $\ve{u}=0$}
\dctb{MatVecPun}{Producto punto con matriz $A$ por vector}{\linebreak$(A \ve{u} ) \cdot \ve{v}= \ve{u} \cdot (A^T \ve{v} )$}}


%\section{Producto punto}
%\cite[2.2]{NJ99} 
%El \de{producto punto} o \de{producto escalar} entre dos vectores-$m$ $\overline{a}$ y $\overline{b}$ es el \underline{número}
%\begin{align*}
%\overline{a} \cdot \overline{b}&:= a_1 b_1 + a_2 b_2 + \cdots + a_m b_m 
%\end{align*}
%
%Para el producto punto no importa si son columnas o renglones
%\begin{align*}[ a_1 \  a_2 \  \cdots \  a_m ] \cdot [ b_1 \ b_2 \ \cdots \ b_m ]&:= a_1 b_1 + a_2 b_2 + \cdots + a_m b_m \\
%\begin{bmatrix} a_1 \\ a_2 \\ \vdots \\ a_m \end{bmatrix} \cdot [ b_1 \  b_2 \  \cdots \  b_m ]&:= a_1 b_1 + a_2 b_2 + \cdots + a_m b_m \\
%[ a_1 \  a_2 \  \cdots \  a_m ] \cdot \begin{bmatrix} b_1 \\ b_2 \\ \vdots \\ b_m \end{bmatrix}&:= a_1 b_1 + a_2 b_2 + \cdots + a_m b_m \\
%\begin{bmatrix} a_1 \\ a_2 \\ \vdots \\ a_m \end{bmatrix} \cdot \begin{bmatrix} b_1 \\ b_2 \\ \vdots \\ b_m \end{bmatrix}
%&:= a_1 b_1 + a_2 b_2 + \cdots + a_m b_m 
%\end{align*}
%
%
%
%\newslide
%\section{Propiedades del producto punto}\cite[pag 81]{NJ99}
%Sean $\ve{u},\ve{v}, \ve{w}$ vectores-$m$ y $c$ cualquier escalar.
%\begin{description}
%\item{Simetría} $ \ve{u} \cdot \ve{v} = \ve{v} \cdot \ve{u}. $
%\item{Aditividad} $ \ve{u} \cdot ( \ve{v} + \ve{w} )=( \ve{u} \cdot \ve{v} ) + (\ve{u} \cdot \ve{w}).$
%\item{Homogeneidad} $c( \ve{u} \cdot \ve{v} )=(c \ve{u} ) \cdot \ve{v} = \ve{u} \cdot (c \ve{v} ).$
%\item{Definición positiva} $ \ve{u} \cdot \ve{u} \geq 0$. Además, $ \ve{u} \cdot \ve{u} = 0$ si y sólo si $\ve{u}=0$.
%\end{description}
 
\newslide
\section{Magnitud}\cite[pag 79]{NJ99}\\
 La \de{norma}, \de{longitud} o \de{magnitud} de un vector $\ve{u}$ es $||u||=\sqrt{ \ve{u} \cdot \ve{u}}$\\
 Un \de{vector unitario} es un vector de magnitud uno.
 
%\section{Propiedades} 
%\begin{description}
Si $ \ve{u} $ y $ \ve{v} $ son dos vectores-$n$ cualquiera, se tiene \\
\teorema{
\dctb{MagEscVec}{Magnitud de escalar por vector}{$||c \ve{u} ||=abs(c) || \ve{u} ||$}
\dctb{MagPos}{La magnitud es positiva}{$ ||\ve{u}|| \geq 0$. Además, $ ||\ve{u}|| = 0$ si y sólo si $\ve{u}=0$}
%\end{description}
\dctb{VectUni}{Vector unitario}{Si $ \ve{u} \neq \ve{0}$ entonces $\frac{1}{|| \ve{u} ||} \ve{u} $ es un vector unitario}}
%\newslide
%\section{Igualdades y desigualdades de magnitud}
% Teorema 5, 6, 7, 8
%\begin{description}
%\item
\teorema{
\dctb{MagSum}{Magitud de la suma}{$|| \ve{u} + \ve{v} ||^2=|| \ve{u} ||^2+|| \ve{v} ||^2+2 \ve{u} \cdot  \ve{v} $}
%\item
\dctb{Pitágoras}{Teorema de Pitágoras}{ Es un caso particular del anterior si $ \ve{u} \cdot \ve{v}=0$ entonces $|| \ve{u} + \ve{v} ||^2=|| \ve{u} ||^2+|| \ve{v} ||^2$}  %cuando $ \ve{u} $ y $ \ve{v} $ son ortogonales.
%\item
\dctb{MagRes}{Magitud de la resta}{$|| \ve{u} - \ve{v} ||^2=|| \ve{u} ||^2+|| \ve{v} ||^2-2 \ve{u} \cdot  \ve{v} $}
\dctb{Cauchy-Schwarz}{Desigualdad de Cauchy-Schwarz}{$abs( \ve{u} \cdot \ve{v} ) \leq || \ve{u} || \ \ || \ve{v} ||$ la igualdad se da cuando  $ \ve{u} $ y $ \ve{v} $ son paralelos}
\dctb{DesTrianM}{Desigualdad triangularen magnitud}{ $|| \ve{u} + \ve{v} || \leq || \ve{u} || + || \ve{v} ||$}}
%\end{description}}

\newslide
\section{Distancia entre dos vectores o dos puntos}
\[dist( \ve{u} , \ve{v} ):=|| \ve{u} - \ve{v} ||\]


\teorema{Si $ \ve{u}, \ve{v} $ y $ \ve{w} $ son vectores-$m$ entonces
%\begin{itemize}
\dctb{DistPos}{Distancia positiva}{ $dist( \ve{u} , \ve{v} ) \geq 0$}
\dctb{DistCero}{Cistancia cero}{ $dist( \ve{u} , \ve{v} ) = 0$ si y sólo si $ \ve{u} = \ve{v} $ }
\dctb{DistConm}{Distancia Conmutativa}{ $dist( \ve{u} , \ve{v} ) = dist( \ve{v} , \ve{u} )$}
\dctb{DesTrianD}{Desigualdad triangular en distancia}{ $dist( \ve{u} , \ve{v} ) \leq  dist( \ve{u} , \ve{w} ) + dist( \ve{w} , \ve{v} )$}}
%\end{itemize}}


\cite[Teorema 4.1.5]{Ant06}
Esta distancia se conoce como distancia euclidiana. %Cualquier otra función que cumpla las propiedades anteriores también es una distancia. Un espacio con la función de distancia se llama espacio métrico.



\newslide
\section{Proyección}\cite[pag 86]{NJ99}\\
La proyección del vector-$m$ $ \ve{u} $ sobre el vector $ \ve{v} $ se define\[ proy_v(\ve{u})=\frac{\ve{u} \cdot \ve{v} }{ \ve{v} \cdot \ve{v} } \ve{v}\]
????????? escribirlo como vector unitario



\newslide
\section{Ángulo y funciones trigonométricas}
Funciones trigonométricas (Imagen y dominio)
Identidades trigonométricas
Deducción de la ley del coseno
Definición de coseno entre dos vectores
Definición del (menor) ángulo entre dos vectores
Definición del ángulo de un vector
 Trigonometría
 Teorema de coseno 

\newslide
Las funciones trigonométricas se definen
\begin{align*}
&sen(\alpha)&&cos(\alpha)&&tan(\alpha)\\
&\frac{Lo}{Hi}&&\frac{La}{Hi}&&\frac{Lo}{La}  
\end{align*}

Por el teorema de Pitágoras se obtiene
\[sen^2(\alpha) + cos^2(\alpha) = 1\]


\newslide
\begin{align*}
&{sen}(\alpha) ={sen}(\alpha + 2\pi) && {sen}(-\alpha) = {sen}(\alpha+\pi) && {sen}(\alpha) = \cos\left(\frac{\pi}{2} - \alpha\right)\\
& \cos(\alpha) = \cos(\alpha + 2\pi)  && \cos(-\alpha) = -\cos(\alpha+ \pi) && \cos(\alpha) = \operatorname{sen}\left(\frac{\pi}{2}-\alpha\right)\\
&  \tan(\alpha) = \tan(\alpha + \pi) && \tan(-\alpha) = -\tan(\alpha) && \tan(\alpha) = \cot\left(\frac{\pi}{2} - \alpha\right)\\
\end{align*}
%\[{sen}\left(\frac{\pi}{2} - \alpha\right) = \cos(\alpha)
%\ \ \ \ \ \cos\left(\frac{\pi}{2} - \alpha\right) = {sen}(\alpha)
%%&\tan\left(\frac{\pi}{2} - \alpha\right) = \cot(\alpha) 
%\]

\newslide
Otras entidades útiles serán 
\[{sen}(\alpha \pm \beta) = {sen}(\alpha) \cos(\beta) \pm \cos(\alpha) {sen}(\beta) \]
\[\cos(\alpha \pm \beta) = \cos(\alpha) \cos(\beta) \mp {sen}(\alpha) {sen}(\beta)\]
\[a{sen}(\alpha)+b\cos(\alpha)=\sqrt{a^2+b^2}\cdot{sen}\left( \alpha+\arctan{\frac{b}{a}} \right)\]

\newslide
Teorema de seno y coseno
\[\frac{a}{{sen}(A)}= \frac{b}{{sen}(B)} = \frac{c}{{sen}(C)}\]
El teorema del coseno se puede resolver por un sistema de ecuaciones


\newslide
Después del repaso regresamos a los vectores.
En $\R^2$ se cumple que 
\[||\ve{u}||\  ||\ve{v}|| cos(\alpha) = \frac{1}{2}(||\ve{u}||^2+||\ve{v}||^2- ||\ve{u}-\ve{v}||^2)=\ve{u} \cdot \ve{v}\]
Con esto se encuentra una forma para calcular el coseno entre dos vectores.
\[cos(\alpha) = \frac{\ve{u} \cdot \ve{v}}{||\ve{u}||\  ||\ve{v}||}\]

\newslide
Aunque la formula anterior  sólo es válida en $\R^2$, si nos permite definir el \de{coseno entre dos vectores} para cualquier $\R^m$.
\[cos(\alpha) := \frac{\ve{u} \cdot \ve{v}}{||\ve{u}||\  ||\ve{v}||}\]

Debido a que la función $arccos$ está definida entre $0$ y $\pi$ entonces el ángulo entre dos vectores siempre es el más pequeños de los dos ángulos.

Calcule el ángulo entre dos vectores paralelos y entre dos vectores  ortogonales.

Los \de{cosenos directores} de un vector $\ve{v}$ son los cosenos entre $\ve{v}$ y cada uno de los ejes coordenados. 

Demuestre en $\R^2$ que el coseno director de un vector $\ve{v}$ con el eje y es $sen(\alpha)$

\newslide

En \cite[pg 223]{Gro06} se define la \de{dirección de un vector} como el ángulo entre $0$ y $2\pi$ que forma el vector con el lado positivo del eje x.

Esta definición no es muy usada ya que arccos y arctan  no tiene una imagen de $0$ a $2\pi$ y por lo tanto es necesario definir la dirección mediante 8 casos!.

\newslide
%\section{producto cruz}
\fbox{\begin{minipage}{9.5cm}
Definición de \de{producto cruz} \cite[pg 113]{NJ99}
\begin{align*}
\bmatriz{u_1\\u_2\\u_3} \times \bmatriz{v_1\\v_2\\v_3}&=\bmatriz{u_2 v_3-v_2 u_3\\u_3 v_1-v_3 u_1\\u_1 v_2-v_1 u_2}\\
&=\veu{i}\vmatriz{u_2&u_3\\v_2&v_3}-\veu{j}\vmatriz{u_1&u_3\\v_1&v_3}+\veu{k}\vmatriz{u_1&u_2\\v_1&v_2}\\
&=!\vmatriz{\veu{i} & \veu{j} & \veu{k}\\u_1&u_2&u_3\\v_1&v_2&v_3}!
\end{align*}
En el último renglón se usa sólo la `notación' de determinante
\end{minipage}}
\fbox{\begin{minipage}{9.5cm}
Propiedades \cite[Teoremas 2.6.\{20 y 21\}]{NJ99}. \\
Si $\ve{u}=\bsmatriz{u_1\\u_2\\u_3}$, $\ve{v}=\bsmatriz{v_1\\v_2\\v_3}$,  $\ve{w}=\bsmatriz{w_1\\w_2\\w_3}$ y $c \in \R$ entonces
%$\ve{u} \times \ve{v} = - \ve{v} \times \ve{u}.$\\
%$\ve{u} \times \ve{v} = - \ve{v} \times \ve{u}.$\\
%$\ve{u} \times \ve{v} = - \ve{v} \times \ve{u}.$\\
%$\ve{u} \times \ve{v} = - \ve{v} \times \ve{u}.$\\
%$\ve{u} \times \ve{v} = - \ve{v} \times \ve{u}.$\\
%$\ve{u} \times \ve{v} = - \ve{v} \times \ve{u}.$\\
%$\ve{u} \times \ve{v} = - \ve{v} \times \ve{u}.$\\
%$\ve{u} \times \ve{v} = - \ve{v} \times \ve{u}.$\\


%\begin{align*}
%\begin{itemize}
%\item $\ve{u} \times \ve{v} = - \ve{v} \times \ve{u}.$
%\item $\ve{u} \times \ve{v} = - \ve{v} \times \ve{u}.$
%\item $\ve{u} \times \ve{v} = - \ve{v} \times \ve{u}.$
%\item $\ve{u} \times \ve{v} = - \ve{v} \times \ve{u}.$
%\item $\ve{u} \times \ve{v} = - \ve{v} \times \ve{u}.$
%\item $\ve{u} \times \ve{v} = - \ve{v} \times \ve{u}.$
%\item $\ve{u} \times \ve{v} = - \ve{v} \times \ve{u}.$
%\item $\ve{u} \times \ve{v} = - \ve{v} \times \ve{u}.$
%\end{itemize}
%\end{align*}
\begin{align}
\tag{AconCruz} \ve{u} \times \ve{v} &= - \ve{v} \times \ve{u}.\\
\tag{DistrCruzI} \ve{u} \times (\ve{v} + \ve{w}) &=  \ve{u} \times \ve{v} + \ve{u} \times \ve{w}.\\
\tag{DistrCruzD} (\ve{v} + \ve{w}) \times \ve{u}  &=  \ve{v} \times \ve{u} + \ve{w} \times \ve{u}.\\
\tag{AsoCruz} c(\ve{u} \times \ve{v}) &=  (c\ve{v}) \times \ve{u}=\ve{v} \times (c\ve{u}).\\
\tag{AnulCruz} \ve{0} \times \ve{u} &=\ve{0}= - \ve{u} \times \ve{0}.\\
\tag{AutoAnul} \ve{u} \times \ve{u} &=  \ve{0}.\\
\tag{CruzCruz} \ve{u} \times (\ve{v} \times \ve{w}) &=  (\ve{u} \cdot \ve{w}) \ve{v}-(\ve{u} \cdot \ve{v}) \ve{w}.\\
\tag{PuntoCruz} \ve{u} \cdot (\ve{v} \times \ve{w}) &= \vsmatriz{u_1 & u_2 & u_3 \\v_1&v_2&v_3\\w_1&w_2&w_3}.\\
\tag{MagCruz} ||\ve{u}  \times \ve{v}||^2 &= ||\ve{u}||^2 ||\ve{v}||^2-(\ve{u} \cdot \ve{v})^2.\\
\tag{SenCruz} ||\ve{u}  \times \ve{v}|| &= ||\ve{u}|| ||\ve{v}|| sen(\theta).
\end{align}
\end{minipage}}


%def. simple
%det de $2 \times 2$
%det de $3 \times 3$
%ejemplos
%o prod cruz de vectores paralelos?
%prod cruz ortogonal.
%conmutativo?
%asociativo?
%anulativa?
%identidad?
%distribuye?
%aso con escalar?
%doble prod cruz
%Identidad de lagrange
%
%Aplic
%volumen del paralelepípedo
%y vectores coplanares
%
%
%$f=qv \times B$
%


\newslide
\section{Rectas}
Recordemos que una \de{recta que pasa por el origen} es el generado por un vector $\ve{v}$ diferente de cero. 
\[R=\{t \ve{v} \mid t \in \R \}\]
y por lo tanto la \de{ecuación de una recta que pasa por el origen} esta dada por
\[\ve{x}=t \ve{v}\]

% Generated with LaTeXDraw 1.9.4
% Thu Oct 18 20:10:47 COT 2012
\scalebox{1} % Change this value to rescale the drawing.
{
\begin{pspicture}(0,-3.0)(6.1,3.0)
\rput(3.1,0.0){\psaxes[linewidth=0.04](0,0)(-3,-3)(3,3)}
\psline[linewidth=0.04cm,fillcolor=Black,linestyle=dashed,dash=0.16cm 0.16cm](0.0,-1.4)(6.0,1.4)
\psline[linewidth=0.08cm,fillcolor=Black,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(3.1,0.0)(5.1,1.0)
\usefont{T1}{ptm}{m}{n}
\rput(3.9314063,0.71){$\ve{v}$}
\usefont{T1}{ptm}{m}{n}
\rput(5.9025,1.61){$R$}
\end{pspicture} 
}



\newslide
Una \de{recta} en general se obtiene al trasladar a un punto $\ve{P}$ una recta que pasa por el origen
\[\mathcal{R}=\{\ve{P}+t \ve{v} \mid t \in \R \}\]
y por lo tanto la \de{ecuación de una recta} que pasa por $\ve{p}$ esta dada por
\[\ve{x}=\ve{P}+t \ve{v}\]

\scalebox{1} % Change this value to rescale the drawing.
{
\begin{pspicture}(0,-3.2)(6.2521877,3.22)
\rput(3.1,-0.2){\psaxes[linewidth=0.04](0,0)(-3,-3)(3,3)}
\psline[linewidth=0.04cm,fillcolor=Black,linestyle=dashed,dash=0.16cm 0.16cm](0.0,0.4)(6.0,3.2)
\psline[linewidth=0.08cm,fillcolor=Black,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(4.4,2.4)(5.5,2.9)
\usefont{T1}{ptm}{m}{n}
\rput(5.0314064,3.01){$\ve{v}$}
\usefont{T1}{ptm}{m}{n}
\rput(6.0025,3.1907814){$\mathcal{R}$}
\psline[linewidth=0.08cm,fillcolor=Black,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(3.1,-0.2)(4.3,2.3)
\psdots[dotsize=0.2](4.3,2.4)
\usefont{T1}{ptm}{m}{n}
\rput(3.8414063,2.61){$\ve{P}$}
\end{pspicture} 
}

\newslide

Cuando se tienen dos puntos $\ve{Q}$ y $\ve{P}$ se puede encontrar el vector dirección
\[\ve{v}=\ve{PQ}=\ve{Q}-\ve{P}\]
Y por lo tanto el conjunto de puntos de la recta queda 
\[\mathcal{R}=\{\ve{P}+t (\ve{Q}-\ve{P}) \mid t \in \R \}\]
y la \de{ecuación de una recta} que pasa por $\ve{P}$ y $\ve{Q}$ esta dada por
\[\ve{x}=\ve{P}+t (\ve{Q}-\ve{P})\]
Esta ecuación se llama \de{ecuación paramétrica de la recta}



% Generated with LaTeXDraw 1.9.4
% Fri Oct 19 08:37:50 COT 2012
% \usepackage[usenames,dvipsnames]{pstricks}
% \usepackage{epsfig}
% \usepackage{pst-grad} % For gradients
% \usepackage{pst-plot} % For axes
\scalebox{1} % Change this value to rescale the drawing.
{
\begin{pspicture}(0,-3.3192189)(6.1446877,3.3592188)
\rput(3.1,-0.3192188){\psaxes[linewidth=0.04](0,0)(-3,-3)(3,3)}
\psline[linewidth=0.04cm,fillcolor=Black,linestyle=dashed,dash=0.16cm 0.16cm](0.0,0.28078118)(6.0,3.0807812)
\psline[linewidth=0.08cm,fillcolor=Black,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(4.4,2.2807813)(5.28,2.759219)
\usefont{T1}{ptm}{m}{n}
\rput(4.731406,3.0907812){$\ve{v}$}
\usefont{T1}{ptm}{m}{n}
\rput(5.995,3.1907814){$\mathcal{R}$}
\psline[linewidth=0.08cm,fillcolor=Black,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(3.1,-0.3192188)(4.3,2.1807814)
\psdots[dotsize=0.2](4.3,2.2807813)
\usefont{T1}{ptm}{m}{n}
\rput(3.9314063,2.4907813){$P$}
\psline[linewidth=0.08cm,fillcolor=Black,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(3.1,-0.3192188)(5.28,2.6592188)
\usefont{T1}{ptm}{m}{n}
\rput(5.731406,2.4907813){$Q$}
\psdots[dotsize=0.2](5.4,2.7807813)
\end{pspicture} 
}


\newslide
Una recta se puede escribir como la solución de un sistema de ecuaciones, 

En $\R^2$ si $a_1$ y $a_2$ no son cero al tiempo entonces la ecuación de la recta se puede representar como la solución de la siguiente ecuación
\[a_1 x+a_2 y = c\]
si se despeja $y$ cuando $a_2 \neq 0$, la ecuación queda
\[y=mx+b\]\\[1cm]
$m$ se conoce como la pendiente y $b$ el punto de corte con el eje y.


En $\R^3$ si $a \neq 0, b \neq 0$ y  $c \neq 0, $ entonces la ecuación de la recta se puede escribir como la solución de este par de ecuaciones llamada \de{ecuación simétrica de la recta}
\[\frac{x-x_0}{a}=\frac{y-y_0}{b}=\frac{z-z_0}{c}\]\\[1cm] 

Estas ecuaciones se obtienen despejando el parámetro en todas las ecuaciones e igualándolas.


%\newslide


%\includegraphics{Camaraoscurasombrilla.eps}


%\newslide
%\includegraphics{CamaraOscuraArbol.eps}

%\includegraphics[scale=0.9]{MesaCamaraOscura3.eps}


%\newslide

% Generated with LaTeXDraw 1.9.4
% Wed Oct 17 21:14:25 COT 2012
% \usepackage[usenames,dvipsnames]{pstricks}
% \usepackage{epsfig}
% \usepackage{pst-grad} % For gradients
% \usepackage{pst-plot} % For axes
\scalebox{1} % Change this value to rescale the drawing.
{
\begin{pspicture}(0,-4.02)(7.02,4.02)%\grilla
\rput{-180.0}(6.0,5.0){\pstriangle[linewidth=0.04,dimen=outer,fillstyle=solid,fillcolor=black](3.0,1.0)(6.0,3.0)}
\psline[linewidth=0.04cm,linestyle=dashed,dash=0.16cm 0.16cm](3.0,1.0)(6.3,-4.0)
\psline[linewidth=0.04cm,linestyle=dashed,dash=0.16cm 0.16cm](6.0,4.0)(4.6,-4.0)
\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm,doubleline=true,doublesep=0.055121526](7.0,-2.0)(5.3,-2.0)
\pspolygon[linewidth=0.04,fillstyle=solid,fillcolor=black](4.6,-4.0)(6.2,-3.9)(6.7,-4.0)
\psline[linewidth=0.04cm,fillcolor=black,linestyle=dotted,dotsep=0.16cm,doubleline=true,doublesep=0.055121526](7.0,-4.0)(7.0,-2.0)
\psline[linewidth=0.04cm,fillcolor=black,linestyle=dotted,dotsep=0.16cm,doubleline=true,doublesep=0.055121526](3.0,-2.0)(3.0,-4.0)
\psline[linewidth=0.04cm,fillcolor=black,linestyle=dotted,dotsep=0.16cm,doubleline=true,doublesep=0.055121526](3.0,-4.0)(7.0,-4.0)
\psline[linewidth=0.04cm,fillcolor=black,linestyle=dotted,dotsep=0.16cm,doubleline=true,doublesep=0.055121526](3.0,-2.0)(4.7,-2.0)
\end{pspicture} 
}
\newslide
% Generated with LaTeXDraw 1.9.4
% Wed Oct 17 21:19:47 COT 2012
% \usepackage[usenames,dvipsnames]{pstricks}
% \usepackage{epsfig}
% \usepackage{pst-grad} % For gradients
% \usepackage{pst-plot} % For axes
\scalebox{1} % Change this value to rescale the drawing.
{
\begin{pspicture}(0,-4.02)(6.96,4.02)
\rput{-180.0}(8.0,5.0){\pstriangle[linewidth=0.04,dimen=outer,fillstyle=solid,fillcolor=black](4.0,1.0)(6.0,3.0)}
\psline[linewidth=0.04cm,linestyle=dashed,dash=0.16cm 0.16cm](1.0,4.0)(2.3,-4.0)
\psline[linewidth=0.04cm,linestyle=dashed,dash=0.16cm 0.16cm](4.0,1.0)(0.6,-4.0)
\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm,doubleline=true,doublesep=0.055121526](1.7,-2.0)(0.0,-2.0)
\pspolygon[linewidth=0.04,fillstyle=solid,fillcolor=black](0.2,-4.0)(0.7,-3.9)(2.3,-4.0)
\psline[linewidth=0.04cm,fillcolor=black,linestyle=dotted,dotsep=0.16cm,doubleline=true,doublesep=0.055121526](4.0,-4.0)(0.0,-4.0)
\psline[linewidth=0.04cm,fillcolor=black,linestyle=dotted,dotsep=0.16cm,doubleline=true,doublesep=0.055121526](4.0,-2.0)(4.0,-3.9)
\psline[linewidth=0.04cm,fillcolor=black,linestyle=dotted,dotsep=0.16cm,doubleline=true,doublesep=0.055121526](4.0,-2.0)(2.2,-2.0)
\psline[linewidth=0.04cm,fillcolor=black,linestyle=dotted,dotsep=0.16cm,doubleline=true,doublesep=0.055121526](0.0,-2.0)(0.0,-4.0)
\end{pspicture} 
}
\newslide
% Generated with LaTeXDraw 1.9.4
% Wed Oct 17 21:36:23 COT 2012
% \usepackage[usenames,dvipsnames]{pstricks}
% \usepackage{epsfig}
% \usepackage{pst-grad} % For gradients
% \usepackage{pst-plot} % For axes
\scalebox{1} % Change this value to rescale the drawing.
{
\begin{pspicture}(0,-4.5034375)(8.12,4.5034375)
\rput{-180.0}(8.0,5.026875){\pstriangle[linewidth=0.04,dimen=outer,fillstyle=solid,fillcolor=black](4.0,1.0134375)(6.0,3.0)}
\psline[linewidth=0.04cm,linestyle=dashed,dash=0.16cm 0.16cm](7.0,4.0134373)(5.6,-3.9865625)
\psline[linewidth=0.04cm,linestyle=dashed,dash=0.16cm 0.16cm](6.9,3.9134376)(0.3,-3.9865625)
\psline[linewidth=0.04cm,linestyle=dotted,dotsep=0.16cm,doubleline=true,doublesep=0.055121526](1.7,-1.9865625)(0.0,-1.9865625)
\pspolygon[linewidth=0.04,fillstyle=solid,fillcolor=black](5.6,-3.9865625)(7.2,-3.8865626)(7.7,-3.9865625)
\pspolygon[linewidth=0.04,fillstyle=solid,fillcolor=black](0.3,-3.9865625)(0.8,-3.8865626)(2.4,-3.9865625)
\psline[linewidth=0.04cm,fillcolor=black,linestyle=dotted,dotsep=0.16cm,doubleline=true,doublesep=0.055121526](2.2,-1.9865625)(5.8,-1.9865625)
\psline[linewidth=0.04cm,fillcolor=black,linestyle=dotted,dotsep=0.16cm,doubleline=true,doublesep=0.055121526](6.2,-1.9865625)(8.0,-1.9865625)
\psline[linewidth=0.04cm,fillcolor=black,linestyle=dotted,dotsep=0.16cm,doubleline=true,doublesep=0.055121526](8.0,-1.9865625)(8.0,-3.9865625)
\psline[linewidth=0.04cm,fillcolor=black,linestyle=dotted,dotsep=0.16cm,doubleline=true,doublesep=0.055121526](8.1,-3.9865625)(0.0,-3.9865625)
\psline[linewidth=0.04cm,fillcolor=black,linestyle=dotted,dotsep=0.16cm,doubleline=true,doublesep=0.055121526](0.0,-3.9865625)(0.0,-1.9865625)
\psline[linewidth=0.04cm,fillcolor=black,linestyle=dotted,dotsep=0.16cm,doubleline=true,doublesep=0.055121526](4.0,-1.9865625)(4.0,-3.9865625)
\usefont{T1}{ptm}{m}{n}
\rput(4.0351562,-4.2765627){(0,0)}
\psdots[dotsize=0.12](4.0,-3.9865625)
\psdots[dotsize=0.12](6.0,-1.9865625)
\psdots[dotsize=0.12](2.0,-1.9865625)
\psdots[dotsize=0.12](7.0,4.0134373)
\usefont{T1}{ptm}{m}{n}
\rput(7.3351564,4.3234377){(3,9)}
\usefont{T1}{ptm}{m}{n}
\rput(2.2951562,-2.2765625){(-2,2)}
\usefont{T1}{ptm}{m}{n}
\rput(6.4351563,-2.1765625){(2,2)}
\end{pspicture} 
}
\begin{itemize}
\item Escriba la ecuación paramétrica de cada una de las rectas
\item Escriba la ecuación con corte y pendiente de cada recta
\item Escriba donde corta cada recta el eje x
\end{itemize}

\newslide
\scalebox{1} % Change this value to rescale the drawing.
{
\begin{pspicture}(0,-4.271719)(8.160156,4.228281)
\rput{-180.0}(8.200313,5.4634376){\pstriangle[linewidth=0.04,dimen=outer,fillstyle=solid,fillcolor=Black](4.1001563,1.2317188)(6.0,3.0)}
\psline[linewidth=0.04cm,linestyle=dashed,dash=0.16cm 0.16cm](4.1001563,1.2317189)(7.400156,-3.7682812)
\psline[linewidth=0.04cm,linestyle=dashed,dash=0.16cm 0.16cm](4.1001563,1.2317189)(0.7001561,-3.7682812)
\psline[linewidth=0.04cm,linestyle=dotted,dash=0.17638889cm 0.10583334cm,doubleline=true,doublesep=0.055121526,doublecolor=white](1.8001561,-1.7828659)(0.1001561,-1.7828659)
\pspolygon[linewidth=0.04,fillstyle=solid,fillcolor=Black](5.700156,-3.7682812)(7.300156,-3.668281)(7.800156,-3.7682812)
\pspolygon[linewidth=0.04,fillstyle=solid,fillcolor=Black](0.3001561,-3.7682812)(0.8001561,-3.668281)(2.400156,-3.7682812)
\psline[linewidth=0.04cm,fillcolor=Black,linestyle=dotted,dash=0.17638889cm 0.10583334cm,doubleline=true,doublesep=0.055121526,doublecolor=white](2.300156,-1.7682811)(5.900156,-1.7682811)
\psline[linewidth=0.04cm,fillcolor=Black,linestyle=dotted,dash=0.17638889cm 0.10583334cm,doubleline=true,doublesep=0.055121526,doublecolor=white](6.300156,-1.7682811)(8.100156,-1.7682811)
\psline[linewidth=0.04cm,fillcolor=Black,linestyle=dotted,dash=0.17638889cm 0.10583334cm,doubleline=true,doublesep=0.055121526,doublecolor=white](8.100156,-1.7682811)(8.100156,-3.7682812)
\psline[linewidth=0.04cm,fillcolor=Black,linestyle=dotted,dash=0.17638889cm 0.10583334cm,doubleline=true,doublesep=0.055121526,doublecolor=white](8.100156,-3.7682812)(0.0,-3.7682812)
\psline[linewidth=0.04cm,fillcolor=Black,linestyle=dotted,dash=0.17638889cm 0.10583334cm,doubleline=true,doublesep=0.055121526,doublecolor=white](0.1001561,-3.668281)(0.1001561,-1.7828659)
\psline[linewidth=0.04cm,fillcolor=Black,linestyle=dotted,dash=0.17638889cm 0.10583334cm,doubleline=true,doublesep=0.055121526,doublecolor=white](4.1001563,-1.7682811)(4.1001563,-3.7682812)
\usefont{T1}{ptm}{m}{n}
\rput(4.0204687,-4.0448437){(0,0)}
\psdots[dotsize=0.12](4.000156,-3.7548437)
\psdots[dotsize=0.12](6.000156,-1.7548436)
\psdots[dotsize=0.12](2.100156,-1.7548436)
\psdots[dotsize=0.12](7.300156,-3.6548438)
\usefont{T1}{ptm}{m}{n}
\rput(7.4504685,-4.0448437){($\frac{10}{3}$,0)}
\usefont{T1}{ptm}{m}{n}
\rput(2.2804685,-2.0448437){(-2,2)}
\usefont{T1}{ptm}{m}{n}
\rput(6.5204687,-1.4448436){(2,2)}
\usefont{T1}{ptm}{m}{n}
\rput(0.61046875,-4.0448437){($\frac{-10}{3}$,0)}
\usefont{T1}{ptm}{m}{n}
\rput(4.960469,1.1551565){($x$,$y$)}
\psdots[dotsize=0.12](0.8001561,-3.6548438)
\end{pspicture} 
}   
\begin{itemize}
\item Escriba la ecuación paramétrica de cada una de las rectas
\item Escriba la ecuación $a_1x+a_2y=c$ de cada recta
\item Resuelva el sistema de las dos ecuaciones para encontrar donde se cruzan las dos rectas
\end{itemize}

\newslide
\section{Planos}
Recordemos que un \de{plano que pasa por el origen} es el generado por dos vectores linealmente independientes.
\[\mathcal{P}=\{s \ve{u}+t \ve{v} \mid s,t \in \R \}\]
y por lo tanto la \de{ecuación vectorial de un plano que pasa por el origen} esta dada por
\[\ve{x}=s \ve{u} + t \ve{v}\]


% Generated with LaTeXDraw 1.9.4
% Sun Oct 21 16:10:44 COT 2012
% \usepackage[usenames,dvipsnames]{pstricks}
% \usepackage{epsfig}
% \usepackage{pst-grad} % For gradients
% \usepackage{pst-plot} % For axes
\scalebox{1} % Change this value to rescale the drawing.
{
\begin{pspicture}(0,-3.02)(6.12,3.02)%\grilla
\definecolor{color1938b}{rgb}{0.8,0.8,0.8}
\definecolor{color1938f}{rgb}{0.4,0.4,0.4}
\psline[linewidth=0.04cm,fillcolor=Black,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(2.0,-1.0)(6.1,-1.0)
\pspolygon[linewidth=0.04,linecolor=color1938b,fillstyle=gradient,gradlines=2000,gradbegin=white,gradend=color1938f,gradmidpoint=1.0](2.9,0.9)(1.3,-0.8)(3.2,-2.8)
\psline[linewidth=0.04cm,fillcolor=Black,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(2.0,-1.0)(0.0,-3.0)
\psline[linewidth=0.04cm,fillcolor=Black,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(2.0,-1.0)(2.0,3.0)
\psdots[dotsize=0.12](2.0,-1.0)
\psline[linewidth=0.04cm,fillcolor=Black,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(2.0,-1.0)(2.4,-0.6)
\psline[linewidth=0.04cm,fillcolor=Black,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(2.0,-1.0)(2.4,-1.4)
\usefont{T1}{ptm}{m}{n}
\rput(2.2314063,-0.39){$\ve{u}$}
\usefont{T1}{ptm}{m}{n}
\rput(2.1314063,-1.49){$\ve{v}$}
\end{pspicture} 
}



\newslide
Un \de{plano} en general se obtiene al trasladar a un punto $\ve{P}$ un plano que pasa por el origen
\[\mathcal{P}=\{\ve{P}+s \ve{u}+t \ve{v} \mid s,t \in \R \}\]
y por lo tanto la \de{ecuación de una plano} que pasa por $\ve{P}$ esta dada por
\[\ve{x}=\ve{P}+s \ve{u}+t \ve{v}\]

% Generated with LaTeXDraw 1.9.4
% Sun Oct 21 16:02:51 COT 2012
% \usepackage[usenames,dvipsnames]{pstricks}
% \usepackage{epsfig}
% \usepackage{pst-grad} % For gradients
% \usepackage{pst-plot} % For axes
\scalebox{1} % Change this value to rescale the drawing.
{
\begin{pspicture}(0,-3.02)(6.12,3.02)%\grilla
\definecolor{color1938b}{rgb}{0.8,0.8,0.8}
\psline[linewidth=0.04cm,fillcolor=Black,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(2.0,-1.0)(2.0,3.0)
\psline[linewidth=0.04cm,fillcolor=Black,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(2.0,-1.0)(6.1,-1.0)
\psline[linewidth=0.04cm,fillcolor=Black,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(2.0,-1.0)(0.0,-3.0)
\pspolygon[linewidth=0.04,linecolor=color1938b,fillstyle=solid,fillcolor=color1938b](3.8,-1.1)(1.8,0.9)(0.8,-2.1)
\psdots[dotsize=0.12](2.0,-0.3)
\psline[linewidth=0.04cm,fillcolor=Black,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(2.0,-0.3)(2.6,-0.8)
\psline[linewidth=0.04cm,fillcolor=Black,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(2.0,-0.3)(1.8,-1.0)
\usefont{T1}{ptm}{m}{n}
\rput(2,0.11){$\ve{P}$}
\rput(1,-0,5){$\mathcal{P}$}
\usefont{T1}{ptm}{m}{n}
\rput(2.5,-0.4){$\ve{v}$}
\usefont{T1}{ptm}{m}{n}
\rput(1.8,-1.09){$\ve{u}$}
\end{pspicture} 
}

\newslide

\teorema{Si los vectores $\ve{u}$ y $\ve{v}$ generan un plano y si el vector $\ve{n}$ es ortogonal (normal) a $\ve{u}$ y a $\ve{v}$ entonces $\ve{n}$ es ortogonal a todos los vectores $\ve{x}$ del plano. Y por lo tanto se dice que $\ve{n}$ es \de{ortogonal al plano}. En $\R^3$, el vector $\ve{n}=\ve{u} \times \ve{v}$ es un vector normal al plano.}%.\\[1cm]

\teorema{El ángulo entre dos planos es el ángulo de sus normales.}


\scalebox{1} % Change this value to rescale the drawing.
{
\begin{pspicture}(0,-3.02)(6.12,3.02)%\grilla
\definecolor{color1938b}{rgb}{0.8,0.8,0.8}
\psline[linewidth=0.04cm,fillcolor=Black,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(2.0,-1.0)(2.0,3.0)
\psline[linewidth=0.04cm,fillcolor=Black,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(2.0,-1.0)(6.1,-1.0)
\psline[linewidth=0.04cm,fillcolor=Black,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(2.0,-1.0)(0.0,-3.0)
\pspolygon[linewidth=0.04,linecolor=color1938b,fillstyle=solid,fillcolor=color1938b](3.8,-1.1)(1.8,0.9)(0.8,-2.1)
\psdots[dotsize=0.12](2.0,-0.3)
\psline[linewidth=0.04cm,fillcolor=Black,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(2.0,-0.3)(2.6,-0.8)
\psline[linewidth=0.04cm,fillcolor=Black,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(2.0,-0.3)(1.8,-1.0)
\psline[linewidth=0.04cm,fillcolor=Black,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(2,-0.3)(3.5,0.3)
\usefont{T1}{ptm}{m}{n}
\rput(2,0.11){$\ve{P}$}
\rput(1,-0,5){$\mathcal{P}$}
\usefont{T1}{ptm}{m}{n}
\rput(2.5,-0.4){$\ve{v}$}
\usefont{T1}{ptm}{m}{n}
\rput(1.8,-1.09){$\ve{u}$}
\rput(3.5,0.1){$\ve{n}$}
\end{pspicture} 
}


%Las siguientes afirmaciones son para $\R^3$
\newslide
\teorema{Tres puntos $\ve{P},\ve{Q},\ve{R}$ no colineales pertenecen a un sólo plano. Ademas los vectores $\ve{u}=\ve{PQ}$ y $\ve{v}=\ve{PR}$ generan el plano.}

\scalebox{1} % Change this value to rescale the drawing.
{
\begin{pspicture}(0,-3.02)(6.12,3.02)%\grilla
\definecolor{color1938b}{rgb}{0.8,0.8,0.8}
\psline[linewidth=0.04cm,fillcolor=Black,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(2.0,-1.0)(2.0,3.0)
\psline[linewidth=0.04cm,fillcolor=Black,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(2.0,-1.0)(6.1,-1.0)
\psline[linewidth=0.04cm,fillcolor=Black,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(2.0,-1.0)(0.0,-3.0)
\pspolygon[linewidth=0.04,linecolor=color1938b,fillstyle=solid,fillcolor=color1938b](3.8,-1.1)(1.8,0.9)(0.8,-2.1)
\psdots[dotsize=0.12](2.0,-0.3)
\psline[linewidth=0.04cm,fillcolor=Black,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(2.0,-0.3)(2.6,-0.8)
\psline[linewidth=0.04cm,fillcolor=Black,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(2.0,-0.3)(1.8,-1.0)
\psline[linewidth=0.04cm,fillcolor=Black,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(2,-0.3)(3.5,0.3)
\usefont{T1}{ptm}{m}{n}
\rput(2,0.11){$\ve{P}$}
\rput(1,-0,5){$\mathcal{P}$}
\usefont{T1}{ptm}{m}{n}
\rput(2.5,-0.4){$\ve{v}$}
\rput(1.5,-0.4){$\ve{u}$}
\usefont{T1}{ptm}{m}{n}
\rput(1.7,-1.2){$\ve{Q}$}
\rput(2.7,-1){$\ve{R}$}
\rput(3.5,0.1){$\ve{n}$}
\end{pspicture} 
}

\newslide

\teorema{En $\R^3$, si $\ve{n} \neq 0$, entonces los puntos $\ve{x}$ que solucionan la ecuación $\ve{n} \cdot (\ve{x} - \ve{P})=0$ forman un plano que pasa por $\ve{P}$ y $\ve{n}$ es ortogonal (normal) al plano, esta ecuación se conoce como \de{punto-normal}.}%.\\[1cm]

\teorema{En $\R^3$, si $\bsmatrizB{a\\b\\c} \neq \ve{0}$, la solución de la ecuación $ax+by+cz+d=0$ es un plano cuya normal es el vector $\ve{n}=\bsmatrizB{a\\b\\c}$.}


\scalebox{1} % Change this value to rescale the drawing.
{
\begin{pspicture}(0,-3.02)(6.12,3.02)
\definecolor{color1938b}{rgb}{0.8,0.8,0.8}
\psline[linewidth=0.04cm,fillcolor=Black,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(2.0,-1.0)(2.0,3.0)
\psline[linewidth=0.04cm,fillcolor=Black,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(2.0,-1.0)(6.1,-1.0)
\psline[linewidth=0.04cm,fillcolor=Black,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(2.0,-1.0)(0.0,-3.0)
\pspolygon[linewidth=0.04,linecolor=color1938b,fillstyle=solid,fillcolor=color1938b](4.0,-1.0)(2.0,1.0)(1.0,-2.0)
\psline[linewidth=0.04cm,fillcolor=Black,arrowsize=0.05291667cm 2.0,arrowlength=1.4,arrowinset=0.4]{->}(2.4,-0.6)(3.5,0.3)
\psdots[dotsize=0.12](2.2,0.1)
\usefont{T1}{ptm}{m}{n}
\rput(2.0514061,0.41){$\ve{P}$}
\usefont{T1}{ptm}{m}{n}
\rput(3.0314062,-0.19){$\ve{n}$}
\rput(5,2){$\ve{n} \cdot (\ve{x} - \ve{P})=0$}
\rput(5,1.5){$ax+by+cz+d=0$}
\end{pspicture} 
}

\newslide
De un tren que viaja a velocidad constante queremos determinar su velocidad ($v$) y posición inicial ($x_0$) para poder determinar su posición en cualquier instante, mediante la ecuación $x=x_0 + v t$. Para lo cual tras personas que han visto el tren nos indican en que momento ($t$) y en que posición ($x$) lo vieron.
\[\bmatriz{1\\2},\bmatriz{2\\4},\bmatriz{3\\3}\]
Pero si resolvemos el sistema de ecuaciones resulta inconsistente, eso quiere decir que ninguna recta pasa por los tres puntos,  por lo tanto es necesario encontrar la recta que ``más'' se acerque a los tres puntos. Para lo cual vamos a formar un plano con los errores de la posición de cada uno de las observaciones y encontramos donde es más cerca ese plano al origen, el punto más cercano corresponde cuando el vector del punto al origen es normal al plano.











\newslide
\chapter{Espacios vectoriales}
\section{Introducción}
Un espacio vectorial de dimensión $n$ se puede ver como un conjunto al cual a cada elemento se le asigna uno y sólo un vector de $R^n$.

Ejemplos:
\begin{align*}
&y=mx+b && \bmatriz{b\\m}\\
&a_0+a_1x+a_2x^2 && \bmatriz{a\\b\\c}\\
&\bmatriz{a&b\\c&d}&&\bmatriz{a\\b\\c\\d}\\
&color&&\bmatriz{r\\g\\b}
\end{align*}
\newslide

Sin embargo, en la definición formal de un espacio vectorial no se menciona $\R^n$, eso permite tener espacios vectoriales más interesantes, como es el caso de las funciones reales.

Un \de{Espacio Vectorial} $V$ es un conjunto que cumple las siguientes propiedades, suponga que $\ve{u},\ve{v},\ve{w} \in V$.
\begin{itemize}
\item Tiene una operación \de{suma} ($+$) que cumple
\begin{itemize}
\item ley clausurativa $\ve{u}+\ve{v} \in V$
\item ley conmutativa $\ve{u}+\ve{v}=\ve{v}+\ve{u}$
\item ley asociativa $(\ve{u}+\ve{v})+\ve{w}=\ve{u}+(\ve{v}+\ve{w})$
\end{itemize}
\item Tiene un elemento \de{cero} que cumple la ley modulativa \[\ve{v}+0=0+\ve{v}\]
\item Para cada elemento $\ve{v}$ hay un elemento \de{opuesto} $\ve{(-v)}$ que cumple la ley del opuesto \[\ve{v}+\ve{(-v)}=\ve{(-v)}+\ve{v}=0\]
\item Tiene una \de{multiplicación por escalar} que cumple
\begin{itemize}
\item ley clusurativa $a\ve{v} \in V$
\item ley distributiva $a(\ve{u}+\ve{v})=a\ve{u}+a\ve{v}=(\ve{u}+\ve{v})a$
\item ley asociativa $a(b(\ve{v}))=(ab)(\ve{v})$
\item ley de la identidad $1\ve{v}=\ve{v}$
\end{itemize}
\end{itemize}

Ejemplos se E. V.
\begin{itemize}
\item $\R , \R^2 , \R^3 $ y en general $\R^n$.
\item $M_{m \times n}$, El conjunto de todas la matrices de ${m \times n}$.
\item $P$, El conjunto de todos los polinomios.
\item $F(R)$,  El conjunto de todas las funciones reales.
\end{itemize}

\newslide
\section{Subespacio vectorial} 
Un \de{subespacio vectorial} $W$ de un espacio $V$ es 
\begin{itemize}
\item un subconjunto de $V$ 
\item con la suma y multiplicación de $V$
\item cumple los axiomas de E.V.  
\end{itemize}%\\[1cm]

\teorema{Un  subconjunto $W$ de un E. V. $V$ es un subespacio de $V$ si y solo si
\begin{itemize}
\item Si $\ve{u} , \ve{v} \in W$ entonces  $\ve{u} + \ve{v} \in W$ 
\item Si $c \in \R$ y $\ve{v} \in W$ entonces $ c \ve{v} \in W$
\end{itemize}}
\newslide
\section{Base} 
Definir I.L, Gen y Base
Demostrar Bases canónicas

\teorema{Si $B$ es una base de un espacio vectorial $V$ de dimensión finita. Sean $\ve{u_1},\ve{u_2},\ldots,\ve{u_n}\in V$. Entonces $\ve{u}$ es una combinación lineal de $\ve{u_1},\ve{u_2},\ldots,\ve{u_n}\in V$ si y sólo si  $[\ve{u}]_B$ es una combinación lineal de $[\ve{u_1}]_B,[\ve{u_2}]_B,\ldots,[\ve{u_n}\in V]_B$  Además los coeficientes de una combinación lineal corresponden con los coeficientes de la otra combinación lineal}

Se extrapolan las nociones de 
\begin{itemize}
\item Combinación lineal
\item Espacio Generado, Espacio Columnas, Imagen
\item Independencia lineal
\item Base
\item Dimensión, rango y nulidad
\item Coordenadas
\item Espacio nulo
\end{itemize}


\newslide
\section{Transformaciones lineales}

Una transformación lineales es....

El dominio y el codominio son...


\newslide

La matriz asociada a una transformación

\newslide
La \de{imagen} de una transformación lineal $T:V \ra W$ ($Im(T)$) se define
\[Im(T):=\{T(v) \mid v \in V\}\]

\teorema{Las coordenadas de la imagen corresponden a la imagen de la matriz de la transformación (La imagen de la matriz es el subespacio generado por las columnas de la matriz, llamado también \de{espacio de las columnas})}.\\[0.5cm]
\teorema{La imagen de $T:V \ra W$ es un subespacio vectorial de $W$} Algunas veces se llama espacio a este subespacio.\\[0.5cm]

La dimensión de la imagen de la transformación lineal $T$ se llama \de{rango} ($\rho(T)=dim(Im(T))$), la cual coincide con la dimensión del espacio de las columnas de la matriz de $T$. 

\newslide

El \de{núcleo} o \de{kernel} de una transformación lineal $T:V \ra W$ ($Nu(T)$) es
\[Nu(T):=\{v \in V \mid T(v)=\ve{0} \}\]

\teorema{Las coordenadas del núcleo corresponden al núcleo de la matriz de la transformación}Algunas veces se llama espacio a este subespacio.\\[0.5cm]

\teorema{El núcleo es de $T:V \ra W$ es un subespacio vectorial de $V$}.\\[0.5cm]

La dimensión del núcleo de la transformación lineal $T$ se llama \de{nulidad} ($\nu(T)=dim(Im(T))$), la cual coincide con la dimensión de la nulidad de la matriz de $T$. 

%$\nu(T)+\rho(T)=dim(Im(T))$

\newslide

Al subespacio generado  por los vectores-$m$ de una transformación matricial \[A=[\ve{v_1} \ \ve{v_2} \ \ldots \ \ve{v_n} ]\] se le llama
\begin{itemize}
\item la \de{imagen} de $A$, ($Im(A)$) o
\item el \de{subespacio columna} de $A$, ($Col(A)$).
\end{itemize}
\[Im(A)=Col(A):=\{A\ve{u} \mid \ve{u} \in \R^m\}=\< \ve{v_1},\ve{v_2}, \ldots ,\ve{v_n} \>\]


Por simplicidad a estos subespacios algunas veces se les llama espacios. 


\newslide

1a1,sobre, isomorfismo



-------------------

\section{Dominio, Codominio e Imagen de la transformación matricial}

%-------------sobre 1a1 iso LI genera espacio

Para una transformación de la matriz $A=[\ve{v_1},\ve{v_2}, \ldots ,\ve{v_n}]$
\[T_A:\R^n \ra \R^m\]
\[T_A\left(\bmatriz{c_1\\c_2\\ \vdots \\ \vdots \\ c_n }\right)=c_1\ve{v_1}+c_2\ve{v_2}+ \ldots +c_n\ve{v_n}\]


$\R^n$ se llama el \de{dominio}, $\R^m$ es el \de{codominio} y la \de{imagen} es el 
%subconjunto del codominio al cual es transformado algún elemento del dominio. Es decir 
%El \de{subespacio generado por los vectores-$m$} $\ve{v_1},\ve{v_2}, \ldots ,\ve{v_n}$ 
 \dn{conjunto} de vectores que se pueden escribir como combinación lineal de $\ve{v_1},\ve{v_2}, \ldots ,\ve{v_n}$, también se le llama el \de{subespacio generado}
 por las columnas de $A$, denotado por $\< \ve{v_1},\ve{v_2}, \ldots ,\ve{v_n} \>$ o por $Gen\{\ve{v_1},\ve{v_2}, \ldots ,\ve{v_n}\}$, otro nombre es el \de{subespacio columna} de $A$, ($Col(A)$). A continuación se presenta los diferentes nombres del mismo conjunto.
%\[  Im(T_A):=
%\{ \] 

%Al subespacio generado  por los vectores-$m$ de una matriz $A=[\ve{v_1} \ \ve{v_2} \ \ldots \ \ve{v_n} ]$ 
.
\begin{align*}&Im(T_A)=\{T_A(\ve{c}) \mid \ve{c} \in \R^m\}=\{A\ve{c} \mid \ve{c} \in \R^m\}=Gen\{\ve{v_1},\ve{v_2}, \ldots ,\ve{v_n}\}\\&=\< \ve{v_1},\ve{v_2}, \ldots ,\ve{v_n} \> =Col(A)=\{c_1\ve{v_1}+c_2\ve{v_2}+\ldots+c_n\ve{v_n}\mid
c_1,c_2, \ldots ,c_n \in \R \}\end{align*}

Una transformación matricial $T_A$ es se dice que es \de{sobre} si la imagen es el codominio, en este caso también se dice que las columnas de $A$ \de{generan el espacio} $\R^m$.

%Si $Gen\{\ve{v_1},\ve{v_2}, \ldots ,\ve{v_n}\}=\R^m$ se dice que los vectores generan todo el espacio $\R^m$ además se dice que la transformación dada por $T \ve{x}=[\ve{v_1},\ve{v_2}, \ldots ,\ve{v_n}]\ve{x}$ es sobre.


El \de{subespacio renglón} de una matriz $A$ corresponde al espacio columna de la transpuesta de $A$, ($Ren(A)=Col(A')$)   

Por simplicidad a estos subespacios algunas veces se les llama espacios. 

Se sugiere mirar el ejemplo 20 y 21 de la sección 2.3 de la página 90 y los ejercicios 1 y 2. 

\section{Algunos subespacios de $\R^2$ y $\R^3$}

Una \de{recta que pasa por el origen} es el generado por un vector diferente de cero.
En su respectivo espacio ($\R^2$ o $\R^3$) 
\begin{itemize}
\item a la recta $\<\ve{i}\>$ se le conoce como el \de{eje x}, 
\item a la recta $\<\ve{j}\>$ se le conoce como el \de{eje y} 
\item en $R^3$ a la recta $\<\ve{k}\>$ se le conoce como el \de{eje z}. 
\end{itemize}
A estos ejes se les conoce como los ejes coordenados.
 
Un \de{plano que pasa por el origen} es el generado por dos vectores no paralelos y diferentes de cero.  $\R^2$ es un plano. En $\R^3$ un plano nunca es $\R^2$ (pero si tiene la misma forma y por eso se dice que un plano es \dn{isomorfo} a $\R^2$) en cambio 
\begin{itemize}
\item al plano $\<\ve{i},\ve{j}\>$ se le conoce como el \de{plano xy}, 
\item al plano $\<\ve{i},\ve{k}\>$ se le conoce como el \de{plano xz},  
\item al plano $\<\ve{j},\ve{z}\>$ se le conoce como el \de{plano yz}. 
\end{itemize} 
A estos planos se les conoce como los planos coordenados.


Ahora nos podemos preguntar que genera el origen y que genera dos vectores paralelos. En el primer caso, el origen genera el mismo origen y nada más. En el segundo caso, para saber que genera dos vectores paralelos vamos a usar el Teorema \cite[2.?.9]{NJ99} que dice:

\teorema{Si uno de los vectores-$m$ $\ve{v_1},\ve{v_2}, \ldots ,\ve{v_n}$ es una combinación lineal del resto 
 entonces el generador permanece igual si se elimina ese vector.}

Si dos vectores son paralelos (ver definición en la sección \ref{sec_vectores}) entonces uno es combinación lineal del otro y por lo tanto se puede eliminar uno de ellos y queda el generado por un vector. Por lo tanto el generado por dos vectores paralelos también es una recta que pasa por el origen.

¿Qué pasa si aplicamos las transformaciones elementales al eje x? La identidad, los dos escalamientos y sumando la segunda componente a la primera, no lo afectan. El intercambio lo transforma en el eje y. Finalmente, sumando la primera componente a la segunda lo transforma en una recta a 45°. 


 ---------------------- pasar eso a ecuaciones.
 
El  \dn{renglón} $i$ está asociado con la i-esima ecuación y en Scilab se selecciona así 

%\codigo{A(i,:)}
\verb!A(i,:)!

 Una \dn{columna} $j$ está asociada con la variable j-esima y en Scilab  se describe así 
 
% \codigo{A(:,j)} 
 \verb!A(:,j)! 
 
 La columna de la derecha  tiene los términos constantes. Para ver esta columna en Scilab se puede usar el símbolo  \verb!$!  que se utiliza para indicar el último valor.
 
% \codigo{A(:,\$)} 
 \verb!A(:,$)! 


 
 %, a esta última columna columna hace referencia la palabra `aumentada'.



 \bibliographystyle{plain}


\begin{thebibliography}{123457890}

\bibite{Blo00}
E. D. Bloch,  \textit{Proofs and Fundamental},   Birkhäuser, Boston, 2000.

\bibite{Ant06}
H. Anton,  \textit{Álgebra Lineal}, Editorial Limusa, 3a. edición, Mexico 2006.

\bibite{Gro05}
S. A. Grossman,  \textit{Álgebra Lineal}, Mc Graw Hill, 5a. edición, Mexico 2005.

\bibite{NJ99} Nakos, Joyner, \textit{Álgebra Lineal con aplicaciones}, Editorial Thomson 1999.

\bibite{Str3a} G. Strang, \textit{Linear Algebra and its applications}, 3a. edición, Thomson Learning.

\bibite{Str3b} G. Strang, \textit{Introduction to Linear Algebra}, 3a. edición, Wellesley Cambridge Press.

 

%\bibite{Max5.26}  \textit{Manual de Maxima}, Ver 5.26.

      \end{thebibliography}

%\printindex
--------
\end{document}

Recomendaciones
equilibrio en 
inicitaiva y cursos
si va mal estudiar mas sin desanimarse
ambiente adecuado en clase, el hablar distrae a sus compañeros
faltar a clase atrasa, tratar de no faltar y en caso de faltar adelantarse
No es un curso para resolver sistema de ecuaciones no se confie si ya sabe resorver Sistemas de ecuaciones o encontrar el determinante

2 parciales sin apuntes c/u de 20%
2 grupos de quices con apuntes cada grupo de 15%


problema con producto matricial\\
morfología y alg lin\\
ejemplos al con fca\\

genra a inf art mor y rel con si



